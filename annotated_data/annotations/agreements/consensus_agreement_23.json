[
    {
        "filename": "paper_1.txt",
        "label": "Unsupported claim",
        "text": "For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets",
        "start": 1372,
        "end": 1505,
        "spans_a": [
            {
                "filename": "paper_1.txt",
                "start": 1372,
                "end": 1505,
                "label": "Unsupported claim",
                "text": "For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets",
                "full_text": "Related Work\n\nLarge-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Sch\u00fctze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge.\n\nIntroducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.\n\nThere are various types of knowledge resources that can be used to enhance language models, focusing on different aspects. For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 . We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge align with our need.\n\n References: \nBanerjee, S. and Lavie, A. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. pp. 65--72\nBodenreider, O. 2004. The unified medical language system (umls): integrating biomedical terminology. In Nucleic acids research. pp. D267--D270\nBosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celikyilmaz, A., and Choi, Y. 2019. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4762--4779 10.18653/v1/P19-1470\nChakrabarty, T., Ghosh, D., Muresan, S., and Peng, N. 2020. R\u02c63: Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7976--7986 10.18653/v1/2020.acl-main.711\nDo, N. and Pavlick, E. 2021. Are rotten apples edible? challenging commonsense inference ability with exceptions. In FINDINGS.\nHendrickx, I., Kim, S. N., Kozareva, Z., Nakov, P., Diarmuid, '., O S'eaghdha, S., Pad'o, M., Pennacchiotti, L., Romano, S., and Szpakowicz 2010. SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation. pp. 33--38\nElizabeth M Huffman, D. I., Athanasiadis, N. E., Anton, Lindsay, A., Haskett, D. L., Doster, D., Stefanidis, N. K., and Lee 2021. How resilient is your team? exploring healthcare providers' well-being during the covid-19 pandemic. In The American Journal of Surgery. pp. 277--284\nHwang, J. D., Bhagavatula, C., Ronan Le Bras, J., Da, K., Sakaguchi, A., Bosselut, Y., and Choi 2020. Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs. In ArXiv. abs/2010.05953\nJi, H., Ke, P., Huang, S., Wei, F., Zhu, X., and Huang, M. 2020. Language generation with multi-hop reasoning on commonsense knowledge graph. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 725--736 10.18653/v1/2020.emnlp-main.54\nKassner, N. and Sch\u00fctze, H. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In ACL.\nKatz, N. and Mcnulty, K. 1994. Reflective listening. In Reflective listening.\nKitaev, N. and Klein, D. 2018. Constituency parsing with a self-attentive encoder. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 2676--2686 10.18653/v1/P18-1249\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 1910. Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs. In Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs.\nLi, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 110--119 10.18653/v1/N16-1014\nLin, C. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out. pp. 74--81\nLiu, W., Zhou, P., Zhao, Z., Wang, Z., Ju, Q., Deng, H., and Wang, P. 2020. K-bert: Enabling language representation with knowledge graph. In ArXiv. abs/1909.07606\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nHuanru Henry Mao, B. P., Majumder, J., Mcauley, G., and Cottrell 2019. Improving neural story generation by targeted common sense grounding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5988--5993 10.18653/v1/D19-1615\nWilliam, R., Miller, S., and Rollnick 2012. Motivational interviewing: Helping people change. In Motivational interviewing: Helping people change.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135\nMario R Paredes, V., Apaolaza, C., Fernandez-Robin, P., Hartmann, D., and Ya\u00f1ez-Martinez 2021. The impact of the covid-19 pandemic on subjective mental well-being: The interplay of perceived threat, future anxiety and resilience. In Personality and Individual Differences. pp. 110455\nP\u00e9rez-Rosas, V., Mihalcea, R., Resnicow, K., Singh, S., and An, L. 2016. Building a motivational interviewing dataset. In Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology. pp. 42--51 10.18653/v1/W16-0305\nPetroni, F., Rockt\u00e4schel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. 2019. Language models as knowledge bases? arXiv preprint. In Language models as knowledge bases? arXiv preprint. arXiv:1909.01066\nPetroni, F., Rockt\u00e4schel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. 2019. Association for Computational Linguistics. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2463--2473 10.18653/v1/D19-1250\nRadford, A. and Narasimhan, K. 2018. Improving language understanding by generative pre-training. In Improving language understanding by generative pre-training.\nReimers, N. and Gurevych, I. 1908. Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs. In Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs.\nReimers, N. and Gurevych, I. 2019. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3982--3992 10.18653/v1/D19-1410\nRen, P., Chen, Z., Monz, C., Ma, J., and De Rijke, M. 2020. Thinking globally, acting locally: Distantly supervised global-to-local knowledge selection for background based conversation. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8697--8704\nSap, M., Ronan Le Bras, E., Allaway, C., Bhagavatula, N., Lourie, H., Rashkin, B., Roof, Noah, A., Smith, Y., and Choi 2019. Atomic: An atlas of machine commonsense for ifthen reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 3027--3035\nShwartz, V., West, P., Ronan, L., Bras, C., Bhagavatula, Y., and Choi 2020. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 4615--4629 10.18653/v1/2020.emnlp-main.373\nSong, H., Zhang, W., Cui, Y., Wang, D., and Liu, T. 2019. Exploiting persona information for diverse generation of conversational responses. In IJCAI.\nSpeer, R., Chin, J., and Havasi, C. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence.\nWolf, T., Sanh, V., Chaumond, J., and Delangue, C. 2019. Transfertransfo: A transfer learning approach for neural network based conversational agents. In Transfertransfo: A transfer learning approach for neural network based conversational agents. arXiv:1901.08149\nZhang, H., Khashabi, D., Song, Y., and Roth, D. 2020. Transomcs: From linguistic graphs to commonsense knowledge. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI). pp. 2020\nZhang, T., Kishore, V., Wu, F., Kilian, Q., Weinberger, Y., and Artzi 2019. Bertscore: Evaluating text generation with bert. In Bertscore: Evaluating text generation with bert. arXiv:1904.09675\nZhao, X., Wu, W., Xu, C., Tao, C., Zhao, D., and Yan, R. 2020. Knowledgegrounded dialogue generation with pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 3377--3390 10.18653/v1/2020.emnlp-main.272"
            }
        ],
        "spans_b": [
            {
                "filename": "paper_1.txt",
                "start": 1372,
                "end": 1506,
                "label": "Unsupported claim",
                "text": "For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets.",
                "full_text": "Related Work\n\nLarge-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Sch\u00fctze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge.\n\nIntroducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.\n\nThere are various types of knowledge resources that can be used to enhance language models, focusing on different aspects. For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 . We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge align with our need.\n\n References: \nBanerjee, S. and Lavie, A. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. pp. 65--72\nBodenreider, O. 2004. The unified medical language system (umls): integrating biomedical terminology. In Nucleic acids research. pp. D267--D270\nBosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celikyilmaz, A., and Choi, Y. 2019. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4762--4779 10.18653/v1/P19-1470\nChakrabarty, T., Ghosh, D., Muresan, S., and Peng, N. 2020. R\u02c63: Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7976--7986 10.18653/v1/2020.acl-main.711\nDo, N. and Pavlick, E. 2021. Are rotten apples edible? challenging commonsense inference ability with exceptions. In FINDINGS.\nHendrickx, I., Kim, S. N., Kozareva, Z., Nakov, P., Diarmuid, '., O S'eaghdha, S., Pad'o, M., Pennacchiotti, L., Romano, S., and Szpakowicz 2010. SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation. pp. 33--38\nElizabeth M Huffman, D. I., Athanasiadis, N. E., Anton, Lindsay, A., Haskett, D. L., Doster, D., Stefanidis, N. K., and Lee 2021. How resilient is your team? exploring healthcare providers' well-being during the covid-19 pandemic. In The American Journal of Surgery. pp. 277--284\nHwang, J. D., Bhagavatula, C., Ronan Le Bras, J., Da, K., Sakaguchi, A., Bosselut, Y., and Choi 2020. Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs. In ArXiv. abs/2010.05953\nJi, H., Ke, P., Huang, S., Wei, F., Zhu, X., and Huang, M. 2020. Language generation with multi-hop reasoning on commonsense knowledge graph. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 725--736 10.18653/v1/2020.emnlp-main.54\nKassner, N. and Sch\u00fctze, H. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In ACL.\nKatz, N. and Mcnulty, K. 1994. Reflective listening. In Reflective listening.\nKitaev, N. and Klein, D. 2018. Constituency parsing with a self-attentive encoder. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 2676--2686 10.18653/v1/P18-1249\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 1910. Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs. In Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs.\nLi, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 110--119 10.18653/v1/N16-1014\nLin, C. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out. pp. 74--81\nLiu, W., Zhou, P., Zhao, Z., Wang, Z., Ju, Q., Deng, H., and Wang, P. 2020. K-bert: Enabling language representation with knowledge graph. In ArXiv. abs/1909.07606\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nHuanru Henry Mao, B. P., Majumder, J., Mcauley, G., and Cottrell 2019. Improving neural story generation by targeted common sense grounding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5988--5993 10.18653/v1/D19-1615\nWilliam, R., Miller, S., and Rollnick 2012. Motivational interviewing: Helping people change. In Motivational interviewing: Helping people change.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135\nMario R Paredes, V., Apaolaza, C., Fernandez-Robin, P., Hartmann, D., and Ya\u00f1ez-Martinez 2021. The impact of the covid-19 pandemic on subjective mental well-being: The interplay of perceived threat, future anxiety and resilience. In Personality and Individual Differences. pp. 110455\nP\u00e9rez-Rosas, V., Mihalcea, R., Resnicow, K., Singh, S., and An, L. 2016. Building a motivational interviewing dataset. In Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology. pp. 42--51 10.18653/v1/W16-0305\nPetroni, F., Rockt\u00e4schel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. 2019. Language models as knowledge bases? arXiv preprint. In Language models as knowledge bases? arXiv preprint. arXiv:1909.01066\nPetroni, F., Rockt\u00e4schel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. 2019. Association for Computational Linguistics. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2463--2473 10.18653/v1/D19-1250\nRadford, A. and Narasimhan, K. 2018. Improving language understanding by generative pre-training. In Improving language understanding by generative pre-training.\nReimers, N. and Gurevych, I. 1908. Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs. In Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs.\nReimers, N. and Gurevych, I. 2019. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3982--3992 10.18653/v1/D19-1410\nRen, P., Chen, Z., Monz, C., Ma, J., and De Rijke, M. 2020. Thinking globally, acting locally: Distantly supervised global-to-local knowledge selection for background based conversation. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8697--8704\nSap, M., Ronan Le Bras, E., Allaway, C., Bhagavatula, N., Lourie, H., Rashkin, B., Roof, Noah, A., Smith, Y., and Choi 2019. Atomic: An atlas of machine commonsense for ifthen reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 3027--3035\nShwartz, V., West, P., Ronan, L., Bras, C., Bhagavatula, Y., and Choi 2020. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 4615--4629 10.18653/v1/2020.emnlp-main.373\nSong, H., Zhang, W., Cui, Y., Wang, D., and Liu, T. 2019. Exploiting persona information for diverse generation of conversational responses. In IJCAI.\nSpeer, R., Chin, J., and Havasi, C. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence.\nWolf, T., Sanh, V., Chaumond, J., and Delangue, C. 2019. Transfertransfo: A transfer learning approach for neural network based conversational agents. In Transfertransfo: A transfer learning approach for neural network based conversational agents. arXiv:1901.08149\nZhang, H., Khashabi, D., Song, Y., and Roth, D. 2020. Transomcs: From linguistic graphs to commonsense knowledge. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI). pp. 2020\nZhang, T., Kishore, V., Wu, F., Kilian, Q., Weinberger, Y., and Artzi 2019. Bertscore: Evaluating text generation with bert. In Bertscore: Evaluating text generation with bert. arXiv:1904.09675\nZhao, X., Wu, W., Xu, C., Tao, C., Zhao, D., and Yan, R. 2020. Knowledgegrounded dialogue generation with pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 3377--3390 10.18653/v1/2020.emnlp-main.272"
            }
        ]
    },
    {
        "filename": "paper_1.txt",
        "label": "Coherence",
        "text": "For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet.",
        "start": 741,
        "end": 1101,
        "spans_a": [
            {
                "filename": "paper_1.txt",
                "start": 741,
                "end": 1101,
                "label": "Coherence",
                "text": "For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet.",
                "full_text": "Related Work\n\nLarge-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Sch\u00fctze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge.\n\nIntroducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.\n\nThere are various types of knowledge resources that can be used to enhance language models, focusing on different aspects. For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 . We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge align with our need.\n\n References: \nBanerjee, S. and Lavie, A. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. pp. 65--72\nBodenreider, O. 2004. The unified medical language system (umls): integrating biomedical terminology. In Nucleic acids research. pp. D267--D270\nBosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celikyilmaz, A., and Choi, Y. 2019. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4762--4779 10.18653/v1/P19-1470\nChakrabarty, T., Ghosh, D., Muresan, S., and Peng, N. 2020. R\u02c63: Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7976--7986 10.18653/v1/2020.acl-main.711\nDo, N. and Pavlick, E. 2021. Are rotten apples edible? challenging commonsense inference ability with exceptions. In FINDINGS.\nHendrickx, I., Kim, S. N., Kozareva, Z., Nakov, P., Diarmuid, '., O S'eaghdha, S., Pad'o, M., Pennacchiotti, L., Romano, S., and Szpakowicz 2010. SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation. pp. 33--38\nElizabeth M Huffman, D. I., Athanasiadis, N. E., Anton, Lindsay, A., Haskett, D. L., Doster, D., Stefanidis, N. K., and Lee 2021. How resilient is your team? exploring healthcare providers' well-being during the covid-19 pandemic. In The American Journal of Surgery. pp. 277--284\nHwang, J. D., Bhagavatula, C., Ronan Le Bras, J., Da, K., Sakaguchi, A., Bosselut, Y., and Choi 2020. Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs. In ArXiv. abs/2010.05953\nJi, H., Ke, P., Huang, S., Wei, F., Zhu, X., and Huang, M. 2020. Language generation with multi-hop reasoning on commonsense knowledge graph. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 725--736 10.18653/v1/2020.emnlp-main.54\nKassner, N. and Sch\u00fctze, H. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In ACL.\nKatz, N. and Mcnulty, K. 1994. Reflective listening. In Reflective listening.\nKitaev, N. and Klein, D. 2018. Constituency parsing with a self-attentive encoder. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 2676--2686 10.18653/v1/P18-1249\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 1910. Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs. In Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs.\nLi, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 110--119 10.18653/v1/N16-1014\nLin, C. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out. pp. 74--81\nLiu, W., Zhou, P., Zhao, Z., Wang, Z., Ju, Q., Deng, H., and Wang, P. 2020. K-bert: Enabling language representation with knowledge graph. In ArXiv. abs/1909.07606\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nHuanru Henry Mao, B. P., Majumder, J., Mcauley, G., and Cottrell 2019. Improving neural story generation by targeted common sense grounding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5988--5993 10.18653/v1/D19-1615\nWilliam, R., Miller, S., and Rollnick 2012. Motivational interviewing: Helping people change. In Motivational interviewing: Helping people change.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135\nMario R Paredes, V., Apaolaza, C., Fernandez-Robin, P., Hartmann, D., and Ya\u00f1ez-Martinez 2021. The impact of the covid-19 pandemic on subjective mental well-being: The interplay of perceived threat, future anxiety and resilience. In Personality and Individual Differences. pp. 110455\nP\u00e9rez-Rosas, V., Mihalcea, R., Resnicow, K., Singh, S., and An, L. 2016. Building a motivational interviewing dataset. In Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology. pp. 42--51 10.18653/v1/W16-0305\nPetroni, F., Rockt\u00e4schel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. 2019. Language models as knowledge bases? arXiv preprint. In Language models as knowledge bases? arXiv preprint. arXiv:1909.01066\nPetroni, F., Rockt\u00e4schel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. 2019. Association for Computational Linguistics. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2463--2473 10.18653/v1/D19-1250\nRadford, A. and Narasimhan, K. 2018. Improving language understanding by generative pre-training. In Improving language understanding by generative pre-training.\nReimers, N. and Gurevych, I. 1908. Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs. In Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs.\nReimers, N. and Gurevych, I. 2019. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3982--3992 10.18653/v1/D19-1410\nRen, P., Chen, Z., Monz, C., Ma, J., and De Rijke, M. 2020. Thinking globally, acting locally: Distantly supervised global-to-local knowledge selection for background based conversation. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8697--8704\nSap, M., Ronan Le Bras, E., Allaway, C., Bhagavatula, N., Lourie, H., Rashkin, B., Roof, Noah, A., Smith, Y., and Choi 2019. Atomic: An atlas of machine commonsense for ifthen reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 3027--3035\nShwartz, V., West, P., Ronan, L., Bras, C., Bhagavatula, Y., and Choi 2020. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 4615--4629 10.18653/v1/2020.emnlp-main.373\nSong, H., Zhang, W., Cui, Y., Wang, D., and Liu, T. 2019. Exploiting persona information for diverse generation of conversational responses. In IJCAI.\nSpeer, R., Chin, J., and Havasi, C. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence.\nWolf, T., Sanh, V., Chaumond, J., and Delangue, C. 2019. Transfertransfo: A transfer learning approach for neural network based conversational agents. In Transfertransfo: A transfer learning approach for neural network based conversational agents. arXiv:1901.08149\nZhang, H., Khashabi, D., Song, Y., and Roth, D. 2020. Transomcs: From linguistic graphs to commonsense knowledge. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI). pp. 2020\nZhang, T., Kishore, V., Wu, F., Kilian, Q., Weinberger, Y., and Artzi 2019. Bertscore: Evaluating text generation with bert. In Bertscore: Evaluating text generation with bert. arXiv:1904.09675\nZhao, X., Wu, W., Xu, C., Tao, C., Zhao, D., and Yan, R. 2020. Knowledgegrounded dialogue generation with pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 3377--3390 10.18653/v1/2020.emnlp-main.272"
            }
        ],
        "spans_b": [
            {
                "filename": "paper_1.txt",
                "start": 559,
                "end": 1101,
                "label": "Coherence",
                "text": "Introducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet.",
                "full_text": "Related Work\n\nLarge-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Sch\u00fctze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge.\n\nIntroducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.\n\nThere are various types of knowledge resources that can be used to enhance language models, focusing on different aspects. For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 . We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge align with our need.\n\n References: \nBanerjee, S. and Lavie, A. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. pp. 65--72\nBodenreider, O. 2004. The unified medical language system (umls): integrating biomedical terminology. In Nucleic acids research. pp. D267--D270\nBosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celikyilmaz, A., and Choi, Y. 2019. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4762--4779 10.18653/v1/P19-1470\nChakrabarty, T., Ghosh, D., Muresan, S., and Peng, N. 2020. R\u02c63: Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7976--7986 10.18653/v1/2020.acl-main.711\nDo, N. and Pavlick, E. 2021. Are rotten apples edible? challenging commonsense inference ability with exceptions. In FINDINGS.\nHendrickx, I., Kim, S. N., Kozareva, Z., Nakov, P., Diarmuid, '., O S'eaghdha, S., Pad'o, M., Pennacchiotti, L., Romano, S., and Szpakowicz 2010. SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation. pp. 33--38\nElizabeth M Huffman, D. I., Athanasiadis, N. E., Anton, Lindsay, A., Haskett, D. L., Doster, D., Stefanidis, N. K., and Lee 2021. How resilient is your team? exploring healthcare providers' well-being during the covid-19 pandemic. In The American Journal of Surgery. pp. 277--284\nHwang, J. D., Bhagavatula, C., Ronan Le Bras, J., Da, K., Sakaguchi, A., Bosselut, Y., and Choi 2020. Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs. In ArXiv. abs/2010.05953\nJi, H., Ke, P., Huang, S., Wei, F., Zhu, X., and Huang, M. 2020. Language generation with multi-hop reasoning on commonsense knowledge graph. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 725--736 10.18653/v1/2020.emnlp-main.54\nKassner, N. and Sch\u00fctze, H. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In ACL.\nKatz, N. and Mcnulty, K. 1994. Reflective listening. In Reflective listening.\nKitaev, N. and Klein, D. 2018. Constituency parsing with a self-attentive encoder. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 2676--2686 10.18653/v1/P18-1249\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 1910. Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs. In Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs.\nLi, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 110--119 10.18653/v1/N16-1014\nLin, C. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out. pp. 74--81\nLiu, W., Zhou, P., Zhao, Z., Wang, Z., Ju, Q., Deng, H., and Wang, P. 2020. K-bert: Enabling language representation with knowledge graph. In ArXiv. abs/1909.07606\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nHuanru Henry Mao, B. P., Majumder, J., Mcauley, G., and Cottrell 2019. Improving neural story generation by targeted common sense grounding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5988--5993 10.18653/v1/D19-1615\nWilliam, R., Miller, S., and Rollnick 2012. Motivational interviewing: Helping people change. In Motivational interviewing: Helping people change.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135\nMario R Paredes, V., Apaolaza, C., Fernandez-Robin, P., Hartmann, D., and Ya\u00f1ez-Martinez 2021. The impact of the covid-19 pandemic on subjective mental well-being: The interplay of perceived threat, future anxiety and resilience. In Personality and Individual Differences. pp. 110455\nP\u00e9rez-Rosas, V., Mihalcea, R., Resnicow, K., Singh, S., and An, L. 2016. Building a motivational interviewing dataset. In Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology. pp. 42--51 10.18653/v1/W16-0305\nPetroni, F., Rockt\u00e4schel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. 2019. Language models as knowledge bases? arXiv preprint. In Language models as knowledge bases? arXiv preprint. arXiv:1909.01066\nPetroni, F., Rockt\u00e4schel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. 2019. Association for Computational Linguistics. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2463--2473 10.18653/v1/D19-1250\nRadford, A. and Narasimhan, K. 2018. Improving language understanding by generative pre-training. In Improving language understanding by generative pre-training.\nReimers, N. and Gurevych, I. 1908. Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs. In Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs.\nReimers, N. and Gurevych, I. 2019. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3982--3992 10.18653/v1/D19-1410\nRen, P., Chen, Z., Monz, C., Ma, J., and De Rijke, M. 2020. Thinking globally, acting locally: Distantly supervised global-to-local knowledge selection for background based conversation. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8697--8704\nSap, M., Ronan Le Bras, E., Allaway, C., Bhagavatula, N., Lourie, H., Rashkin, B., Roof, Noah, A., Smith, Y., and Choi 2019. Atomic: An atlas of machine commonsense for ifthen reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 3027--3035\nShwartz, V., West, P., Ronan, L., Bras, C., Bhagavatula, Y., and Choi 2020. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 4615--4629 10.18653/v1/2020.emnlp-main.373\nSong, H., Zhang, W., Cui, Y., Wang, D., and Liu, T. 2019. Exploiting persona information for diverse generation of conversational responses. In IJCAI.\nSpeer, R., Chin, J., and Havasi, C. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence.\nWolf, T., Sanh, V., Chaumond, J., and Delangue, C. 2019. Transfertransfo: A transfer learning approach for neural network based conversational agents. In Transfertransfo: A transfer learning approach for neural network based conversational agents. arXiv:1901.08149\nZhang, H., Khashabi, D., Song, Y., and Roth, D. 2020. Transomcs: From linguistic graphs to commonsense knowledge. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI). pp. 2020\nZhang, T., Kishore, V., Wu, F., Kilian, Q., Weinberger, Y., and Artzi 2019. Bertscore: Evaluating text generation with bert. In Bertscore: Evaluating text generation with bert. arXiv:1904.09675\nZhao, X., Wu, W., Xu, C., Tao, C., Zhao, D., and Yan, R. 2020. Knowledgegrounded dialogue generation with pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 3377--3390 10.18653/v1/2020.emnlp-main.272"
            }
        ]
    },
    {
        "filename": "paper_2.txt",
        "label": "Format",
        "text": "Ajjour et al. 1",
        "start": 4881,
        "end": 4894,
        "spans_a": [
            {
                "filename": "paper_2.txt",
                "start": 4880,
                "end": 4896,
                "label": "Format",
                "text": "Ajjour et al. 1",
                "full_text": "Background\n\nHuman values are of concern to most if not to all social sciences (Rokeach, 1973) and have also been integrated into computational frameworks of argumentation (Bench-Capon, 2003). In NLP, values have been analyzed for personality profiling (Maheshwari et al., 2017), but not yet for argument mining, as considered here.\n\nValues in Social Science\n\nRokeach (1973) already described the two concepts of (1) a value as a belief pertaining to desirable end states or modes of conduct and (2) a value system as prioritization of values based on cultural, social, and personal factors. These definitions attribute values to persons rather than to objects, facilitating a systematic analysis (Rokeach, 1973). The paper at hand targets the personal values behind arguments, meaning the values in the former sense that the arguments, mostly implicitly, resort to.\n\nSeveral of the value schemes proposed in the literature pertain to specific purposes. England (1967) suggested 66 values related to management decisions, such as high productivity and prestige, and categorized them by relevant entity, for example business organizations and individuals. Brown and Crace (2002) looked at 14 values for counseling and therapy, such as responsibility and spirituality, and Kahle et al. (1988) at nine for consumer research, such as warm relationships and excitement.\n\nOther proposed value schemes are more generic. Combining research from anthropology, sociology, philosophy, and psychology, Rokeach (1973) estimates the total number of human values to be fewer than hundreds, and develops a practical survey of 36 values that distinguishes between values pertaining to desirable end states and desirable behavior. Specifically for cross-cultural analyses, Schwartz et al. ( 2012) derived 48 value questions from the universal needs of individuals and societies, including obeying all the laws and to be humble. Moreover, Schwartz (1994) proposes a relatedness of values by their tendency to be compatible in their pursuit (see Figure 1). This relatedness reflects two \"higher order\" conflicts: (1) openness to change/own thoughts vs. conservation/submission, and (2) self-transcension (directed towards others/the environment) vs. self-enhancing (directed towards one's self), allowing to analyse values at several levels. Cheng and Fleischmann (2010) consolidates 12 schemes into a \"meta-inventory\" with 16 values, such as honesty and justice, revealing a large overlap in schemes across fields of research. However, as the meta-inventory is strictly more coarse-grained than Schwartz et al.'s theory we do not investigate it further for this paper.\n\nValues in Argumentation Research\n\nFormal argumentation employs value systems to model audience-specific preferences, that is, an argument's strength depends on the degree to which the audience reveres the values the argument resorts to. Examples include value-based argumentation schemes (van der Weide et al., 2009), defeasible logic programming (Teze et al., 2019), and the value-based argumentation framework of Bench-Capon (2003). The latter is an extension of the abstract argumentation framework of Dung (1995) that has already been applied manually to analyze interactions with reasoning and persuasion subject to a specific value system (Atkinson and Bench-Capon, 2021). This paper present a first step towards the large-scale automatic application of these works as it takes values to argument mining. Feldman (2021) recently showed the strong connection between values and the moral foundation theory (Haidt, 2012). Like personal values, this theory analyzes ethical reasoning behind human choices, but considers five rather abstract \"foundations:\" care, fairness, loyalty, authority, and purity. Alshomary and Wachsmuth (2021) hypothesized that the foundations could be used for audiencespecific argument generation. Kobbe et al. (2020) tried to classify arguments by foundations, but noted a low human agreement due to the vagueness of the foundations. We assume values can here contribute to the classification by foundations.\n\nPartly, values overlap with ideas of framing in communication, that is, the selection and emphasis of specific aspects of (perceived) reality to promote a particular problem definition, causal interpretation, ethical evaluation, and/or recommendation (Entman, 1993). In frames, values can define the costs and benefits of options (Entman, 1993) whereas common value systems are used for evaluation. Frames have been studied computationally for news (Naderi and Hirst, 2015), political speech (De Vreese, 2005), and argumentation (Ajjour et al., 2019). In argumentation, some values are so prevalent that they constitute frames of their own, indicating a potential use of values in frame identification. For example, 14 of the 54 values we use in this work are also frames in the dataset of Ajjour et al. 1 Values may be considered as aspects under which to group arguments. Some researchers have mined aspects from text (Trautmann, 2020) or used them to control argument generation (Schiller et al., 2021). Others have studied the task of opinion summarization in arguments (Egan et al., 2016;Misra et al., 2016;Chen et al., 2019), aiming at the most important aspects discussed in a debate. Related, the task of key point analysis (Bar-Haim et al., 2020;Friedman et al., 2021) is to generate a small set of concise statements that each represent a different aspect. We argue that analyzing the values found in a collection of arguments provides a new perspective to aspects in argumentation, focusing on the \"why\" behind an argument's reasoning.\n\n References: \nAjjour, Y., Alshomary, M., Wachsmuth, H., and Stein, B. 2019. Modeling Frames in Argumentation. In 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing (EMNLP 2019). pp. 2922--2932\nAlshomary, M. and Wachsmuth, H. 2021. Toward audience-aware argument generation. In Patterns. pp. 100253 10.1016/j.patter.2021.100253\nKatie Atkinson and Trevor Bench-Capon. 2021. Valuebased argumentation. In Journal of Applied Logics. pp. 1543--1588\nBabbar, R., Partalas, I., Gaussier, E., and Amini, M. 2013. On flat versus hierarchical classification in large-scale taxonomies. In 27th Annual Conference on Neural Information Processing Systems (NIPS 2013). pp. 1824--1832\nBar-Haim, R., Eden, L., Friedman, R., Kantor, Y., Lahav, D., and Slonim, N. 2020. From arguments to key points: Towards automatic argument summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4029--4039 10.18653/v1/2020.acl-main.371\nTrevor, J. M. 2003. Persuasion in practical argument using value-based argumentation frameworks. In J. Log. Comput. pp. 429--448 10.1093/logcom/13.3.429\nTrevor, J. M. 2021. Bench-Capon. 2021. Audiences and argument strength. In 3rd Workshop on Argument Strength.\nBrown, D. and Kelly Crace, R. 2002. Life values inventory facilitator's guide. In Life values inventory facilitator's guide.\nChen, S., Khashabi, D., Yin, W., Callison-Burch, C., and Roth, D. 2019. Seeing things from a different angle:discovering diverse perspectives about claims. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019). pp. 542--557 10.18653/v1/N19-1053\nCheng, A. and Fleischmann, K. R. 2010. Developing a meta-inventory of human values. In 73rd ASIS&T Annual Meeting (ASIST 2010). pp. 1--10 10.1002/meet.14504701232\nVreese, C.H.D. 2005. News framing: Theory and typology. Information design journal & document design. In News framing: Theory and typology. Information design journal & document design. pp. 13\nPhan Minh, D. 1995. On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. In Artificial Intelligence. pp. 321--357 10.1016/0004-3702(94)00041-X\nEgan, C., Siddharthan, A., and Wyner, A. Z. 2016. Summarising the points made in online political debates. In Proceedings of the Third Workshop on Argument Mining, hosted by the 54th Annual Meeting of the Association for Computational Linguistics, ArgMining@ACL 2016. 10.18653/v1/w16-2816\nEngland, G. W. 1967. Personal value systems of american managers. In Academy of Management journal. pp. 53--68\nRobert M Entman 1993. Framing: Towards clarification of a fractured paradigm. McQuail's reader in mass communication theory. In Framing: Towards clarification of a fractured paradigm. McQuail's reader in mass communication theory. pp. 390--397\nFeldman, G. 2021. Personal values and moral foundations: Examining relations and joint prediction of moral variables. Social Psychological and Personality. In Science. pp. 676--686\nFriedman, R., Dankin, L., Katz, Y., Hou, Y., and Slonim, N. 2021. Overview of KPA-2021 shared task: Key point based quantitative summarization. In Proceedings of the 8th Workshop on Argumentation Mining.\nGretz, S., Friedman, R., Cohen-Karlik, E., Toledo, A., Lahav, D., Aharonov, R., and Slonim, N. 2020. A large-scale dataset for argument quality ranking: Construction and analysis. In 34th AAAI Conference on Artificial Intelligence (AAAI 2020). pp. 7805--7813 10.1609/aaai.v34i05.6285\nHaerpfer, C., Inglehart, R., Moreno, A., Welzel, C., Kizilova, K., Diez-Medrano, J., Lagos, M., Norris, P., Ponarin, E., and Puranen, B. 2020. World values survey. In World values survey. 10.14281/18241.13\nHaidt, J. 2012. The righteous mind: Why good people are divided by politics and religion. In The righteous mind: Why good people are divided by politics and religion.\nHovy, D., Berg-Kirkpatrick, T., Vaswani, A., and Hovy, E. 2013. Learning whom to trust with mace. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013). pp. 1120--1130\nLynn R Kahle, B., Poulos, A., and Sukhdial 1988. Changes in social values in the united states during the past decade. In Journal of Advertising Research. pp. 35--41\nKobbe, J., Rehbein, I., Hulpus, I., and Stuckenschmidt, H. 2020. Exploring morality in argumentation. In Proceedings of the 7th Workshop on Argument Mining. pp. 30--40\nKrippendorff, K. 2004. Measuring the reliability of qualitative text analysis data. In Quality & quantity. pp. 787--800 10.1007/s11135-004-8107-7\nLoza Menc\u00eda, E. and Jannsen, F. 2016. Learning rules for multi-label classification: a stacking and a separate-and-conquer approach. In Machine Learning. pp. 77--126 10.1007/s10994-016-5552-1\nMaheshwari, T., Reganti, A. N., Gupta, S., Jamatia, A., Kumar, U., Gamb\u00e4ck, B., and Das, A. 2017. A societal sentiment analysis: Predicting the values and ethics of individuals by analysing social media content. In 15th Conference of the European Chapter of the Association for Computational Linguistics. pp. 731--741 10.18653/v1/e17-1069\nMisra, A., Ecker, B., and Walker, M. 2016. Measuring the similarity of sentential arguments in dialogue. In Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. pp. 276--287 10.18653/v1/W16-3636\nNaderi, N. and Hirst, G. 2015. Argumentation mining in parliamentary discourse. In Principles and practice of multi-agent systems. pp. 16--25\nRokeach, M. 1973. The nature of human values. In The nature of human values.\nSchiller, B., Daxenberger, J., and Gurevych, I. 2021. Aspect-controlled neural argument generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 380--396 10.18653/v1/2021.naacl-main.34\nShalom, H. and Schwartz 1994. Are there universal aspects in the structure and contents of human values?. In Journal of Social Issues. pp. 19--45 10.1111/j.1540-4560.1994.tb01196.x\nShalom, H., Schwartz, J., Cieciuch, M., Vecchione, E., Davidov, R., Fischer, C., Beierlein, A., Ramos, M., Verkasalo, J., L\u00f6nnqvist, K., and Demirutku 2012. Refining the theory of basic individual values. In Journal of personality and social psychology. pp. 103 10.1037/a0029393\nJohn R Searle 2003. Rationality in action. In Rationality in action.\nCarlos Teze, J., Perello-Moragues, A., Godo, L., and Noriega, P. 2019. Practical reasoning using values: an argumentative approach based on a hierarchy of values. In Annals of Mathematics and Artificial Intelligence. pp. 293--319 10.1007/s10472-019-09660-8\nTrautmann, D. 2020. Aspect-based argument mining. In Proceedings of the 7th Workshop on Argument Mining. pp. 41--52\nThomas, L., Van Der Weide, F., Dignum, J., Ch, Meyer, H., Prakken, G., and Vreeswijk 2009. Practical reasoning using values. In Argumentation in Multi-Agent Systems. pp. 79--93 10.1007/978-3-642-12805-9_5\nWilcox, R. R. 1996. Statistics for the Social Sciences. In Statistics for the Social Sciences."
            }
        ],
        "spans_b": [
            {
                "filename": "paper_2.txt",
                "start": 4881,
                "end": 4894,
                "label": "Format",
                "text": "Ajjour et al.",
                "full_text": "Background\n\nHuman values are of concern to most if not to all social sciences (Rokeach, 1973) and have also been integrated into computational frameworks of argumentation (Bench-Capon, 2003). In NLP, values have been analyzed for personality profiling (Maheshwari et al., 2017), but not yet for argument mining, as considered here.\n\nValues in Social Science\n\nRokeach (1973) already described the two concepts of (1) a value as a belief pertaining to desirable end states or modes of conduct and (2) a value system as prioritization of values based on cultural, social, and personal factors. These definitions attribute values to persons rather than to objects, facilitating a systematic analysis (Rokeach, 1973). The paper at hand targets the personal values behind arguments, meaning the values in the former sense that the arguments, mostly implicitly, resort to.\n\nSeveral of the value schemes proposed in the literature pertain to specific purposes. England (1967) suggested 66 values related to management decisions, such as high productivity and prestige, and categorized them by relevant entity, for example business organizations and individuals. Brown and Crace (2002) looked at 14 values for counseling and therapy, such as responsibility and spirituality, and Kahle et al. (1988) at nine for consumer research, such as warm relationships and excitement.\n\nOther proposed value schemes are more generic. Combining research from anthropology, sociology, philosophy, and psychology, Rokeach (1973) estimates the total number of human values to be fewer than hundreds, and develops a practical survey of 36 values that distinguishes between values pertaining to desirable end states and desirable behavior. Specifically for cross-cultural analyses, Schwartz et al. ( 2012) derived 48 value questions from the universal needs of individuals and societies, including obeying all the laws and to be humble. Moreover, Schwartz (1994) proposes a relatedness of values by their tendency to be compatible in their pursuit (see Figure 1). This relatedness reflects two \"higher order\" conflicts: (1) openness to change/own thoughts vs. conservation/submission, and (2) self-transcension (directed towards others/the environment) vs. self-enhancing (directed towards one's self), allowing to analyse values at several levels. Cheng and Fleischmann (2010) consolidates 12 schemes into a \"meta-inventory\" with 16 values, such as honesty and justice, revealing a large overlap in schemes across fields of research. However, as the meta-inventory is strictly more coarse-grained than Schwartz et al.'s theory we do not investigate it further for this paper.\n\nValues in Argumentation Research\n\nFormal argumentation employs value systems to model audience-specific preferences, that is, an argument's strength depends on the degree to which the audience reveres the values the argument resorts to. Examples include value-based argumentation schemes (van der Weide et al., 2009), defeasible logic programming (Teze et al., 2019), and the value-based argumentation framework of Bench-Capon (2003). The latter is an extension of the abstract argumentation framework of Dung (1995) that has already been applied manually to analyze interactions with reasoning and persuasion subject to a specific value system (Atkinson and Bench-Capon, 2021). This paper present a first step towards the large-scale automatic application of these works as it takes values to argument mining. Feldman (2021) recently showed the strong connection between values and the moral foundation theory (Haidt, 2012). Like personal values, this theory analyzes ethical reasoning behind human choices, but considers five rather abstract \"foundations:\" care, fairness, loyalty, authority, and purity. Alshomary and Wachsmuth (2021) hypothesized that the foundations could be used for audiencespecific argument generation. Kobbe et al. (2020) tried to classify arguments by foundations, but noted a low human agreement due to the vagueness of the foundations. We assume values can here contribute to the classification by foundations.\n\nPartly, values overlap with ideas of framing in communication, that is, the selection and emphasis of specific aspects of (perceived) reality to promote a particular problem definition, causal interpretation, ethical evaluation, and/or recommendation (Entman, 1993). In frames, values can define the costs and benefits of options (Entman, 1993) whereas common value systems are used for evaluation. Frames have been studied computationally for news (Naderi and Hirst, 2015), political speech (De Vreese, 2005), and argumentation (Ajjour et al., 2019). In argumentation, some values are so prevalent that they constitute frames of their own, indicating a potential use of values in frame identification. For example, 14 of the 54 values we use in this work are also frames in the dataset of Ajjour et al. 1 Values may be considered as aspects under which to group arguments. Some researchers have mined aspects from text (Trautmann, 2020) or used them to control argument generation (Schiller et al., 2021). Others have studied the task of opinion summarization in arguments (Egan et al., 2016;Misra et al., 2016;Chen et al., 2019), aiming at the most important aspects discussed in a debate. Related, the task of key point analysis (Bar-Haim et al., 2020;Friedman et al., 2021) is to generate a small set of concise statements that each represent a different aspect. We argue that analyzing the values found in a collection of arguments provides a new perspective to aspects in argumentation, focusing on the \"why\" behind an argument's reasoning.\n\n References: \nAjjour, Y., Alshomary, M., Wachsmuth, H., and Stein, B. 2019. Modeling Frames in Argumentation. In 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing (EMNLP 2019). pp. 2922--2932\nAlshomary, M. and Wachsmuth, H. 2021. Toward audience-aware argument generation. In Patterns. pp. 100253 10.1016/j.patter.2021.100253\nKatie Atkinson and Trevor Bench-Capon. 2021. Valuebased argumentation. In Journal of Applied Logics. pp. 1543--1588\nBabbar, R., Partalas, I., Gaussier, E., and Amini, M. 2013. On flat versus hierarchical classification in large-scale taxonomies. In 27th Annual Conference on Neural Information Processing Systems (NIPS 2013). pp. 1824--1832\nBar-Haim, R., Eden, L., Friedman, R., Kantor, Y., Lahav, D., and Slonim, N. 2020. From arguments to key points: Towards automatic argument summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4029--4039 10.18653/v1/2020.acl-main.371\nTrevor, J. M. 2003. Persuasion in practical argument using value-based argumentation frameworks. In J. Log. Comput. pp. 429--448 10.1093/logcom/13.3.429\nTrevor, J. M. 2021. Bench-Capon. 2021. Audiences and argument strength. In 3rd Workshop on Argument Strength.\nBrown, D. and Kelly Crace, R. 2002. Life values inventory facilitator's guide. In Life values inventory facilitator's guide.\nChen, S., Khashabi, D., Yin, W., Callison-Burch, C., and Roth, D. 2019. Seeing things from a different angle:discovering diverse perspectives about claims. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019). pp. 542--557 10.18653/v1/N19-1053\nCheng, A. and Fleischmann, K. R. 2010. Developing a meta-inventory of human values. In 73rd ASIS&T Annual Meeting (ASIST 2010). pp. 1--10 10.1002/meet.14504701232\nVreese, C.H.D. 2005. News framing: Theory and typology. Information design journal & document design. In News framing: Theory and typology. Information design journal & document design. pp. 13\nPhan Minh, D. 1995. On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. In Artificial Intelligence. pp. 321--357 10.1016/0004-3702(94)00041-X\nEgan, C., Siddharthan, A., and Wyner, A. Z. 2016. Summarising the points made in online political debates. In Proceedings of the Third Workshop on Argument Mining, hosted by the 54th Annual Meeting of the Association for Computational Linguistics, ArgMining@ACL 2016. 10.18653/v1/w16-2816\nEngland, G. W. 1967. Personal value systems of american managers. In Academy of Management journal. pp. 53--68\nRobert M Entman 1993. Framing: Towards clarification of a fractured paradigm. McQuail's reader in mass communication theory. In Framing: Towards clarification of a fractured paradigm. McQuail's reader in mass communication theory. pp. 390--397\nFeldman, G. 2021. Personal values and moral foundations: Examining relations and joint prediction of moral variables. Social Psychological and Personality. In Science. pp. 676--686\nFriedman, R., Dankin, L., Katz, Y., Hou, Y., and Slonim, N. 2021. Overview of KPA-2021 shared task: Key point based quantitative summarization. In Proceedings of the 8th Workshop on Argumentation Mining.\nGretz, S., Friedman, R., Cohen-Karlik, E., Toledo, A., Lahav, D., Aharonov, R., and Slonim, N. 2020. A large-scale dataset for argument quality ranking: Construction and analysis. In 34th AAAI Conference on Artificial Intelligence (AAAI 2020). pp. 7805--7813 10.1609/aaai.v34i05.6285\nHaerpfer, C., Inglehart, R., Moreno, A., Welzel, C., Kizilova, K., Diez-Medrano, J., Lagos, M., Norris, P., Ponarin, E., and Puranen, B. 2020. World values survey. In World values survey. 10.14281/18241.13\nHaidt, J. 2012. The righteous mind: Why good people are divided by politics and religion. In The righteous mind: Why good people are divided by politics and religion.\nHovy, D., Berg-Kirkpatrick, T., Vaswani, A., and Hovy, E. 2013. Learning whom to trust with mace. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013). pp. 1120--1130\nLynn R Kahle, B., Poulos, A., and Sukhdial 1988. Changes in social values in the united states during the past decade. In Journal of Advertising Research. pp. 35--41\nKobbe, J., Rehbein, I., Hulpus, I., and Stuckenschmidt, H. 2020. Exploring morality in argumentation. In Proceedings of the 7th Workshop on Argument Mining. pp. 30--40\nKrippendorff, K. 2004. Measuring the reliability of qualitative text analysis data. In Quality & quantity. pp. 787--800 10.1007/s11135-004-8107-7\nLoza Menc\u00eda, E. and Jannsen, F. 2016. Learning rules for multi-label classification: a stacking and a separate-and-conquer approach. In Machine Learning. pp. 77--126 10.1007/s10994-016-5552-1\nMaheshwari, T., Reganti, A. N., Gupta, S., Jamatia, A., Kumar, U., Gamb\u00e4ck, B., and Das, A. 2017. A societal sentiment analysis: Predicting the values and ethics of individuals by analysing social media content. In 15th Conference of the European Chapter of the Association for Computational Linguistics. pp. 731--741 10.18653/v1/e17-1069\nMisra, A., Ecker, B., and Walker, M. 2016. Measuring the similarity of sentential arguments in dialogue. In Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. pp. 276--287 10.18653/v1/W16-3636\nNaderi, N. and Hirst, G. 2015. Argumentation mining in parliamentary discourse. In Principles and practice of multi-agent systems. pp. 16--25\nRokeach, M. 1973. The nature of human values. In The nature of human values.\nSchiller, B., Daxenberger, J., and Gurevych, I. 2021. Aspect-controlled neural argument generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 380--396 10.18653/v1/2021.naacl-main.34\nShalom, H. and Schwartz 1994. Are there universal aspects in the structure and contents of human values?. In Journal of Social Issues. pp. 19--45 10.1111/j.1540-4560.1994.tb01196.x\nShalom, H., Schwartz, J., Cieciuch, M., Vecchione, E., Davidov, R., Fischer, C., Beierlein, A., Ramos, M., Verkasalo, J., L\u00f6nnqvist, K., and Demirutku 2012. Refining the theory of basic individual values. In Journal of personality and social psychology. pp. 103 10.1037/a0029393\nJohn R Searle 2003. Rationality in action. In Rationality in action.\nCarlos Teze, J., Perello-Moragues, A., Godo, L., and Noriega, P. 2019. Practical reasoning using values: an argumentative approach based on a hierarchy of values. In Annals of Mathematics and Artificial Intelligence. pp. 293--319 10.1007/s10472-019-09660-8\nTrautmann, D. 2020. Aspect-based argument mining. In Proceedings of the 7th Workshop on Argument Mining. pp. 41--52\nThomas, L., Van Der Weide, F., Dignum, J., Ch, Meyer, H., Prakken, G., and Vreeswijk 2009. Practical reasoning using values. In Argumentation in Multi-Agent Systems. pp. 79--93 10.1007/978-3-642-12805-9_5\nWilcox, R. R. 1996. Statistics for the Social Sciences. In Statistics for the Social Sciences."
            }
        ]
    },
    {
        "filename": "paper_2.txt",
        "label": "Coherence",
        "text": "Feldman (2021) recently showed the strong connection between values and the moral foundation theory (Haidt, 2012). Like personal values, this theory analyzes ethical reasoning behind human choices, but considers five rather abstract \"foundations:\" care, fairness, loyalty, authority, and purity. Alshomary and Wachsmuth (2021) hypothesized that the foundations could be used for audiencespecific argument generation. Kobbe et al. (2020) tried to classify arguments by foundations, but noted a low human agreement due to the vagueness of the foundations. We assume values can here contribute to the classification by foundations.",
        "start": 3461,
        "end": 4015,
        "spans_a": [
            {
                "filename": "paper_2.txt",
                "start": 3461,
                "end": 4089,
                "label": "Coherence",
                "text": "Feldman (2021) recently showed the strong connection between values and the moral foundation theory (Haidt, 2012). Like personal values, this theory analyzes ethical reasoning behind human choices, but considers five rather abstract \"foundations:\" care, fairness, loyalty, authority, and purity. Alshomary and Wachsmuth (2021) hypothesized that the foundations could be used for audiencespecific argument generation. Kobbe et al. (2020) tried to classify arguments by foundations, but noted a low human agreement due to the vagueness of the foundations. We assume values can here contribute to the classification by foundations.",
                "full_text": "Background\n\nHuman values are of concern to most if not to all social sciences (Rokeach, 1973) and have also been integrated into computational frameworks of argumentation (Bench-Capon, 2003). In NLP, values have been analyzed for personality profiling (Maheshwari et al., 2017), but not yet for argument mining, as considered here.\n\nValues in Social Science\n\nRokeach (1973) already described the two concepts of (1) a value as a belief pertaining to desirable end states or modes of conduct and (2) a value system as prioritization of values based on cultural, social, and personal factors. These definitions attribute values to persons rather than to objects, facilitating a systematic analysis (Rokeach, 1973). The paper at hand targets the personal values behind arguments, meaning the values in the former sense that the arguments, mostly implicitly, resort to.\n\nSeveral of the value schemes proposed in the literature pertain to specific purposes. England (1967) suggested 66 values related to management decisions, such as high productivity and prestige, and categorized them by relevant entity, for example business organizations and individuals. Brown and Crace (2002) looked at 14 values for counseling and therapy, such as responsibility and spirituality, and Kahle et al. (1988) at nine for consumer research, such as warm relationships and excitement.\n\nOther proposed value schemes are more generic. Combining research from anthropology, sociology, philosophy, and psychology, Rokeach (1973) estimates the total number of human values to be fewer than hundreds, and develops a practical survey of 36 values that distinguishes between values pertaining to desirable end states and desirable behavior. Specifically for cross-cultural analyses, Schwartz et al. ( 2012) derived 48 value questions from the universal needs of individuals and societies, including obeying all the laws and to be humble. Moreover, Schwartz (1994) proposes a relatedness of values by their tendency to be compatible in their pursuit (see Figure 1). This relatedness reflects two \"higher order\" conflicts: (1) openness to change/own thoughts vs. conservation/submission, and (2) self-transcension (directed towards others/the environment) vs. self-enhancing (directed towards one's self), allowing to analyse values at several levels. Cheng and Fleischmann (2010) consolidates 12 schemes into a \"meta-inventory\" with 16 values, such as honesty and justice, revealing a large overlap in schemes across fields of research. However, as the meta-inventory is strictly more coarse-grained than Schwartz et al.'s theory we do not investigate it further for this paper.\n\nValues in Argumentation Research\n\nFormal argumentation employs value systems to model audience-specific preferences, that is, an argument's strength depends on the degree to which the audience reveres the values the argument resorts to. Examples include value-based argumentation schemes (van der Weide et al., 2009), defeasible logic programming (Teze et al., 2019), and the value-based argumentation framework of Bench-Capon (2003). The latter is an extension of the abstract argumentation framework of Dung (1995) that has already been applied manually to analyze interactions with reasoning and persuasion subject to a specific value system (Atkinson and Bench-Capon, 2021). This paper present a first step towards the large-scale automatic application of these works as it takes values to argument mining. Feldman (2021) recently showed the strong connection between values and the moral foundation theory (Haidt, 2012). Like personal values, this theory analyzes ethical reasoning behind human choices, but considers five rather abstract \"foundations:\" care, fairness, loyalty, authority, and purity. Alshomary and Wachsmuth (2021) hypothesized that the foundations could be used for audiencespecific argument generation. Kobbe et al. (2020) tried to classify arguments by foundations, but noted a low human agreement due to the vagueness of the foundations. We assume values can here contribute to the classification by foundations.\n\nPartly, values overlap with ideas of framing in communication, that is, the selection and emphasis of specific aspects of (perceived) reality to promote a particular problem definition, causal interpretation, ethical evaluation, and/or recommendation (Entman, 1993). In frames, values can define the costs and benefits of options (Entman, 1993) whereas common value systems are used for evaluation. Frames have been studied computationally for news (Naderi and Hirst, 2015), political speech (De Vreese, 2005), and argumentation (Ajjour et al., 2019). In argumentation, some values are so prevalent that they constitute frames of their own, indicating a potential use of values in frame identification. For example, 14 of the 54 values we use in this work are also frames in the dataset of Ajjour et al. 1 Values may be considered as aspects under which to group arguments. Some researchers have mined aspects from text (Trautmann, 2020) or used them to control argument generation (Schiller et al., 2021). Others have studied the task of opinion summarization in arguments (Egan et al., 2016;Misra et al., 2016;Chen et al., 2019), aiming at the most important aspects discussed in a debate. Related, the task of key point analysis (Bar-Haim et al., 2020;Friedman et al., 2021) is to generate a small set of concise statements that each represent a different aspect. We argue that analyzing the values found in a collection of arguments provides a new perspective to aspects in argumentation, focusing on the \"why\" behind an argument's reasoning.\n\n References: \nAjjour, Y., Alshomary, M., Wachsmuth, H., and Stein, B. 2019. Modeling Frames in Argumentation. In 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing (EMNLP 2019). pp. 2922--2932\nAlshomary, M. and Wachsmuth, H. 2021. Toward audience-aware argument generation. In Patterns. pp. 100253 10.1016/j.patter.2021.100253\nKatie Atkinson and Trevor Bench-Capon. 2021. Valuebased argumentation. In Journal of Applied Logics. pp. 1543--1588\nBabbar, R., Partalas, I., Gaussier, E., and Amini, M. 2013. On flat versus hierarchical classification in large-scale taxonomies. In 27th Annual Conference on Neural Information Processing Systems (NIPS 2013). pp. 1824--1832\nBar-Haim, R., Eden, L., Friedman, R., Kantor, Y., Lahav, D., and Slonim, N. 2020. From arguments to key points: Towards automatic argument summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4029--4039 10.18653/v1/2020.acl-main.371\nTrevor, J. M. 2003. Persuasion in practical argument using value-based argumentation frameworks. In J. Log. Comput. pp. 429--448 10.1093/logcom/13.3.429\nTrevor, J. M. 2021. Bench-Capon. 2021. Audiences and argument strength. In 3rd Workshop on Argument Strength.\nBrown, D. and Kelly Crace, R. 2002. Life values inventory facilitator's guide. In Life values inventory facilitator's guide.\nChen, S., Khashabi, D., Yin, W., Callison-Burch, C., and Roth, D. 2019. Seeing things from a different angle:discovering diverse perspectives about claims. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019). pp. 542--557 10.18653/v1/N19-1053\nCheng, A. and Fleischmann, K. R. 2010. Developing a meta-inventory of human values. In 73rd ASIS&T Annual Meeting (ASIST 2010). pp. 1--10 10.1002/meet.14504701232\nVreese, C.H.D. 2005. News framing: Theory and typology. Information design journal & document design. In News framing: Theory and typology. Information design journal & document design. pp. 13\nPhan Minh, D. 1995. On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. In Artificial Intelligence. pp. 321--357 10.1016/0004-3702(94)00041-X\nEgan, C., Siddharthan, A., and Wyner, A. Z. 2016. Summarising the points made in online political debates. In Proceedings of the Third Workshop on Argument Mining, hosted by the 54th Annual Meeting of the Association for Computational Linguistics, ArgMining@ACL 2016. 10.18653/v1/w16-2816\nEngland, G. W. 1967. Personal value systems of american managers. In Academy of Management journal. pp. 53--68\nRobert M Entman 1993. Framing: Towards clarification of a fractured paradigm. McQuail's reader in mass communication theory. In Framing: Towards clarification of a fractured paradigm. McQuail's reader in mass communication theory. pp. 390--397\nFeldman, G. 2021. Personal values and moral foundations: Examining relations and joint prediction of moral variables. Social Psychological and Personality. In Science. pp. 676--686\nFriedman, R., Dankin, L., Katz, Y., Hou, Y., and Slonim, N. 2021. Overview of KPA-2021 shared task: Key point based quantitative summarization. In Proceedings of the 8th Workshop on Argumentation Mining.\nGretz, S., Friedman, R., Cohen-Karlik, E., Toledo, A., Lahav, D., Aharonov, R., and Slonim, N. 2020. A large-scale dataset for argument quality ranking: Construction and analysis. In 34th AAAI Conference on Artificial Intelligence (AAAI 2020). pp. 7805--7813 10.1609/aaai.v34i05.6285\nHaerpfer, C., Inglehart, R., Moreno, A., Welzel, C., Kizilova, K., Diez-Medrano, J., Lagos, M., Norris, P., Ponarin, E., and Puranen, B. 2020. World values survey. In World values survey. 10.14281/18241.13\nHaidt, J. 2012. The righteous mind: Why good people are divided by politics and religion. In The righteous mind: Why good people are divided by politics and religion.\nHovy, D., Berg-Kirkpatrick, T., Vaswani, A., and Hovy, E. 2013. Learning whom to trust with mace. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013). pp. 1120--1130\nLynn R Kahle, B., Poulos, A., and Sukhdial 1988. Changes in social values in the united states during the past decade. In Journal of Advertising Research. pp. 35--41\nKobbe, J., Rehbein, I., Hulpus, I., and Stuckenschmidt, H. 2020. Exploring morality in argumentation. In Proceedings of the 7th Workshop on Argument Mining. pp. 30--40\nKrippendorff, K. 2004. Measuring the reliability of qualitative text analysis data. In Quality & quantity. pp. 787--800 10.1007/s11135-004-8107-7\nLoza Menc\u00eda, E. and Jannsen, F. 2016. Learning rules for multi-label classification: a stacking and a separate-and-conquer approach. In Machine Learning. pp. 77--126 10.1007/s10994-016-5552-1\nMaheshwari, T., Reganti, A. N., Gupta, S., Jamatia, A., Kumar, U., Gamb\u00e4ck, B., and Das, A. 2017. A societal sentiment analysis: Predicting the values and ethics of individuals by analysing social media content. In 15th Conference of the European Chapter of the Association for Computational Linguistics. pp. 731--741 10.18653/v1/e17-1069\nMisra, A., Ecker, B., and Walker, M. 2016. Measuring the similarity of sentential arguments in dialogue. In Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. pp. 276--287 10.18653/v1/W16-3636\nNaderi, N. and Hirst, G. 2015. Argumentation mining in parliamentary discourse. In Principles and practice of multi-agent systems. pp. 16--25\nRokeach, M. 1973. The nature of human values. In The nature of human values.\nSchiller, B., Daxenberger, J., and Gurevych, I. 2021. Aspect-controlled neural argument generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 380--396 10.18653/v1/2021.naacl-main.34\nShalom, H. and Schwartz 1994. Are there universal aspects in the structure and contents of human values?. In Journal of Social Issues. pp. 19--45 10.1111/j.1540-4560.1994.tb01196.x\nShalom, H., Schwartz, J., Cieciuch, M., Vecchione, E., Davidov, R., Fischer, C., Beierlein, A., Ramos, M., Verkasalo, J., L\u00f6nnqvist, K., and Demirutku 2012. Refining the theory of basic individual values. In Journal of personality and social psychology. pp. 103 10.1037/a0029393\nJohn R Searle 2003. Rationality in action. In Rationality in action.\nCarlos Teze, J., Perello-Moragues, A., Godo, L., and Noriega, P. 2019. Practical reasoning using values: an argumentative approach based on a hierarchy of values. In Annals of Mathematics and Artificial Intelligence. pp. 293--319 10.1007/s10472-019-09660-8\nTrautmann, D. 2020. Aspect-based argument mining. In Proceedings of the 7th Workshop on Argument Mining. pp. 41--52\nThomas, L., Van Der Weide, F., Dignum, J., Ch, Meyer, H., Prakken, G., and Vreeswijk 2009. Practical reasoning using values. In Argumentation in Multi-Agent Systems. pp. 79--93 10.1007/978-3-642-12805-9_5\nWilcox, R. R. 1996. Statistics for the Social Sciences. In Statistics for the Social Sciences."
            }
        ],
        "spans_b": [
            {
                "filename": "paper_2.txt",
                "start": 3461,
                "end": 4015,
                "label": "Coherence",
                "text": "Feldman (2021) recently showed the strong connection between values and the moral foundation theory (Haidt, 2012). Like personal values, this theory analyzes ethical reasoning behind human choices, but considers five rather abstract \"foundations:\" care, fairness, loyalty, authority, and purity. Alshomary and Wachsmuth (2021) hypothesized that the foundations could be used for audiencespecific argument generation. Kobbe et al. (2020) tried to classify arguments by foundations, but noted a low human agreement due to the vagueness of the foundations.",
                "full_text": "Background\n\nHuman values are of concern to most if not to all social sciences (Rokeach, 1973) and have also been integrated into computational frameworks of argumentation (Bench-Capon, 2003). In NLP, values have been analyzed for personality profiling (Maheshwari et al., 2017), but not yet for argument mining, as considered here.\n\nValues in Social Science\n\nRokeach (1973) already described the two concepts of (1) a value as a belief pertaining to desirable end states or modes of conduct and (2) a value system as prioritization of values based on cultural, social, and personal factors. These definitions attribute values to persons rather than to objects, facilitating a systematic analysis (Rokeach, 1973). The paper at hand targets the personal values behind arguments, meaning the values in the former sense that the arguments, mostly implicitly, resort to.\n\nSeveral of the value schemes proposed in the literature pertain to specific purposes. England (1967) suggested 66 values related to management decisions, such as high productivity and prestige, and categorized them by relevant entity, for example business organizations and individuals. Brown and Crace (2002) looked at 14 values for counseling and therapy, such as responsibility and spirituality, and Kahle et al. (1988) at nine for consumer research, such as warm relationships and excitement.\n\nOther proposed value schemes are more generic. Combining research from anthropology, sociology, philosophy, and psychology, Rokeach (1973) estimates the total number of human values to be fewer than hundreds, and develops a practical survey of 36 values that distinguishes between values pertaining to desirable end states and desirable behavior. Specifically for cross-cultural analyses, Schwartz et al. ( 2012) derived 48 value questions from the universal needs of individuals and societies, including obeying all the laws and to be humble. Moreover, Schwartz (1994) proposes a relatedness of values by their tendency to be compatible in their pursuit (see Figure 1). This relatedness reflects two \"higher order\" conflicts: (1) openness to change/own thoughts vs. conservation/submission, and (2) self-transcension (directed towards others/the environment) vs. self-enhancing (directed towards one's self), allowing to analyse values at several levels. Cheng and Fleischmann (2010) consolidates 12 schemes into a \"meta-inventory\" with 16 values, such as honesty and justice, revealing a large overlap in schemes across fields of research. However, as the meta-inventory is strictly more coarse-grained than Schwartz et al.'s theory we do not investigate it further for this paper.\n\nValues in Argumentation Research\n\nFormal argumentation employs value systems to model audience-specific preferences, that is, an argument's strength depends on the degree to which the audience reveres the values the argument resorts to. Examples include value-based argumentation schemes (van der Weide et al., 2009), defeasible logic programming (Teze et al., 2019), and the value-based argumentation framework of Bench-Capon (2003). The latter is an extension of the abstract argumentation framework of Dung (1995) that has already been applied manually to analyze interactions with reasoning and persuasion subject to a specific value system (Atkinson and Bench-Capon, 2021). This paper present a first step towards the large-scale automatic application of these works as it takes values to argument mining. Feldman (2021) recently showed the strong connection between values and the moral foundation theory (Haidt, 2012). Like personal values, this theory analyzes ethical reasoning behind human choices, but considers five rather abstract \"foundations:\" care, fairness, loyalty, authority, and purity. Alshomary and Wachsmuth (2021) hypothesized that the foundations could be used for audiencespecific argument generation. Kobbe et al. (2020) tried to classify arguments by foundations, but noted a low human agreement due to the vagueness of the foundations. We assume values can here contribute to the classification by foundations.\n\nPartly, values overlap with ideas of framing in communication, that is, the selection and emphasis of specific aspects of (perceived) reality to promote a particular problem definition, causal interpretation, ethical evaluation, and/or recommendation (Entman, 1993). In frames, values can define the costs and benefits of options (Entman, 1993) whereas common value systems are used for evaluation. Frames have been studied computationally for news (Naderi and Hirst, 2015), political speech (De Vreese, 2005), and argumentation (Ajjour et al., 2019). In argumentation, some values are so prevalent that they constitute frames of their own, indicating a potential use of values in frame identification. For example, 14 of the 54 values we use in this work are also frames in the dataset of Ajjour et al. 1 Values may be considered as aspects under which to group arguments. Some researchers have mined aspects from text (Trautmann, 2020) or used them to control argument generation (Schiller et al., 2021). Others have studied the task of opinion summarization in arguments (Egan et al., 2016;Misra et al., 2016;Chen et al., 2019), aiming at the most important aspects discussed in a debate. Related, the task of key point analysis (Bar-Haim et al., 2020;Friedman et al., 2021) is to generate a small set of concise statements that each represent a different aspect. We argue that analyzing the values found in a collection of arguments provides a new perspective to aspects in argumentation, focusing on the \"why\" behind an argument's reasoning.\n\n References: \nAjjour, Y., Alshomary, M., Wachsmuth, H., and Stein, B. 2019. Modeling Frames in Argumentation. In 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing (EMNLP 2019). pp. 2922--2932\nAlshomary, M. and Wachsmuth, H. 2021. Toward audience-aware argument generation. In Patterns. pp. 100253 10.1016/j.patter.2021.100253\nKatie Atkinson and Trevor Bench-Capon. 2021. Valuebased argumentation. In Journal of Applied Logics. pp. 1543--1588\nBabbar, R., Partalas, I., Gaussier, E., and Amini, M. 2013. On flat versus hierarchical classification in large-scale taxonomies. In 27th Annual Conference on Neural Information Processing Systems (NIPS 2013). pp. 1824--1832\nBar-Haim, R., Eden, L., Friedman, R., Kantor, Y., Lahav, D., and Slonim, N. 2020. From arguments to key points: Towards automatic argument summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4029--4039 10.18653/v1/2020.acl-main.371\nTrevor, J. M. 2003. Persuasion in practical argument using value-based argumentation frameworks. In J. Log. Comput. pp. 429--448 10.1093/logcom/13.3.429\nTrevor, J. M. 2021. Bench-Capon. 2021. Audiences and argument strength. In 3rd Workshop on Argument Strength.\nBrown, D. and Kelly Crace, R. 2002. Life values inventory facilitator's guide. In Life values inventory facilitator's guide.\nChen, S., Khashabi, D., Yin, W., Callison-Burch, C., and Roth, D. 2019. Seeing things from a different angle:discovering diverse perspectives about claims. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019). pp. 542--557 10.18653/v1/N19-1053\nCheng, A. and Fleischmann, K. R. 2010. Developing a meta-inventory of human values. In 73rd ASIS&T Annual Meeting (ASIST 2010). pp. 1--10 10.1002/meet.14504701232\nVreese, C.H.D. 2005. News framing: Theory and typology. Information design journal & document design. In News framing: Theory and typology. Information design journal & document design. pp. 13\nPhan Minh, D. 1995. On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. In Artificial Intelligence. pp. 321--357 10.1016/0004-3702(94)00041-X\nEgan, C., Siddharthan, A., and Wyner, A. Z. 2016. Summarising the points made in online political debates. In Proceedings of the Third Workshop on Argument Mining, hosted by the 54th Annual Meeting of the Association for Computational Linguistics, ArgMining@ACL 2016. 10.18653/v1/w16-2816\nEngland, G. W. 1967. Personal value systems of american managers. In Academy of Management journal. pp. 53--68\nRobert M Entman 1993. Framing: Towards clarification of a fractured paradigm. McQuail's reader in mass communication theory. In Framing: Towards clarification of a fractured paradigm. McQuail's reader in mass communication theory. pp. 390--397\nFeldman, G. 2021. Personal values and moral foundations: Examining relations and joint prediction of moral variables. Social Psychological and Personality. In Science. pp. 676--686\nFriedman, R., Dankin, L., Katz, Y., Hou, Y., and Slonim, N. 2021. Overview of KPA-2021 shared task: Key point based quantitative summarization. In Proceedings of the 8th Workshop on Argumentation Mining.\nGretz, S., Friedman, R., Cohen-Karlik, E., Toledo, A., Lahav, D., Aharonov, R., and Slonim, N. 2020. A large-scale dataset for argument quality ranking: Construction and analysis. In 34th AAAI Conference on Artificial Intelligence (AAAI 2020). pp. 7805--7813 10.1609/aaai.v34i05.6285\nHaerpfer, C., Inglehart, R., Moreno, A., Welzel, C., Kizilova, K., Diez-Medrano, J., Lagos, M., Norris, P., Ponarin, E., and Puranen, B. 2020. World values survey. In World values survey. 10.14281/18241.13\nHaidt, J. 2012. The righteous mind: Why good people are divided by politics and religion. In The righteous mind: Why good people are divided by politics and religion.\nHovy, D., Berg-Kirkpatrick, T., Vaswani, A., and Hovy, E. 2013. Learning whom to trust with mace. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013). pp. 1120--1130\nLynn R Kahle, B., Poulos, A., and Sukhdial 1988. Changes in social values in the united states during the past decade. In Journal of Advertising Research. pp. 35--41\nKobbe, J., Rehbein, I., Hulpus, I., and Stuckenschmidt, H. 2020. Exploring morality in argumentation. In Proceedings of the 7th Workshop on Argument Mining. pp. 30--40\nKrippendorff, K. 2004. Measuring the reliability of qualitative text analysis data. In Quality & quantity. pp. 787--800 10.1007/s11135-004-8107-7\nLoza Menc\u00eda, E. and Jannsen, F. 2016. Learning rules for multi-label classification: a stacking and a separate-and-conquer approach. In Machine Learning. pp. 77--126 10.1007/s10994-016-5552-1\nMaheshwari, T., Reganti, A. N., Gupta, S., Jamatia, A., Kumar, U., Gamb\u00e4ck, B., and Das, A. 2017. A societal sentiment analysis: Predicting the values and ethics of individuals by analysing social media content. In 15th Conference of the European Chapter of the Association for Computational Linguistics. pp. 731--741 10.18653/v1/e17-1069\nMisra, A., Ecker, B., and Walker, M. 2016. Measuring the similarity of sentential arguments in dialogue. In Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. pp. 276--287 10.18653/v1/W16-3636\nNaderi, N. and Hirst, G. 2015. Argumentation mining in parliamentary discourse. In Principles and practice of multi-agent systems. pp. 16--25\nRokeach, M. 1973. The nature of human values. In The nature of human values.\nSchiller, B., Daxenberger, J., and Gurevych, I. 2021. Aspect-controlled neural argument generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 380--396 10.18653/v1/2021.naacl-main.34\nShalom, H. and Schwartz 1994. Are there universal aspects in the structure and contents of human values?. In Journal of Social Issues. pp. 19--45 10.1111/j.1540-4560.1994.tb01196.x\nShalom, H., Schwartz, J., Cieciuch, M., Vecchione, E., Davidov, R., Fischer, C., Beierlein, A., Ramos, M., Verkasalo, J., L\u00f6nnqvist, K., and Demirutku 2012. Refining the theory of basic individual values. In Journal of personality and social psychology. pp. 103 10.1037/a0029393\nJohn R Searle 2003. Rationality in action. In Rationality in action.\nCarlos Teze, J., Perello-Moragues, A., Godo, L., and Noriega, P. 2019. Practical reasoning using values: an argumentative approach based on a hierarchy of values. In Annals of Mathematics and Artificial Intelligence. pp. 293--319 10.1007/s10472-019-09660-8\nTrautmann, D. 2020. Aspect-based argument mining. In Proceedings of the 7th Workshop on Argument Mining. pp. 41--52\nThomas, L., Van Der Weide, F., Dignum, J., Ch, Meyer, H., Prakken, G., and Vreeswijk 2009. Practical reasoning using values. In Argumentation in Multi-Agent Systems. pp. 79--93 10.1007/978-3-642-12805-9_5\nWilcox, R. R. 1996. Statistics for the Social Sciences. In Statistics for the Social Sciences."
            }
        ]
    },
    {
        "filename": "paper_3.txt",
        "label": "Unsupported claim",
        "text": "However, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks.",
        "start": 715,
        "end": 1194,
        "spans_a": [
            {
                "filename": "paper_3.txt",
                "start": 715,
                "end": 1194,
                "label": "Unsupported claim",
                "text": "However, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks.",
                "full_text": "Introduction\n\nStarting with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al., 2019; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020). Following the pre-training techniques in NLP, self-supervised speech representation learning has also been investigated and shown promising results, benefiting from richly learned representations (Chung and Glass, 2018;Chuang et al., 2019;Song et al., 2019;Baevski et al., 2020;Hsu et al., 2021;Chung et al., 2021a), such as wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021).\n\nHowever, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks. How to design a unified encoder-decoder model that can take advantage of both unlabeled speech and text data to improve various spoken language processing tasks is not well explored.\n\nInspired by the T5 method (Raffel et al., 2019), we attempt to formulate each spoken language processing task as a speech/text to speech/text problem via an encoder-decoder framework, which enables us to use the same pre-trained model with bimodal data across diverse tasks, as shown in Figure 1. To achieve this, we propose a unified-modal pre-training framework, SpeechT5, containing an encoder-decoder backbone network and modal-specific pre/post-nets. With the pre-nets, the input speech/text is embedded in a shared space, and the encoder-decoder backbone network models the sequence-to-sequence conversion, from which the model-specific post-nets generate the speech/text output. Particularly, SpeechT5 is mainly pre-trained with a denoising sequence-tosequence method by leveraging large-scale unlabeled text and speech corpus. To align the textual and acoustic information into a unified semantic space, the proposed SpeechT5 model ( 1) maps text and speech representations into a shared vector quantization space, and (2) randomly mixes up the quantized latent representations and the contextual states, which can better guide the quantizer to learn the cross-modal features.\n\nWe fine-tune SpeechT5 on a wide variety of downstream spoken language processing tasks, including automatic speech recognition (ASR), textto-speech (TTS), speech translation (ST), voice conversion (VC), speech enhancement (SE), and speaker identification (SID). Massive experiments show that the proposed SpeechT5 model achieves a significant improvement on these spoken language processing tasks compared with the state-of-theart baselines. Specifically, the proposed SpeechT5 outperforms wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021) with the BASE model on the ASR task and also performs better than the state-of-the-art voice Transformer network  on the VC task. Besides, SpeechT5 is significantly superior to SpeechNet (Chen et al., 2021b) and pre-trained models from SUPERB (Yang et al., 2021) and achieves the stateof-the-art performance (i.e., 96.49%) on the SID task. We further provide an empirical comparison of the pre-training tasks and modules, and the ablation study demonstrates the effectiveness of the proposed joint speech-text pre-training method.\n\nThe contributions of this paper are summarized as follows.\n\n\u2022 To the best of our knowledge, this is the first work to investigate a unified encoder-decoder framework for various spoken language processing tasks.\n\n\u2022 We propose a cross-modal vector quantization approach, which learns the implicit alignment between acoustic and textual represen-tation with large-scale unlabeled speech and text data.\n\n\u2022 Extensive experiments on spoken language processing tasks demonstrate the effectiveness and superiority of the proposed SpeechT5 model.\n\n References: \nBaevski, A., Auli, M., and Mohamed, A. 2019. Effectiveness of self-supervised pretraining for speech recognition. In Effectiveness of self-supervised pretraining for speech recognition. arXiv:1911.03912\nBaevski, A., Zhou, Y., Mohamed, A., and Auli, M. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Proceedings of the 34th Conference on Neural Information Processing Systems. pp. 12449--12460\nChen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., Zhou, L., Ren, S., Qian, Y., Qian, Y., Wu, J., Zeng, M., and Wei, F. 2021. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. In Wavlm: Large-scale self-supervised pre-training for full stack speech processing. arXiv:2110.13900\nChen, Y., Chi, P., Yang, S., Chang, K., Lin, J., Huang, S., Liu, D., and Liu, C. Cheng-Kuang Lee, and Hungyi Lee. 2021b. Speechnet: A universal modularized model for speech processing tasks. In Cheng-Kuang Lee, and Hungyi Lee. 2021b. Speechnet: A universal modularized model for speech processing tasks. arXiv:2105.03070\nChen, Z., Zhang, Y., Rosenberg, A., Ramabhadran, B., Wang, G., and Moreno, P. 2021. Injecting text in self-supervised speech pretraining. In Injecting text in self-supervised speech pretraining. arXiv:2108.12226\nChuang, Y., Liu, C., Lee, H., and Lee, L. 2019. Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering. In Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering. arXiv:1910.11559\nChung, J.S., Huh, J., and Mun, S. 2020. Delving into VoxCeleb: Environment invariant speaker recognition. In Proceedings of Odyssey. pp. 349--356\nChung, Y. and Glass, J. 2018. Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech. In Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech. arXiv:1803.08976\nChung, Y., Zhang, Y., Han, W., Chiu, C., Qin, J., Pang, R., and Wu, Y. 2021. W2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training. In W2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training. arXiv:2108.06209\nChung, Y., Zhu, C., and Zeng, M. 2021. SPLAT: Speech-language joint pre-training for spoken language understanding. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1897--1907 10.18653/v1/2021.naacl-main.152\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423\nMattia, A. D., Gangi, R., Cattoni, L., Bentivogli, M., Negri, M., and Turchi 2019. MuST-C: a Multilingual Speech Translation Corpus. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2012--2017 10.18653/v1/N19-1202\nDong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y., Gao, J., Zhou, M., and Hon, H. 2019. Unified language model pre-training for natural language understanding and generation. In Proceedings of the 33rd Conference on Neural Information Processing Systems. pp. 13063--13075\nGulcehre, C., Firat, O., Xu, K., Cho, K., Barrault, L., Lin, H., Bougares, F., Schwenk, H., and Bengio, Y. 2015. On using monolingual corpora in neural machine translation. In On using monolingual corpora in neural machine translation. arXiv:1503.03535\nHori, T., Watanabe, S., and Hershey, J. 2017. Joint CTC/attention decoding for end-to-end speech recognition. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 518--529 10.18653/v1/P17-1048\nHsu, W., Bolte, B., Tsai, Y.H., and Lakhotia, K. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. In Hubert: Self-supervised speech representation learning by masked prediction of hidden units. arXiv:2106.07447\nHuang, W., Hayashi, T., Wu, Y., Kameoka, H., and Toda, T. 2021. Pretraining techniques for sequence-to-sequence voice conversion. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 745--755 10.1109/TASLP.2021.3049336\nInaguma, H., Kiyono, S., Duh, K., Karita, S., NelsonEnrique, Soplin, Y., Hayashi, T., and Watanabe, S. 2020. Espnet-st: Allin-one speech translation toolkit. In Espnet-st: Allin-one speech translation toolkit. arXiv:2004.10234\nJensen, J. and Taal, C. H. 2016. An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers. In IEEE/ACM Transactions on Audio Speech and Language Processing. pp. 2009--2022 10.1109/TASLP.2016.2585878\nKameoka, H., Huang, W., Tanaka, K., Kaneko, T., Hojo, N., and Toda, T. 2021. Many-to-many voice transformer network. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 656--670 10.1109/TASLP.2020.3047262\nKharitonov, E., Lee, A., Polyak, A., Adi, Y., Copet, J., Lakhotia, K., Nguyen, T., Rivi\u00e8re, M., Mohamed, A., and Dupoux, E. 2021. Text-free prosody-aware generative spoken language modeling. In Text-free prosody-aware generative spoken language modeling. arXiv:2109.03264\nKim, M., Kim, G., Lee, S., and Ha, J. 2021. St-bert: Cross-modal language model pre-training for end-to-end spoken language understanding. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 7478--7482 10.1109/ICASSP39728.2021.9414558\nDiederik, P., Kingma, J., and Ba 2014. Adam: A method for stochastic optimization. In Adam: A method for stochastic optimization. arXiv:1412.6980\nKominek, J. and Black, A. W. 2004. The cmu arctic speech databases. In Proceedings of the Fifth ISCA workshop on speech synthesis.\nKong, J., Kim, J., and Bae, J. 2020. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. In Proceedings of the 34th Conference on Neural Information Processing Systems. pp. 17022--17033\nLakhotia, K., Kharitonov, E., Hsu, W., Adi, Y., Polyak, A., Bolte, B., Nguyen, T., Copet, J., and Baevski, A. Generative spoken language modeling from raw audio. In Generative spoken language modeling from raw audio. arXiv:2102.01192\nLample, G. and Conneau, A. 2019. Crosslingual language model pretraining. In Crosslingual language model pretraining. arXiv:1901.07291\nLe, H., Pino, J., Wang, C., Gu, J., Schwab, D., and Besacier, L. 2021. Lightweight adapter tuning for multilingual speech translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 817--824 10.18653/v1/2021.acl-short.103\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703\nLi, N., Liu, S., Liu, Y., Zhao, S., and Liu, M. 2019. Neural speech synthesis with transformer network. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 6706--6713 10.1609/aaai.v33i01.33016706\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nMittag, G. and M\u00f6ller, S. 2020. Deep learning based assessment of synthetic speech naturalness. In Proceedings of the 2020 Interspeech. pp. 1748--1752\nNagrani, A. 2017. Joon Son Chung, and Andrew Zisserman. In Voxceleb: A large-scale speaker identification dataset. arXiv:1706.08612\nVan Den Oord, A., Vinyals, O., and Kavukcuoglu, K. 2017. Neural discrete representation learning. In Neural discrete representation learning. arXiv:1711.00937\nOtt, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In fairseq: A fast, extensible toolkit for sequence modeling. arXiv:1904.01038\nPanayotov, V., Chen, G., Povey, D., and Khudanpur, S. 2015. Librispeech: an asr corpus based on public domain audio books. In Proceedings of the 2015 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 5206--5210 10.1109/ICASSP.2015.7178964\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2227--2237 10.18653/v1/N18-1202\nQian, Y., Bianv, X., Shi, Y., Kanda, N., Shen, L., Xiao, Z., and Zeng, M. 2021. Speech-language pre-training for end-to-end spoken language understanding. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 7458--7462 10.1109/ICASSP39728.2021.9414900\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. 2019. Language models are unsupervised multitask learners. In OpenAI blog. pp. 9\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.J. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. In Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv:1910.10683\nChandan, K. A., Reddy, H., Dubey, V., Gopal, R., Cutler, S., and Braun Hannes Gamper, Robert Aichner, and Sriram Srinivasan. 2021. ICASSP 2021 deep noise suppression challenge. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 6623--6627 10.1109/ICASSP39728.2021.9415105\nShuo Ren, L., Zhou, S., Liu, F., Wei, M., Zhou, S., and Ma 2021. Semface: Pre-training encoder and decoder with a semantic interface for neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 4518--4527 10.18653/v1/2021.acl-long.348\nRix, A. W., Beerends, J. G., Hollier, M. P., and AP 2001. Perceptual evaluation of speech quality (PESQ) -A new method for speech quality assessment of telephone networks and codecs. In Proceedings of the 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. pp. 749--752 10.1109/ICASSP.2001.941023\nSebastian, B. and Ivan, T. 2020. Data augmentation and loss normalization for deep noise suppression. In Proceedings of Speech and Computer. pp. 79--86\nShaw, P., Uszkoreit, J., and Vaswani, A. 2018. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 464--468 10.18653/v1/N18-2074\nShen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y., Wang, Y., Skerrv-Ryan, R., Saurous, R. A., Agiomvrgiannakis, Y., and Wu, Y. 2018. Natural tts synthesis by conditioning wavenet on mel spectrogram predictions. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 4779--4783 10.1109/ICASSP.2018.8461368\nSnyder, D., Garcia-Romero, D., Sell, G., Povey, D., and Khudanpur, S. 2018. Xvectors: Robust DNN embeddings for speaker recognition. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 5329--5333 10.1109/ICASSP.2018.8461375\nSong, X., Wang, G., Wu, Z., Huang, Y., Su, D., Yu, D., and Meng, H. 2019. Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks. In Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks. arXiv:1910.10387\nSynnaeve, G., Xu, Q., Kahn, J., Likhomanenko, T., Grave, E., Pratap, V., Sriram, A., Liptchinsky, V., and Collobert, R. 2020. End-to-end asr: from supervised to semi-supervised learning with modern architectures. In End-to-end asr: from supervised to semi-supervised learning with modern architectures. arXiv:1911.08460\nTachibana, H., Uenoyama, K., and Aihara, S. 2018. Efficiently trainable text-tospeech system based on deep convolutional networks with guided attention. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 4784--4788 10.1109/ICASSP.2018.8461829\nTjandra, A., Sakti, S., and Nakamura, S. 2020. Machine speech chain. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 976--989 10.1109/TASLP.2020.2977776\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. 2017. Attention is all you need. In Proceedings of the 31st Conference on Neural Information Processing Systems. pp. 6000--6010 10.5555/3295222.3295349\nWang, C., Tang, Y., Ma, X., Wu, A., Okhonko, D., and Pino, J. 2020. Fairseq s2t: Fast speech-to-text modeling with fairseq. In Fairseq s2t: Fast speech-to-text modeling with fairseq. arXiv:2010.05171\nWang, C., Wu, Y., Qian, Y., Kumatani, K., Liu, S., Wei, F., Zeng, M., and Huang, X. 2021. Unispeech: Unified speech representation learning with labeled and unlabeled data. In Unispeech: Unified speech representation learning with labeled and unlabeled data. arXiv:2101.07597\nWatanabe, S., Hori, T., Karita, S., Hayashi, T., Nishitoba, J., Unno, Y., NelsonEnrique, Soplin, Y., Heymann, J., Wiesner, M., and Chen, N. 2018. Espnet: Endto-end speech processing toolkit. In Espnet: Endto-end speech processing toolkit. arXiv:1804.00015\nWatanabe, S., Hori, T., Kim, S., Hershey, J. R., and Hayashi, T. 2017. Hybrid ctc/attention architecture for end-to-end speech recognition. In IEEE Journal of Selected Topics in Signal Processing. pp. 1240--1253 10.1109/JSTSP.2017.2763455\nWichern, G., Antognini, J., Flynn, M., Zhu, L. R., Mcquinn, E., Crow, D., Manilow, E., and Roux, J. L. 2019. WHAM!: Extending speech separation to noisy environments. In WHAM!: Extending speech separation to noisy environments. arXiv:1907.01160\nYamamoto, R., Song, E., and Kim, J. 2020. Parallel Wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram. In Proceedings of the 2020 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 6199--6203 10.1109/ICASSP40776.2020.9053795\nShu-Wen Yang, P., Chi, Y., Chuang, Cheng, -.J., Lai, K., Lakhotia, Yist, Y., Lin, A. T., Liu, J., Shi, X., Chang, G., and Lin 2021. Superb: Speech processing universal performance benchmark. In Superb: Speech processing universal performance benchmark. arXiv:2105.01051"
            }
        ],
        "spans_b": [
            {
                "filename": "paper_3.txt",
                "start": 715,
                "end": 1194,
                "label": "Unsupported claim",
                "text": "However, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks.",
                "full_text": "Introduction\n\nStarting with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al., 2019; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020). Following the pre-training techniques in NLP, self-supervised speech representation learning has also been investigated and shown promising results, benefiting from richly learned representations (Chung and Glass, 2018;Chuang et al., 2019;Song et al., 2019;Baevski et al., 2020;Hsu et al., 2021;Chung et al., 2021a), such as wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021).\n\nHowever, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks. How to design a unified encoder-decoder model that can take advantage of both unlabeled speech and text data to improve various spoken language processing tasks is not well explored.\n\nInspired by the T5 method (Raffel et al., 2019), we attempt to formulate each spoken language processing task as a speech/text to speech/text problem via an encoder-decoder framework, which enables us to use the same pre-trained model with bimodal data across diverse tasks, as shown in Figure 1. To achieve this, we propose a unified-modal pre-training framework, SpeechT5, containing an encoder-decoder backbone network and modal-specific pre/post-nets. With the pre-nets, the input speech/text is embedded in a shared space, and the encoder-decoder backbone network models the sequence-to-sequence conversion, from which the model-specific post-nets generate the speech/text output. Particularly, SpeechT5 is mainly pre-trained with a denoising sequence-tosequence method by leveraging large-scale unlabeled text and speech corpus. To align the textual and acoustic information into a unified semantic space, the proposed SpeechT5 model ( 1) maps text and speech representations into a shared vector quantization space, and (2) randomly mixes up the quantized latent representations and the contextual states, which can better guide the quantizer to learn the cross-modal features.\n\nWe fine-tune SpeechT5 on a wide variety of downstream spoken language processing tasks, including automatic speech recognition (ASR), textto-speech (TTS), speech translation (ST), voice conversion (VC), speech enhancement (SE), and speaker identification (SID). Massive experiments show that the proposed SpeechT5 model achieves a significant improvement on these spoken language processing tasks compared with the state-of-theart baselines. Specifically, the proposed SpeechT5 outperforms wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021) with the BASE model on the ASR task and also performs better than the state-of-the-art voice Transformer network  on the VC task. Besides, SpeechT5 is significantly superior to SpeechNet (Chen et al., 2021b) and pre-trained models from SUPERB (Yang et al., 2021) and achieves the stateof-the-art performance (i.e., 96.49%) on the SID task. We further provide an empirical comparison of the pre-training tasks and modules, and the ablation study demonstrates the effectiveness of the proposed joint speech-text pre-training method.\n\nThe contributions of this paper are summarized as follows.\n\n\u2022 To the best of our knowledge, this is the first work to investigate a unified encoder-decoder framework for various spoken language processing tasks.\n\n\u2022 We propose a cross-modal vector quantization approach, which learns the implicit alignment between acoustic and textual represen-tation with large-scale unlabeled speech and text data.\n\n\u2022 Extensive experiments on spoken language processing tasks demonstrate the effectiveness and superiority of the proposed SpeechT5 model.\n\n References: \nBaevski, A., Auli, M., and Mohamed, A. 2019. Effectiveness of self-supervised pretraining for speech recognition. In Effectiveness of self-supervised pretraining for speech recognition. arXiv:1911.03912\nBaevski, A., Zhou, Y., Mohamed, A., and Auli, M. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Proceedings of the 34th Conference on Neural Information Processing Systems. pp. 12449--12460\nChen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., Zhou, L., Ren, S., Qian, Y., Qian, Y., Wu, J., Zeng, M., and Wei, F. 2021. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. In Wavlm: Large-scale self-supervised pre-training for full stack speech processing. arXiv:2110.13900\nChen, Y., Chi, P., Yang, S., Chang, K., Lin, J., Huang, S., Liu, D., and Liu, C. Cheng-Kuang Lee, and Hungyi Lee. 2021b. Speechnet: A universal modularized model for speech processing tasks. In Cheng-Kuang Lee, and Hungyi Lee. 2021b. Speechnet: A universal modularized model for speech processing tasks. arXiv:2105.03070\nChen, Z., Zhang, Y., Rosenberg, A., Ramabhadran, B., Wang, G., and Moreno, P. 2021. Injecting text in self-supervised speech pretraining. In Injecting text in self-supervised speech pretraining. arXiv:2108.12226\nChuang, Y., Liu, C., Lee, H., and Lee, L. 2019. Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering. In Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering. arXiv:1910.11559\nChung, J.S., Huh, J., and Mun, S. 2020. Delving into VoxCeleb: Environment invariant speaker recognition. In Proceedings of Odyssey. pp. 349--356\nChung, Y. and Glass, J. 2018. Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech. In Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech. arXiv:1803.08976\nChung, Y., Zhang, Y., Han, W., Chiu, C., Qin, J., Pang, R., and Wu, Y. 2021. W2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training. In W2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training. arXiv:2108.06209\nChung, Y., Zhu, C., and Zeng, M. 2021. SPLAT: Speech-language joint pre-training for spoken language understanding. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1897--1907 10.18653/v1/2021.naacl-main.152\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423\nMattia, A. D., Gangi, R., Cattoni, L., Bentivogli, M., Negri, M., and Turchi 2019. MuST-C: a Multilingual Speech Translation Corpus. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2012--2017 10.18653/v1/N19-1202\nDong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y., Gao, J., Zhou, M., and Hon, H. 2019. Unified language model pre-training for natural language understanding and generation. In Proceedings of the 33rd Conference on Neural Information Processing Systems. pp. 13063--13075\nGulcehre, C., Firat, O., Xu, K., Cho, K., Barrault, L., Lin, H., Bougares, F., Schwenk, H., and Bengio, Y. 2015. On using monolingual corpora in neural machine translation. In On using monolingual corpora in neural machine translation. arXiv:1503.03535\nHori, T., Watanabe, S., and Hershey, J. 2017. Joint CTC/attention decoding for end-to-end speech recognition. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 518--529 10.18653/v1/P17-1048\nHsu, W., Bolte, B., Tsai, Y.H., and Lakhotia, K. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. In Hubert: Self-supervised speech representation learning by masked prediction of hidden units. arXiv:2106.07447\nHuang, W., Hayashi, T., Wu, Y., Kameoka, H., and Toda, T. 2021. Pretraining techniques for sequence-to-sequence voice conversion. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 745--755 10.1109/TASLP.2021.3049336\nInaguma, H., Kiyono, S., Duh, K., Karita, S., NelsonEnrique, Soplin, Y., Hayashi, T., and Watanabe, S. 2020. Espnet-st: Allin-one speech translation toolkit. In Espnet-st: Allin-one speech translation toolkit. arXiv:2004.10234\nJensen, J. and Taal, C. H. 2016. An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers. In IEEE/ACM Transactions on Audio Speech and Language Processing. pp. 2009--2022 10.1109/TASLP.2016.2585878\nKameoka, H., Huang, W., Tanaka, K., Kaneko, T., Hojo, N., and Toda, T. 2021. Many-to-many voice transformer network. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 656--670 10.1109/TASLP.2020.3047262\nKharitonov, E., Lee, A., Polyak, A., Adi, Y., Copet, J., Lakhotia, K., Nguyen, T., Rivi\u00e8re, M., Mohamed, A., and Dupoux, E. 2021. Text-free prosody-aware generative spoken language modeling. In Text-free prosody-aware generative spoken language modeling. arXiv:2109.03264\nKim, M., Kim, G., Lee, S., and Ha, J. 2021. St-bert: Cross-modal language model pre-training for end-to-end spoken language understanding. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 7478--7482 10.1109/ICASSP39728.2021.9414558\nDiederik, P., Kingma, J., and Ba 2014. Adam: A method for stochastic optimization. In Adam: A method for stochastic optimization. arXiv:1412.6980\nKominek, J. and Black, A. W. 2004. The cmu arctic speech databases. In Proceedings of the Fifth ISCA workshop on speech synthesis.\nKong, J., Kim, J., and Bae, J. 2020. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. In Proceedings of the 34th Conference on Neural Information Processing Systems. pp. 17022--17033\nLakhotia, K., Kharitonov, E., Hsu, W., Adi, Y., Polyak, A., Bolte, B., Nguyen, T., Copet, J., and Baevski, A. Generative spoken language modeling from raw audio. In Generative spoken language modeling from raw audio. arXiv:2102.01192\nLample, G. and Conneau, A. 2019. Crosslingual language model pretraining. In Crosslingual language model pretraining. arXiv:1901.07291\nLe, H., Pino, J., Wang, C., Gu, J., Schwab, D., and Besacier, L. 2021. Lightweight adapter tuning for multilingual speech translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 817--824 10.18653/v1/2021.acl-short.103\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703\nLi, N., Liu, S., Liu, Y., Zhao, S., and Liu, M. 2019. Neural speech synthesis with transformer network. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 6706--6713 10.1609/aaai.v33i01.33016706\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nMittag, G. and M\u00f6ller, S. 2020. Deep learning based assessment of synthetic speech naturalness. In Proceedings of the 2020 Interspeech. pp. 1748--1752\nNagrani, A. 2017. Joon Son Chung, and Andrew Zisserman. In Voxceleb: A large-scale speaker identification dataset. arXiv:1706.08612\nVan Den Oord, A., Vinyals, O., and Kavukcuoglu, K. 2017. Neural discrete representation learning. In Neural discrete representation learning. arXiv:1711.00937\nOtt, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In fairseq: A fast, extensible toolkit for sequence modeling. arXiv:1904.01038\nPanayotov, V., Chen, G., Povey, D., and Khudanpur, S. 2015. Librispeech: an asr corpus based on public domain audio books. In Proceedings of the 2015 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 5206--5210 10.1109/ICASSP.2015.7178964\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2227--2237 10.18653/v1/N18-1202\nQian, Y., Bianv, X., Shi, Y., Kanda, N., Shen, L., Xiao, Z., and Zeng, M. 2021. Speech-language pre-training for end-to-end spoken language understanding. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 7458--7462 10.1109/ICASSP39728.2021.9414900\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. 2019. Language models are unsupervised multitask learners. In OpenAI blog. pp. 9\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.J. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. In Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv:1910.10683\nChandan, K. A., Reddy, H., Dubey, V., Gopal, R., Cutler, S., and Braun Hannes Gamper, Robert Aichner, and Sriram Srinivasan. 2021. ICASSP 2021 deep noise suppression challenge. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 6623--6627 10.1109/ICASSP39728.2021.9415105\nShuo Ren, L., Zhou, S., Liu, F., Wei, M., Zhou, S., and Ma 2021. Semface: Pre-training encoder and decoder with a semantic interface for neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 4518--4527 10.18653/v1/2021.acl-long.348\nRix, A. W., Beerends, J. G., Hollier, M. P., and AP 2001. Perceptual evaluation of speech quality (PESQ) -A new method for speech quality assessment of telephone networks and codecs. In Proceedings of the 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. pp. 749--752 10.1109/ICASSP.2001.941023\nSebastian, B. and Ivan, T. 2020. Data augmentation and loss normalization for deep noise suppression. In Proceedings of Speech and Computer. pp. 79--86\nShaw, P., Uszkoreit, J., and Vaswani, A. 2018. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 464--468 10.18653/v1/N18-2074\nShen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y., Wang, Y., Skerrv-Ryan, R., Saurous, R. A., Agiomvrgiannakis, Y., and Wu, Y. 2018. Natural tts synthesis by conditioning wavenet on mel spectrogram predictions. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 4779--4783 10.1109/ICASSP.2018.8461368\nSnyder, D., Garcia-Romero, D., Sell, G., Povey, D., and Khudanpur, S. 2018. Xvectors: Robust DNN embeddings for speaker recognition. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 5329--5333 10.1109/ICASSP.2018.8461375\nSong, X., Wang, G., Wu, Z., Huang, Y., Su, D., Yu, D., and Meng, H. 2019. Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks. In Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks. arXiv:1910.10387\nSynnaeve, G., Xu, Q., Kahn, J., Likhomanenko, T., Grave, E., Pratap, V., Sriram, A., Liptchinsky, V., and Collobert, R. 2020. End-to-end asr: from supervised to semi-supervised learning with modern architectures. In End-to-end asr: from supervised to semi-supervised learning with modern architectures. arXiv:1911.08460\nTachibana, H., Uenoyama, K., and Aihara, S. 2018. Efficiently trainable text-tospeech system based on deep convolutional networks with guided attention. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 4784--4788 10.1109/ICASSP.2018.8461829\nTjandra, A., Sakti, S., and Nakamura, S. 2020. Machine speech chain. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 976--989 10.1109/TASLP.2020.2977776\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. 2017. Attention is all you need. In Proceedings of the 31st Conference on Neural Information Processing Systems. pp. 6000--6010 10.5555/3295222.3295349\nWang, C., Tang, Y., Ma, X., Wu, A., Okhonko, D., and Pino, J. 2020. Fairseq s2t: Fast speech-to-text modeling with fairseq. In Fairseq s2t: Fast speech-to-text modeling with fairseq. arXiv:2010.05171\nWang, C., Wu, Y., Qian, Y., Kumatani, K., Liu, S., Wei, F., Zeng, M., and Huang, X. 2021. Unispeech: Unified speech representation learning with labeled and unlabeled data. In Unispeech: Unified speech representation learning with labeled and unlabeled data. arXiv:2101.07597\nWatanabe, S., Hori, T., Karita, S., Hayashi, T., Nishitoba, J., Unno, Y., NelsonEnrique, Soplin, Y., Heymann, J., Wiesner, M., and Chen, N. 2018. Espnet: Endto-end speech processing toolkit. In Espnet: Endto-end speech processing toolkit. arXiv:1804.00015\nWatanabe, S., Hori, T., Kim, S., Hershey, J. R., and Hayashi, T. 2017. Hybrid ctc/attention architecture for end-to-end speech recognition. In IEEE Journal of Selected Topics in Signal Processing. pp. 1240--1253 10.1109/JSTSP.2017.2763455\nWichern, G., Antognini, J., Flynn, M., Zhu, L. R., Mcquinn, E., Crow, D., Manilow, E., and Roux, J. L. 2019. WHAM!: Extending speech separation to noisy environments. In WHAM!: Extending speech separation to noisy environments. arXiv:1907.01160\nYamamoto, R., Song, E., and Kim, J. 2020. Parallel Wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram. In Proceedings of the 2020 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 6199--6203 10.1109/ICASSP40776.2020.9053795\nShu-Wen Yang, P., Chi, Y., Chuang, Cheng, -.J., Lai, K., Lakhotia, Yist, Y., Lin, A. T., Liu, J., Shi, X., Chang, G., and Lin 2021. Superb: Speech processing universal performance benchmark. In Superb: Speech processing universal performance benchmark. arXiv:2105.01051"
            }
        ]
    },
    {
        "filename": "paper_4.txt",
        "label": "Unsupported claim",
        "text": "In terms of the live evaluation, competitions such as Convai2 report such evaluations as highly challenging, with many of the resulting dialogues reported to be senseless, offensive, or simply not in line with instructions and ultimately live evaluation results have been discarded.",
        "start": 728,
        "end": 1010,
        "spans_a": [
            {
                "filename": "paper_4.txt",
                "start": 728,
                "end": 1010,
                "label": "Unsupported claim",
                "text": "In terms of the live evaluation, competitions such as Convai2 report such evaluations as highly challenging, with many of the resulting dialogues reported to be senseless, offensive, or simply not in line with instructions and ultimately live evaluation results have been discarded.",
                "full_text": "Problems in Past Evaluations\n\nA common issue occurs that can potentially impact the validity of results is filtering the set of systems to be evaluated via automatic metric scores. Since metric scores are known to be a poor substitute for human assessment, this only results in the pos-sibility that the best system according to human judges is inadvertently filtered out at this stage. For example, ConvAI2 (Dinan et al., 2019) ranked models firstly using automatic metrics before top models according to metric scores were assessed by crowd-sourced workers on Mechanical Turk, while similarly in the sixth Dialog System Technology Challenge (DSTC6) systems were filtered according to metric scores prior to human evaluation.\n\nIn terms of the live evaluation, competitions such as Convai2 report such evaluations as highly challenging, with many of the resulting dialogues reported to be senseless, offensive, or simply not in line with instructions and ultimately live evaluation results have been discarded.\n\nDespite challenges, competitions that operate in the public domain, making data and evaluation techniques available to researchers (such as ourselves) should be applauded for such efforts.\n\nOn the other hand, competitions that (for one reason or another) do not release data and evaluation techniques into the public domain have reported relative success in terms of human evaluation. However until such methods can be accessed and independently verified through replication studies, they will unfortunately have little impact . The first Amazon Alexa Socialbot Grand Challenge required human assessors to score how coherent and engaging conversations were on a 1-5 rating scale by two distinct groups: volunteer Amazon employees (experts), and general Alexa users (crowds) (Ram et al., 2018), are reported to achieve a correlation of overall scores for the two types of human assessors at 0.93. The absolute average rating across all chatbots was reported to be 20% lower for experts compared to general users. In an additional effort to evaluate models, conversational user experience, coherence, engagement, domain coverage, topical diversity, and conversational depth were assessed (1-5 scale), with combined scores reported to correlate with those of general users at r = 0.66. In addition to methods and data not being publicly available, correlations are difficult to interpret since no detail is provided about the number of judgments on which the correlation is calculated for example.\n\nIn addition to competitions that generally aim to include human evaluation of systems, automatic metrics are often proposed for dialogue evaluation, themselves requiring a human evaluation data set on which to evaluate the proposed metric. How-ever, inappropriate statistics are often applied. For example, Pang et al. (2020) propose a holistic metric to automatically evaluate four distinct aspects of dialogue, and a human evaluation experiment is deployed on Mechanical Turk using a 1-5 rating scale. The mean correlation between human assessors is reported as r = 0.61. However, mean correlations are unfortunately difficult to interpret, since correlation coefficients are not additive , averages calculated in the usual way cannot be assumed to reflect central tendency, and unfortunately, the distribution of correlations is not reported (Alexander, 1990). Mehri and Eskenazi (2020b) propose USR (Un-Supervised and Reference-free), an unsupervised model that predicts the quality of dialog for a range of criteria using various rating scales: understandable (0-1 rating scale), natural (1-3), maintains context (1-3), interesting (1-3), uses knowledge (0-1); overall quality (1-5). Despite human evaluation being carried out by experts inter-annotator agreement levels varied depending on criteria being measured, ranging from as low as 0.298. Additionally, although correlations between human assessments are reported as significant at p < 0.01, despite such statistics often being reported for correlations, they are unfortunately not very meaningful in terms of their impact on correlation interpretation and can be somewhat misleading. Contrary to common expectations, even small effect sizes (low r) can produce very low p-values (strong significance) in such tests. Aiming to achieve a significant correlation is an extremely low bar to reach in terms of consistency, since a low p-value in this case simply rejects the null hypothesis that the correlation is zero.\n\nIn addition to the above issues, human evaluation of dialogue systems rarely take into account the fact that differences in performance can occur simply by chance. The method of human evaluation we propose provides a means of applying standard tests for statistical significance to avoid concluding differences that are highly likely to have occurred simply by chance.\n\n References: \nRalph, A. 1990. A note on averaging correlations. In Bulletin of the Psychonomic Society. pp. 335--336\nAlexandrov, A. 2010. Characteristics of singleitem measures in likert scale format. In The Electronic Journal of Business Research Methods. pp. 1--12\nBarrault, L., Biesialska, M., Bojar, O., Costa-Juss\u00e0, M. R., Federmann, C., Graham, Y., Grundkiewicz, R., Haddow, B., Huck, M., Joanis, E., Kocmi, T., Koehn, P., Lo, C., Ljube\u0161i\u0107, N., Monz, C., Morishita, M., Nagata, M., and Nakazawa, T. Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (wmt20). In Proceedings of the Fifth Conference on Machine Translation. In Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (wmt20). In Proceedings of the Fifth Conference on Machine Translation. pp. 1--54\nBojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint-Amand, H., Soricut, R., Specia, L., and Tamchyna, A. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation. pp. 12--58\nBojar, O., Buck, C., Callison-Burch, C., Federmann, C., Haddow, B., Koehn, P., Monz, C., Post, M., Soricut, R., and Specia, L. 2013. Proceedings of the Eighth Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation. pp. 1--44\nCallison-Burch, C., Koehn, P., Monz, C., Post, M., Soricut, R., and Specia, L. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation. pp. 10--51\nCallison-Burch, C., Koehn, P., Monz, C., and Zaidan, O. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation. pp. 22--64\nDenkowski, M. and Lavie, A. 2011. Automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation. pp. 85--91\nDinan, E., Logacheva, V., Malykh, V., Miller, A. H., Shuster, K., Urbanek, J., Kiela, D., Szlam, A., Serban, I., and Lowe, R. 2019.\nCorr abs/1902.00098\nDinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston, J. 2018. Wizard of wikipedia: Knowledge-powered conversational agents. In Wizard of wikipedia: Knowledge-powered conversational agents. abs/1811.01241\nFinch, S. E. and Choi, J. D. 2020. Towards unified dialogue system evaluation: A comprehensive analysis of current evaluation protocols. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue. pp. 236--245\nGraham, Y., Baldwin, T., Moffat, A., and Zobel, J. 2013. Crowd-sourcing of human judgments of machine translation fluency. In Proceedings of the Australasian Language Technology Association Workshop 2013 (ALTA 2013). pp. 16--24\nHochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. In Neural Comput. pp. 1735--1780 10.1162/neco.1997.9.8.1735\nHumeau, S., Shuster, K., Lachaux, M., and Weston, J. 1905. Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring. In Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring.\nLasecki, W. S., Teevan, J., and Kamar, E. 2014. Information extraction and manipulation threats in crowd-powered systems. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing, CSCW '14. pp. 248--256 10.1145/2531602.2531733\nLin, C. and Hovy, E. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology. pp. 71--78\nLiu, Q., Ihler, A. T., and Steyvers, M. 2013. Scoring workers in crowdsourcing: How many control questions are enough?. In Advances in Neural Information Processing Systems.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.\nMiller, A. H., Fisch, A., Dodge, J., Amir-Hossein, Karimi, A., Bordes, J., and Weston 2016. Key-value memory networks for directly reading documents. In Key-value memory networks for directly reading documents.\nPang, B., Nijkamp, E., Han, W., Zhou, L., Liu, Y., and Tu, K. 2020. Towards holistic and automatic evaluation of open-domain dialogue generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 3619--3629 10.18653/v1/2020.acl-main.333\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02. pp. 311--318 10.3115/1073083.1073135\nRam, A., Prasad, R., Khatri, C., Venkatesh, A., Gabriel, R., Liu, Q., Nunn, J., Hedayatnia, B., Cheng, M., Nagar, A., and King, E. 2018. Gene Hwang, and Art Pettigrue. In Gene Hwang, and Art Pettigrue.\nSorodoc, I., Lau, J. H., Aletras, N., and Baldwin, T. 2017. Multimodal topic labelling. In Proceedings of the 15th Conference of the European Chapter. pp. 701--706\nValencia, S. Association for Computational Linguistics. In Association for Computational Linguistics.\nSutskever, I., Vinyals, O., Quoc, V., and Le 2014. Sequence to sequence learning with neural networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems. pp. 3104--3112\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser, L., Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., and Wang, W. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. In Oriol Vinyals. abs/1609.08144\nZhang, Y., Sun, S., Galley, M., Chen, Y., Brockett, C., Gao, X., Gao, J., Liu, J., and Dolan, B. 2020. DIALOGPT : Largescale generative pre-training for conversational response generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. pp. 270--278 10.18653/v1/2020.acl-demos.30"
            }
        ],
        "spans_b": [
            {
                "filename": "paper_4.txt",
                "start": 728,
                "end": 1010,
                "label": "Unsupported claim",
                "text": "In terms of the live evaluation, competitions such as Convai2 report such evaluations as highly challenging, with many of the resulting dialogues reported to be senseless, offensive, or simply not in line with instructions and ultimately live evaluation results have been discarded.",
                "full_text": "Problems in Past Evaluations\n\nA common issue occurs that can potentially impact the validity of results is filtering the set of systems to be evaluated via automatic metric scores. Since metric scores are known to be a poor substitute for human assessment, this only results in the pos-sibility that the best system according to human judges is inadvertently filtered out at this stage. For example, ConvAI2 (Dinan et al., 2019) ranked models firstly using automatic metrics before top models according to metric scores were assessed by crowd-sourced workers on Mechanical Turk, while similarly in the sixth Dialog System Technology Challenge (DSTC6) systems were filtered according to metric scores prior to human evaluation.\n\nIn terms of the live evaluation, competitions such as Convai2 report such evaluations as highly challenging, with many of the resulting dialogues reported to be senseless, offensive, or simply not in line with instructions and ultimately live evaluation results have been discarded.\n\nDespite challenges, competitions that operate in the public domain, making data and evaluation techniques available to researchers (such as ourselves) should be applauded for such efforts.\n\nOn the other hand, competitions that (for one reason or another) do not release data and evaluation techniques into the public domain have reported relative success in terms of human evaluation. However until such methods can be accessed and independently verified through replication studies, they will unfortunately have little impact . The first Amazon Alexa Socialbot Grand Challenge required human assessors to score how coherent and engaging conversations were on a 1-5 rating scale by two distinct groups: volunteer Amazon employees (experts), and general Alexa users (crowds) (Ram et al., 2018), are reported to achieve a correlation of overall scores for the two types of human assessors at 0.93. The absolute average rating across all chatbots was reported to be 20% lower for experts compared to general users. In an additional effort to evaluate models, conversational user experience, coherence, engagement, domain coverage, topical diversity, and conversational depth were assessed (1-5 scale), with combined scores reported to correlate with those of general users at r = 0.66. In addition to methods and data not being publicly available, correlations are difficult to interpret since no detail is provided about the number of judgments on which the correlation is calculated for example.\n\nIn addition to competitions that generally aim to include human evaluation of systems, automatic metrics are often proposed for dialogue evaluation, themselves requiring a human evaluation data set on which to evaluate the proposed metric. How-ever, inappropriate statistics are often applied. For example, Pang et al. (2020) propose a holistic metric to automatically evaluate four distinct aspects of dialogue, and a human evaluation experiment is deployed on Mechanical Turk using a 1-5 rating scale. The mean correlation between human assessors is reported as r = 0.61. However, mean correlations are unfortunately difficult to interpret, since correlation coefficients are not additive , averages calculated in the usual way cannot be assumed to reflect central tendency, and unfortunately, the distribution of correlations is not reported (Alexander, 1990). Mehri and Eskenazi (2020b) propose USR (Un-Supervised and Reference-free), an unsupervised model that predicts the quality of dialog for a range of criteria using various rating scales: understandable (0-1 rating scale), natural (1-3), maintains context (1-3), interesting (1-3), uses knowledge (0-1); overall quality (1-5). Despite human evaluation being carried out by experts inter-annotator agreement levels varied depending on criteria being measured, ranging from as low as 0.298. Additionally, although correlations between human assessments are reported as significant at p < 0.01, despite such statistics often being reported for correlations, they are unfortunately not very meaningful in terms of their impact on correlation interpretation and can be somewhat misleading. Contrary to common expectations, even small effect sizes (low r) can produce very low p-values (strong significance) in such tests. Aiming to achieve a significant correlation is an extremely low bar to reach in terms of consistency, since a low p-value in this case simply rejects the null hypothesis that the correlation is zero.\n\nIn addition to the above issues, human evaluation of dialogue systems rarely take into account the fact that differences in performance can occur simply by chance. The method of human evaluation we propose provides a means of applying standard tests for statistical significance to avoid concluding differences that are highly likely to have occurred simply by chance.\n\n References: \nRalph, A. 1990. A note on averaging correlations. In Bulletin of the Psychonomic Society. pp. 335--336\nAlexandrov, A. 2010. Characteristics of singleitem measures in likert scale format. In The Electronic Journal of Business Research Methods. pp. 1--12\nBarrault, L., Biesialska, M., Bojar, O., Costa-Juss\u00e0, M. R., Federmann, C., Graham, Y., Grundkiewicz, R., Haddow, B., Huck, M., Joanis, E., Kocmi, T., Koehn, P., Lo, C., Ljube\u0161i\u0107, N., Monz, C., Morishita, M., Nagata, M., and Nakazawa, T. Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (wmt20). In Proceedings of the Fifth Conference on Machine Translation. In Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (wmt20). In Proceedings of the Fifth Conference on Machine Translation. pp. 1--54\nBojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint-Amand, H., Soricut, R., Specia, L., and Tamchyna, A. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation. pp. 12--58\nBojar, O., Buck, C., Callison-Burch, C., Federmann, C., Haddow, B., Koehn, P., Monz, C., Post, M., Soricut, R., and Specia, L. 2013. Proceedings of the Eighth Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation. pp. 1--44\nCallison-Burch, C., Koehn, P., Monz, C., Post, M., Soricut, R., and Specia, L. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation. pp. 10--51\nCallison-Burch, C., Koehn, P., Monz, C., and Zaidan, O. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation. pp. 22--64\nDenkowski, M. and Lavie, A. 2011. Automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation. pp. 85--91\nDinan, E., Logacheva, V., Malykh, V., Miller, A. H., Shuster, K., Urbanek, J., Kiela, D., Szlam, A., Serban, I., and Lowe, R. 2019.\nCorr abs/1902.00098\nDinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston, J. 2018. Wizard of wikipedia: Knowledge-powered conversational agents. In Wizard of wikipedia: Knowledge-powered conversational agents. abs/1811.01241\nFinch, S. E. and Choi, J. D. 2020. Towards unified dialogue system evaluation: A comprehensive analysis of current evaluation protocols. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue. pp. 236--245\nGraham, Y., Baldwin, T., Moffat, A., and Zobel, J. 2013. Crowd-sourcing of human judgments of machine translation fluency. In Proceedings of the Australasian Language Technology Association Workshop 2013 (ALTA 2013). pp. 16--24\nHochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. In Neural Comput. pp. 1735--1780 10.1162/neco.1997.9.8.1735\nHumeau, S., Shuster, K., Lachaux, M., and Weston, J. 1905. Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring. In Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring.\nLasecki, W. S., Teevan, J., and Kamar, E. 2014. Information extraction and manipulation threats in crowd-powered systems. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing, CSCW '14. pp. 248--256 10.1145/2531602.2531733\nLin, C. and Hovy, E. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology. pp. 71--78\nLiu, Q., Ihler, A. T., and Steyvers, M. 2013. Scoring workers in crowdsourcing: How many control questions are enough?. In Advances in Neural Information Processing Systems.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.\nMiller, A. H., Fisch, A., Dodge, J., Amir-Hossein, Karimi, A., Bordes, J., and Weston 2016. Key-value memory networks for directly reading documents. In Key-value memory networks for directly reading documents.\nPang, B., Nijkamp, E., Han, W., Zhou, L., Liu, Y., and Tu, K. 2020. Towards holistic and automatic evaluation of open-domain dialogue generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 3619--3629 10.18653/v1/2020.acl-main.333\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02. pp. 311--318 10.3115/1073083.1073135\nRam, A., Prasad, R., Khatri, C., Venkatesh, A., Gabriel, R., Liu, Q., Nunn, J., Hedayatnia, B., Cheng, M., Nagar, A., and King, E. 2018. Gene Hwang, and Art Pettigrue. In Gene Hwang, and Art Pettigrue.\nSorodoc, I., Lau, J. H., Aletras, N., and Baldwin, T. 2017. Multimodal topic labelling. In Proceedings of the 15th Conference of the European Chapter. pp. 701--706\nValencia, S. Association for Computational Linguistics. In Association for Computational Linguistics.\nSutskever, I., Vinyals, O., Quoc, V., and Le 2014. Sequence to sequence learning with neural networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems. pp. 3104--3112\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser, L., Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., and Wang, W. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. In Oriol Vinyals. abs/1609.08144\nZhang, Y., Sun, S., Galley, M., Chen, Y., Brockett, C., Gao, X., Gao, J., Liu, J., and Dolan, B. 2020. DIALOGPT : Largescale generative pre-training for conversational response generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. pp. 270--278 10.18653/v1/2020.acl-demos.30"
            }
        ]
    },
    {
        "filename": "paper_4.txt",
        "label": "Unsupported claim",
        "text": "On the other hand, competitions that (for one reason or another) do not release data and evaluation techniques into the public domain have reported relative success in terms of human evaluation.",
        "start": 1202,
        "end": 1396,
        "spans_a": [
            {
                "filename": "paper_4.txt",
                "start": 1202,
                "end": 1396,
                "label": "Unsupported claim",
                "text": "On the other hand, competitions that (for one reason or another) do not release data and evaluation techniques into the public domain have reported relative success in terms of human evaluation.",
                "full_text": "Problems in Past Evaluations\n\nA common issue occurs that can potentially impact the validity of results is filtering the set of systems to be evaluated via automatic metric scores. Since metric scores are known to be a poor substitute for human assessment, this only results in the pos-sibility that the best system according to human judges is inadvertently filtered out at this stage. For example, ConvAI2 (Dinan et al., 2019) ranked models firstly using automatic metrics before top models according to metric scores were assessed by crowd-sourced workers on Mechanical Turk, while similarly in the sixth Dialog System Technology Challenge (DSTC6) systems were filtered according to metric scores prior to human evaluation.\n\nIn terms of the live evaluation, competitions such as Convai2 report such evaluations as highly challenging, with many of the resulting dialogues reported to be senseless, offensive, or simply not in line with instructions and ultimately live evaluation results have been discarded.\n\nDespite challenges, competitions that operate in the public domain, making data and evaluation techniques available to researchers (such as ourselves) should be applauded for such efforts.\n\nOn the other hand, competitions that (for one reason or another) do not release data and evaluation techniques into the public domain have reported relative success in terms of human evaluation. However until such methods can be accessed and independently verified through replication studies, they will unfortunately have little impact . The first Amazon Alexa Socialbot Grand Challenge required human assessors to score how coherent and engaging conversations were on a 1-5 rating scale by two distinct groups: volunteer Amazon employees (experts), and general Alexa users (crowds) (Ram et al., 2018), are reported to achieve a correlation of overall scores for the two types of human assessors at 0.93. The absolute average rating across all chatbots was reported to be 20% lower for experts compared to general users. In an additional effort to evaluate models, conversational user experience, coherence, engagement, domain coverage, topical diversity, and conversational depth were assessed (1-5 scale), with combined scores reported to correlate with those of general users at r = 0.66. In addition to methods and data not being publicly available, correlations are difficult to interpret since no detail is provided about the number of judgments on which the correlation is calculated for example.\n\nIn addition to competitions that generally aim to include human evaluation of systems, automatic metrics are often proposed for dialogue evaluation, themselves requiring a human evaluation data set on which to evaluate the proposed metric. How-ever, inappropriate statistics are often applied. For example, Pang et al. (2020) propose a holistic metric to automatically evaluate four distinct aspects of dialogue, and a human evaluation experiment is deployed on Mechanical Turk using a 1-5 rating scale. The mean correlation between human assessors is reported as r = 0.61. However, mean correlations are unfortunately difficult to interpret, since correlation coefficients are not additive , averages calculated in the usual way cannot be assumed to reflect central tendency, and unfortunately, the distribution of correlations is not reported (Alexander, 1990). Mehri and Eskenazi (2020b) propose USR (Un-Supervised and Reference-free), an unsupervised model that predicts the quality of dialog for a range of criteria using various rating scales: understandable (0-1 rating scale), natural (1-3), maintains context (1-3), interesting (1-3), uses knowledge (0-1); overall quality (1-5). Despite human evaluation being carried out by experts inter-annotator agreement levels varied depending on criteria being measured, ranging from as low as 0.298. Additionally, although correlations between human assessments are reported as significant at p < 0.01, despite such statistics often being reported for correlations, they are unfortunately not very meaningful in terms of their impact on correlation interpretation and can be somewhat misleading. Contrary to common expectations, even small effect sizes (low r) can produce very low p-values (strong significance) in such tests. Aiming to achieve a significant correlation is an extremely low bar to reach in terms of consistency, since a low p-value in this case simply rejects the null hypothesis that the correlation is zero.\n\nIn addition to the above issues, human evaluation of dialogue systems rarely take into account the fact that differences in performance can occur simply by chance. The method of human evaluation we propose provides a means of applying standard tests for statistical significance to avoid concluding differences that are highly likely to have occurred simply by chance.\n\n References: \nRalph, A. 1990. A note on averaging correlations. In Bulletin of the Psychonomic Society. pp. 335--336\nAlexandrov, A. 2010. Characteristics of singleitem measures in likert scale format. In The Electronic Journal of Business Research Methods. pp. 1--12\nBarrault, L., Biesialska, M., Bojar, O., Costa-Juss\u00e0, M. R., Federmann, C., Graham, Y., Grundkiewicz, R., Haddow, B., Huck, M., Joanis, E., Kocmi, T., Koehn, P., Lo, C., Ljube\u0161i\u0107, N., Monz, C., Morishita, M., Nagata, M., and Nakazawa, T. Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (wmt20). In Proceedings of the Fifth Conference on Machine Translation. In Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (wmt20). In Proceedings of the Fifth Conference on Machine Translation. pp. 1--54\nBojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint-Amand, H., Soricut, R., Specia, L., and Tamchyna, A. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation. pp. 12--58\nBojar, O., Buck, C., Callison-Burch, C., Federmann, C., Haddow, B., Koehn, P., Monz, C., Post, M., Soricut, R., and Specia, L. 2013. Proceedings of the Eighth Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation. pp. 1--44\nCallison-Burch, C., Koehn, P., Monz, C., Post, M., Soricut, R., and Specia, L. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation. pp. 10--51\nCallison-Burch, C., Koehn, P., Monz, C., and Zaidan, O. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation. pp. 22--64\nDenkowski, M. and Lavie, A. 2011. Automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation. pp. 85--91\nDinan, E., Logacheva, V., Malykh, V., Miller, A. H., Shuster, K., Urbanek, J., Kiela, D., Szlam, A., Serban, I., and Lowe, R. 2019.\nCorr abs/1902.00098\nDinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston, J. 2018. Wizard of wikipedia: Knowledge-powered conversational agents. In Wizard of wikipedia: Knowledge-powered conversational agents. abs/1811.01241\nFinch, S. E. and Choi, J. D. 2020. Towards unified dialogue system evaluation: A comprehensive analysis of current evaluation protocols. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue. pp. 236--245\nGraham, Y., Baldwin, T., Moffat, A., and Zobel, J. 2013. Crowd-sourcing of human judgments of machine translation fluency. In Proceedings of the Australasian Language Technology Association Workshop 2013 (ALTA 2013). pp. 16--24\nHochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. In Neural Comput. pp. 1735--1780 10.1162/neco.1997.9.8.1735\nHumeau, S., Shuster, K., Lachaux, M., and Weston, J. 1905. Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring. In Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring.\nLasecki, W. S., Teevan, J., and Kamar, E. 2014. Information extraction and manipulation threats in crowd-powered systems. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing, CSCW '14. pp. 248--256 10.1145/2531602.2531733\nLin, C. and Hovy, E. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology. pp. 71--78\nLiu, Q., Ihler, A. T., and Steyvers, M. 2013. Scoring workers in crowdsourcing: How many control questions are enough?. In Advances in Neural Information Processing Systems.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.\nMiller, A. H., Fisch, A., Dodge, J., Amir-Hossein, Karimi, A., Bordes, J., and Weston 2016. Key-value memory networks for directly reading documents. In Key-value memory networks for directly reading documents.\nPang, B., Nijkamp, E., Han, W., Zhou, L., Liu, Y., and Tu, K. 2020. Towards holistic and automatic evaluation of open-domain dialogue generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 3619--3629 10.18653/v1/2020.acl-main.333\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02. pp. 311--318 10.3115/1073083.1073135\nRam, A., Prasad, R., Khatri, C., Venkatesh, A., Gabriel, R., Liu, Q., Nunn, J., Hedayatnia, B., Cheng, M., Nagar, A., and King, E. 2018. Gene Hwang, and Art Pettigrue. In Gene Hwang, and Art Pettigrue.\nSorodoc, I., Lau, J. H., Aletras, N., and Baldwin, T. 2017. Multimodal topic labelling. In Proceedings of the 15th Conference of the European Chapter. pp. 701--706\nValencia, S. Association for Computational Linguistics. In Association for Computational Linguistics.\nSutskever, I., Vinyals, O., Quoc, V., and Le 2014. Sequence to sequence learning with neural networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems. pp. 3104--3112\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser, L., Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., and Wang, W. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. In Oriol Vinyals. abs/1609.08144\nZhang, Y., Sun, S., Galley, M., Chen, Y., Brockett, C., Gao, X., Gao, J., Liu, J., and Dolan, B. 2020. DIALOGPT : Largescale generative pre-training for conversational response generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. pp. 270--278 10.18653/v1/2020.acl-demos.30"
            }
        ],
        "spans_b": [
            {
                "filename": "paper_4.txt",
                "start": 1202,
                "end": 1396,
                "label": "Unsupported claim",
                "text": "On the other hand, competitions that (for one reason or another) do not release data and evaluation techniques into the public domain have reported relative success in terms of human evaluation.",
                "full_text": "Problems in Past Evaluations\n\nA common issue occurs that can potentially impact the validity of results is filtering the set of systems to be evaluated via automatic metric scores. Since metric scores are known to be a poor substitute for human assessment, this only results in the pos-sibility that the best system according to human judges is inadvertently filtered out at this stage. For example, ConvAI2 (Dinan et al., 2019) ranked models firstly using automatic metrics before top models according to metric scores were assessed by crowd-sourced workers on Mechanical Turk, while similarly in the sixth Dialog System Technology Challenge (DSTC6) systems were filtered according to metric scores prior to human evaluation.\n\nIn terms of the live evaluation, competitions such as Convai2 report such evaluations as highly challenging, with many of the resulting dialogues reported to be senseless, offensive, or simply not in line with instructions and ultimately live evaluation results have been discarded.\n\nDespite challenges, competitions that operate in the public domain, making data and evaluation techniques available to researchers (such as ourselves) should be applauded for such efforts.\n\nOn the other hand, competitions that (for one reason or another) do not release data and evaluation techniques into the public domain have reported relative success in terms of human evaluation. However until such methods can be accessed and independently verified through replication studies, they will unfortunately have little impact . The first Amazon Alexa Socialbot Grand Challenge required human assessors to score how coherent and engaging conversations were on a 1-5 rating scale by two distinct groups: volunteer Amazon employees (experts), and general Alexa users (crowds) (Ram et al., 2018), are reported to achieve a correlation of overall scores for the two types of human assessors at 0.93. The absolute average rating across all chatbots was reported to be 20% lower for experts compared to general users. In an additional effort to evaluate models, conversational user experience, coherence, engagement, domain coverage, topical diversity, and conversational depth were assessed (1-5 scale), with combined scores reported to correlate with those of general users at r = 0.66. In addition to methods and data not being publicly available, correlations are difficult to interpret since no detail is provided about the number of judgments on which the correlation is calculated for example.\n\nIn addition to competitions that generally aim to include human evaluation of systems, automatic metrics are often proposed for dialogue evaluation, themselves requiring a human evaluation data set on which to evaluate the proposed metric. How-ever, inappropriate statistics are often applied. For example, Pang et al. (2020) propose a holistic metric to automatically evaluate four distinct aspects of dialogue, and a human evaluation experiment is deployed on Mechanical Turk using a 1-5 rating scale. The mean correlation between human assessors is reported as r = 0.61. However, mean correlations are unfortunately difficult to interpret, since correlation coefficients are not additive , averages calculated in the usual way cannot be assumed to reflect central tendency, and unfortunately, the distribution of correlations is not reported (Alexander, 1990). Mehri and Eskenazi (2020b) propose USR (Un-Supervised and Reference-free), an unsupervised model that predicts the quality of dialog for a range of criteria using various rating scales: understandable (0-1 rating scale), natural (1-3), maintains context (1-3), interesting (1-3), uses knowledge (0-1); overall quality (1-5). Despite human evaluation being carried out by experts inter-annotator agreement levels varied depending on criteria being measured, ranging from as low as 0.298. Additionally, although correlations between human assessments are reported as significant at p < 0.01, despite such statistics often being reported for correlations, they are unfortunately not very meaningful in terms of their impact on correlation interpretation and can be somewhat misleading. Contrary to common expectations, even small effect sizes (low r) can produce very low p-values (strong significance) in such tests. Aiming to achieve a significant correlation is an extremely low bar to reach in terms of consistency, since a low p-value in this case simply rejects the null hypothesis that the correlation is zero.\n\nIn addition to the above issues, human evaluation of dialogue systems rarely take into account the fact that differences in performance can occur simply by chance. The method of human evaluation we propose provides a means of applying standard tests for statistical significance to avoid concluding differences that are highly likely to have occurred simply by chance.\n\n References: \nRalph, A. 1990. A note on averaging correlations. In Bulletin of the Psychonomic Society. pp. 335--336\nAlexandrov, A. 2010. Characteristics of singleitem measures in likert scale format. In The Electronic Journal of Business Research Methods. pp. 1--12\nBarrault, L., Biesialska, M., Bojar, O., Costa-Juss\u00e0, M. R., Federmann, C., Graham, Y., Grundkiewicz, R., Haddow, B., Huck, M., Joanis, E., Kocmi, T., Koehn, P., Lo, C., Ljube\u0161i\u0107, N., Monz, C., Morishita, M., Nagata, M., and Nakazawa, T. Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (wmt20). In Proceedings of the Fifth Conference on Machine Translation. In Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (wmt20). In Proceedings of the Fifth Conference on Machine Translation. pp. 1--54\nBojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint-Amand, H., Soricut, R., Specia, L., and Tamchyna, A. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation. pp. 12--58\nBojar, O., Buck, C., Callison-Burch, C., Federmann, C., Haddow, B., Koehn, P., Monz, C., Post, M., Soricut, R., and Specia, L. 2013. Proceedings of the Eighth Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation. pp. 1--44\nCallison-Burch, C., Koehn, P., Monz, C., Post, M., Soricut, R., and Specia, L. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation. pp. 10--51\nCallison-Burch, C., Koehn, P., Monz, C., and Zaidan, O. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation. pp. 22--64\nDenkowski, M. and Lavie, A. 2011. Automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation. pp. 85--91\nDinan, E., Logacheva, V., Malykh, V., Miller, A. H., Shuster, K., Urbanek, J., Kiela, D., Szlam, A., Serban, I., and Lowe, R. 2019.\nCorr abs/1902.00098\nDinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston, J. 2018. Wizard of wikipedia: Knowledge-powered conversational agents. In Wizard of wikipedia: Knowledge-powered conversational agents. abs/1811.01241\nFinch, S. E. and Choi, J. D. 2020. Towards unified dialogue system evaluation: A comprehensive analysis of current evaluation protocols. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue. pp. 236--245\nGraham, Y., Baldwin, T., Moffat, A., and Zobel, J. 2013. Crowd-sourcing of human judgments of machine translation fluency. In Proceedings of the Australasian Language Technology Association Workshop 2013 (ALTA 2013). pp. 16--24\nHochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. In Neural Comput. pp. 1735--1780 10.1162/neco.1997.9.8.1735\nHumeau, S., Shuster, K., Lachaux, M., and Weston, J. 1905. Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring. In Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring.\nLasecki, W. S., Teevan, J., and Kamar, E. 2014. Information extraction and manipulation threats in crowd-powered systems. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing, CSCW '14. pp. 248--256 10.1145/2531602.2531733\nLin, C. and Hovy, E. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology. pp. 71--78\nLiu, Q., Ihler, A. T., and Steyvers, M. 2013. Scoring workers in crowdsourcing: How many control questions are enough?. In Advances in Neural Information Processing Systems.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.\nMiller, A. H., Fisch, A., Dodge, J., Amir-Hossein, Karimi, A., Bordes, J., and Weston 2016. Key-value memory networks for directly reading documents. In Key-value memory networks for directly reading documents.\nPang, B., Nijkamp, E., Han, W., Zhou, L., Liu, Y., and Tu, K. 2020. Towards holistic and automatic evaluation of open-domain dialogue generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 3619--3629 10.18653/v1/2020.acl-main.333\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02. pp. 311--318 10.3115/1073083.1073135\nRam, A., Prasad, R., Khatri, C., Venkatesh, A., Gabriel, R., Liu, Q., Nunn, J., Hedayatnia, B., Cheng, M., Nagar, A., and King, E. 2018. Gene Hwang, and Art Pettigrue. In Gene Hwang, and Art Pettigrue.\nSorodoc, I., Lau, J. H., Aletras, N., and Baldwin, T. 2017. Multimodal topic labelling. In Proceedings of the 15th Conference of the European Chapter. pp. 701--706\nValencia, S. Association for Computational Linguistics. In Association for Computational Linguistics.\nSutskever, I., Vinyals, O., Quoc, V., and Le 2014. Sequence to sequence learning with neural networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems. pp. 3104--3112\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser, L., Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., and Wang, W. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. In Oriol Vinyals. abs/1609.08144\nZhang, Y., Sun, S., Galley, M., Chen, Y., Brockett, C., Gao, X., Gao, J., Liu, J., and Dolan, B. 2020. DIALOGPT : Largescale generative pre-training for conversational response generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. pp. 270--278 10.18653/v1/2020.acl-demos.30"
            }
        ]
    },
    {
        "filename": "paper_4.txt",
        "label": "Unsupported claim",
        "text": "while similarly in the sixth Dialog System Technology Challenge (DSTC6) systems were filtered according to metric scores prior to human evaluation.",
        "start": 595,
        "end": 726,
        "spans_a": [
            {
                "filename": "paper_4.txt",
                "start": 579,
                "end": 726,
                "label": "Unsupported claim",
                "text": "while similarly in the sixth Dialog System Technology Challenge (DSTC6) systems were filtered according to metric scores prior to human evaluation.",
                "full_text": "Problems in Past Evaluations\n\nA common issue occurs that can potentially impact the validity of results is filtering the set of systems to be evaluated via automatic metric scores. Since metric scores are known to be a poor substitute for human assessment, this only results in the pos-sibility that the best system according to human judges is inadvertently filtered out at this stage. For example, ConvAI2 (Dinan et al., 2019) ranked models firstly using automatic metrics before top models according to metric scores were assessed by crowd-sourced workers on Mechanical Turk, while similarly in the sixth Dialog System Technology Challenge (DSTC6) systems were filtered according to metric scores prior to human evaluation.\n\nIn terms of the live evaluation, competitions such as Convai2 report such evaluations as highly challenging, with many of the resulting dialogues reported to be senseless, offensive, or simply not in line with instructions and ultimately live evaluation results have been discarded.\n\nDespite challenges, competitions that operate in the public domain, making data and evaluation techniques available to researchers (such as ourselves) should be applauded for such efforts.\n\nOn the other hand, competitions that (for one reason or another) do not release data and evaluation techniques into the public domain have reported relative success in terms of human evaluation. However until such methods can be accessed and independently verified through replication studies, they will unfortunately have little impact . The first Amazon Alexa Socialbot Grand Challenge required human assessors to score how coherent and engaging conversations were on a 1-5 rating scale by two distinct groups: volunteer Amazon employees (experts), and general Alexa users (crowds) (Ram et al., 2018), are reported to achieve a correlation of overall scores for the two types of human assessors at 0.93. The absolute average rating across all chatbots was reported to be 20% lower for experts compared to general users. In an additional effort to evaluate models, conversational user experience, coherence, engagement, domain coverage, topical diversity, and conversational depth were assessed (1-5 scale), with combined scores reported to correlate with those of general users at r = 0.66. In addition to methods and data not being publicly available, correlations are difficult to interpret since no detail is provided about the number of judgments on which the correlation is calculated for example.\n\nIn addition to competitions that generally aim to include human evaluation of systems, automatic metrics are often proposed for dialogue evaluation, themselves requiring a human evaluation data set on which to evaluate the proposed metric. How-ever, inappropriate statistics are often applied. For example, Pang et al. (2020) propose a holistic metric to automatically evaluate four distinct aspects of dialogue, and a human evaluation experiment is deployed on Mechanical Turk using a 1-5 rating scale. The mean correlation between human assessors is reported as r = 0.61. However, mean correlations are unfortunately difficult to interpret, since correlation coefficients are not additive , averages calculated in the usual way cannot be assumed to reflect central tendency, and unfortunately, the distribution of correlations is not reported (Alexander, 1990). Mehri and Eskenazi (2020b) propose USR (Un-Supervised and Reference-free), an unsupervised model that predicts the quality of dialog for a range of criteria using various rating scales: understandable (0-1 rating scale), natural (1-3), maintains context (1-3), interesting (1-3), uses knowledge (0-1); overall quality (1-5). Despite human evaluation being carried out by experts inter-annotator agreement levels varied depending on criteria being measured, ranging from as low as 0.298. Additionally, although correlations between human assessments are reported as significant at p < 0.01, despite such statistics often being reported for correlations, they are unfortunately not very meaningful in terms of their impact on correlation interpretation and can be somewhat misleading. Contrary to common expectations, even small effect sizes (low r) can produce very low p-values (strong significance) in such tests. Aiming to achieve a significant correlation is an extremely low bar to reach in terms of consistency, since a low p-value in this case simply rejects the null hypothesis that the correlation is zero.\n\nIn addition to the above issues, human evaluation of dialogue systems rarely take into account the fact that differences in performance can occur simply by chance. The method of human evaluation we propose provides a means of applying standard tests for statistical significance to avoid concluding differences that are highly likely to have occurred simply by chance.\n\n References: \nRalph, A. 1990. A note on averaging correlations. In Bulletin of the Psychonomic Society. pp. 335--336\nAlexandrov, A. 2010. Characteristics of singleitem measures in likert scale format. In The Electronic Journal of Business Research Methods. pp. 1--12\nBarrault, L., Biesialska, M., Bojar, O., Costa-Juss\u00e0, M. R., Federmann, C., Graham, Y., Grundkiewicz, R., Haddow, B., Huck, M., Joanis, E., Kocmi, T., Koehn, P., Lo, C., Ljube\u0161i\u0107, N., Monz, C., Morishita, M., Nagata, M., and Nakazawa, T. Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (wmt20). In Proceedings of the Fifth Conference on Machine Translation. In Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (wmt20). In Proceedings of the Fifth Conference on Machine Translation. pp. 1--54\nBojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint-Amand, H., Soricut, R., Specia, L., and Tamchyna, A. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation. pp. 12--58\nBojar, O., Buck, C., Callison-Burch, C., Federmann, C., Haddow, B., Koehn, P., Monz, C., Post, M., Soricut, R., and Specia, L. 2013. Proceedings of the Eighth Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation. pp. 1--44\nCallison-Burch, C., Koehn, P., Monz, C., Post, M., Soricut, R., and Specia, L. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation. pp. 10--51\nCallison-Burch, C., Koehn, P., Monz, C., and Zaidan, O. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation. pp. 22--64\nDenkowski, M. and Lavie, A. 2011. Automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation. pp. 85--91\nDinan, E., Logacheva, V., Malykh, V., Miller, A. H., Shuster, K., Urbanek, J., Kiela, D., Szlam, A., Serban, I., and Lowe, R. 2019.\nCorr abs/1902.00098\nDinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston, J. 2018. Wizard of wikipedia: Knowledge-powered conversational agents. In Wizard of wikipedia: Knowledge-powered conversational agents. abs/1811.01241\nFinch, S. E. and Choi, J. D. 2020. Towards unified dialogue system evaluation: A comprehensive analysis of current evaluation protocols. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue. pp. 236--245\nGraham, Y., Baldwin, T., Moffat, A., and Zobel, J. 2013. Crowd-sourcing of human judgments of machine translation fluency. In Proceedings of the Australasian Language Technology Association Workshop 2013 (ALTA 2013). pp. 16--24\nHochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. In Neural Comput. pp. 1735--1780 10.1162/neco.1997.9.8.1735\nHumeau, S., Shuster, K., Lachaux, M., and Weston, J. 1905. Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring. In Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring.\nLasecki, W. S., Teevan, J., and Kamar, E. 2014. Information extraction and manipulation threats in crowd-powered systems. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing, CSCW '14. pp. 248--256 10.1145/2531602.2531733\nLin, C. and Hovy, E. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology. pp. 71--78\nLiu, Q., Ihler, A. T., and Steyvers, M. 2013. Scoring workers in crowdsourcing: How many control questions are enough?. In Advances in Neural Information Processing Systems.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.\nMiller, A. H., Fisch, A., Dodge, J., Amir-Hossein, Karimi, A., Bordes, J., and Weston 2016. Key-value memory networks for directly reading documents. In Key-value memory networks for directly reading documents.\nPang, B., Nijkamp, E., Han, W., Zhou, L., Liu, Y., and Tu, K. 2020. Towards holistic and automatic evaluation of open-domain dialogue generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 3619--3629 10.18653/v1/2020.acl-main.333\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02. pp. 311--318 10.3115/1073083.1073135\nRam, A., Prasad, R., Khatri, C., Venkatesh, A., Gabriel, R., Liu, Q., Nunn, J., Hedayatnia, B., Cheng, M., Nagar, A., and King, E. 2018. Gene Hwang, and Art Pettigrue. In Gene Hwang, and Art Pettigrue.\nSorodoc, I., Lau, J. H., Aletras, N., and Baldwin, T. 2017. Multimodal topic labelling. In Proceedings of the 15th Conference of the European Chapter. pp. 701--706\nValencia, S. Association for Computational Linguistics. In Association for Computational Linguistics.\nSutskever, I., Vinyals, O., Quoc, V., and Le 2014. Sequence to sequence learning with neural networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems. pp. 3104--3112\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser, L., Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., and Wang, W. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. In Oriol Vinyals. abs/1609.08144\nZhang, Y., Sun, S., Galley, M., Chen, Y., Brockett, C., Gao, X., Gao, J., Liu, J., and Dolan, B. 2020. DIALOGPT : Largescale generative pre-training for conversational response generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. pp. 270--278 10.18653/v1/2020.acl-demos.30"
            }
        ],
        "spans_b": [
            {
                "filename": "paper_4.txt",
                "start": 595,
                "end": 726,
                "label": "Unsupported claim",
                "text": "in the sixth Dialog System Technology Challenge (DSTC6) systems were filtered according to metric scores prior to human evaluation.",
                "full_text": "Problems in Past Evaluations\n\nA common issue occurs that can potentially impact the validity of results is filtering the set of systems to be evaluated via automatic metric scores. Since metric scores are known to be a poor substitute for human assessment, this only results in the pos-sibility that the best system according to human judges is inadvertently filtered out at this stage. For example, ConvAI2 (Dinan et al., 2019) ranked models firstly using automatic metrics before top models according to metric scores were assessed by crowd-sourced workers on Mechanical Turk, while similarly in the sixth Dialog System Technology Challenge (DSTC6) systems were filtered according to metric scores prior to human evaluation.\n\nIn terms of the live evaluation, competitions such as Convai2 report such evaluations as highly challenging, with many of the resulting dialogues reported to be senseless, offensive, or simply not in line with instructions and ultimately live evaluation results have been discarded.\n\nDespite challenges, competitions that operate in the public domain, making data and evaluation techniques available to researchers (such as ourselves) should be applauded for such efforts.\n\nOn the other hand, competitions that (for one reason or another) do not release data and evaluation techniques into the public domain have reported relative success in terms of human evaluation. However until such methods can be accessed and independently verified through replication studies, they will unfortunately have little impact . The first Amazon Alexa Socialbot Grand Challenge required human assessors to score how coherent and engaging conversations were on a 1-5 rating scale by two distinct groups: volunteer Amazon employees (experts), and general Alexa users (crowds) (Ram et al., 2018), are reported to achieve a correlation of overall scores for the two types of human assessors at 0.93. The absolute average rating across all chatbots was reported to be 20% lower for experts compared to general users. In an additional effort to evaluate models, conversational user experience, coherence, engagement, domain coverage, topical diversity, and conversational depth were assessed (1-5 scale), with combined scores reported to correlate with those of general users at r = 0.66. In addition to methods and data not being publicly available, correlations are difficult to interpret since no detail is provided about the number of judgments on which the correlation is calculated for example.\n\nIn addition to competitions that generally aim to include human evaluation of systems, automatic metrics are often proposed for dialogue evaluation, themselves requiring a human evaluation data set on which to evaluate the proposed metric. How-ever, inappropriate statistics are often applied. For example, Pang et al. (2020) propose a holistic metric to automatically evaluate four distinct aspects of dialogue, and a human evaluation experiment is deployed on Mechanical Turk using a 1-5 rating scale. The mean correlation between human assessors is reported as r = 0.61. However, mean correlations are unfortunately difficult to interpret, since correlation coefficients are not additive , averages calculated in the usual way cannot be assumed to reflect central tendency, and unfortunately, the distribution of correlations is not reported (Alexander, 1990). Mehri and Eskenazi (2020b) propose USR (Un-Supervised and Reference-free), an unsupervised model that predicts the quality of dialog for a range of criteria using various rating scales: understandable (0-1 rating scale), natural (1-3), maintains context (1-3), interesting (1-3), uses knowledge (0-1); overall quality (1-5). Despite human evaluation being carried out by experts inter-annotator agreement levels varied depending on criteria being measured, ranging from as low as 0.298. Additionally, although correlations between human assessments are reported as significant at p < 0.01, despite such statistics often being reported for correlations, they are unfortunately not very meaningful in terms of their impact on correlation interpretation and can be somewhat misleading. Contrary to common expectations, even small effect sizes (low r) can produce very low p-values (strong significance) in such tests. Aiming to achieve a significant correlation is an extremely low bar to reach in terms of consistency, since a low p-value in this case simply rejects the null hypothesis that the correlation is zero.\n\nIn addition to the above issues, human evaluation of dialogue systems rarely take into account the fact that differences in performance can occur simply by chance. The method of human evaluation we propose provides a means of applying standard tests for statistical significance to avoid concluding differences that are highly likely to have occurred simply by chance.\n\n References: \nRalph, A. 1990. A note on averaging correlations. In Bulletin of the Psychonomic Society. pp. 335--336\nAlexandrov, A. 2010. Characteristics of singleitem measures in likert scale format. In The Electronic Journal of Business Research Methods. pp. 1--12\nBarrault, L., Biesialska, M., Bojar, O., Costa-Juss\u00e0, M. R., Federmann, C., Graham, Y., Grundkiewicz, R., Haddow, B., Huck, M., Joanis, E., Kocmi, T., Koehn, P., Lo, C., Ljube\u0161i\u0107, N., Monz, C., Morishita, M., Nagata, M., and Nakazawa, T. Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (wmt20). In Proceedings of the Fifth Conference on Machine Translation. In Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (wmt20). In Proceedings of the Fifth Conference on Machine Translation. pp. 1--54\nBojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina, P., Post, M., Saint-Amand, H., Soricut, R., Specia, L., and Tamchyna, A. 2014. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation. pp. 12--58\nBojar, O., Buck, C., Callison-Burch, C., Federmann, C., Haddow, B., Koehn, P., Monz, C., Post, M., Soricut, R., and Specia, L. 2013. Proceedings of the Eighth Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation. pp. 1--44\nCallison-Burch, C., Koehn, P., Monz, C., Post, M., Soricut, R., and Specia, L. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation. pp. 10--51\nCallison-Burch, C., Koehn, P., Monz, C., and Zaidan, O. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation. pp. 22--64\nDenkowski, M. and Lavie, A. 2011. Automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation. pp. 85--91\nDinan, E., Logacheva, V., Malykh, V., Miller, A. H., Shuster, K., Urbanek, J., Kiela, D., Szlam, A., Serban, I., and Lowe, R. 2019.\nCorr abs/1902.00098\nDinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston, J. 2018. Wizard of wikipedia: Knowledge-powered conversational agents. In Wizard of wikipedia: Knowledge-powered conversational agents. abs/1811.01241\nFinch, S. E. and Choi, J. D. 2020. Towards unified dialogue system evaluation: A comprehensive analysis of current evaluation protocols. In Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue. pp. 236--245\nGraham, Y., Baldwin, T., Moffat, A., and Zobel, J. 2013. Crowd-sourcing of human judgments of machine translation fluency. In Proceedings of the Australasian Language Technology Association Workshop 2013 (ALTA 2013). pp. 16--24\nHochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. In Neural Comput. pp. 1735--1780 10.1162/neco.1997.9.8.1735\nHumeau, S., Shuster, K., Lachaux, M., and Weston, J. 1905. Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring. In Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring.\nLasecki, W. S., Teevan, J., and Kamar, E. 2014. Information extraction and manipulation threats in crowd-powered systems. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work & Social Computing, CSCW '14. pp. 248--256 10.1145/2531602.2531733\nLin, C. and Hovy, E. 2003. Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology. pp. 71--78\nLiu, Q., Ihler, A. T., and Steyvers, M. 2013. Scoring workers in crowdsourcing: How many control questions are enough?. In Advances in Neural Information Processing Systems.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.\nMiller, A. H., Fisch, A., Dodge, J., Amir-Hossein, Karimi, A., Bordes, J., and Weston 2016. Key-value memory networks for directly reading documents. In Key-value memory networks for directly reading documents.\nPang, B., Nijkamp, E., Han, W., Zhou, L., Liu, Y., and Tu, K. 2020. Towards holistic and automatic evaluation of open-domain dialogue generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 3619--3629 10.18653/v1/2020.acl-main.333\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02. pp. 311--318 10.3115/1073083.1073135\nRam, A., Prasad, R., Khatri, C., Venkatesh, A., Gabriel, R., Liu, Q., Nunn, J., Hedayatnia, B., Cheng, M., Nagar, A., and King, E. 2018. Gene Hwang, and Art Pettigrue. In Gene Hwang, and Art Pettigrue.\nSorodoc, I., Lau, J. H., Aletras, N., and Baldwin, T. 2017. Multimodal topic labelling. In Proceedings of the 15th Conference of the European Chapter. pp. 701--706\nValencia, S. Association for Computational Linguistics. In Association for Computational Linguistics.\nSutskever, I., Vinyals, O., Quoc, V., and Le 2014. Sequence to sequence learning with neural networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems. pp. 3104--3112\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser, L., Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., and Wang, W. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. In Oriol Vinyals. abs/1609.08144\nZhang, Y., Sun, S., Galley, M., Chen, Y., Brockett, C., Gao, X., Gao, J., Liu, J., and Dolan, B. 2020. DIALOGPT : Largescale generative pre-training for conversational response generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. pp. 270--278 10.18653/v1/2020.acl-demos.30"
            }
        ]
    },
    {
        "filename": "paper_5.txt",
        "label": "Unsupported claim",
        "text": "ZuCo corpus",
        "start": 2365,
        "end": 2376,
        "spans_a": [
            {
                "filename": "paper_5.txt",
                "start": 2365,
                "end": 2376,
                "label": "Unsupported claim",
                "text": "ZuCo corpus",
                "full_text": "Introduction\n\nThe usefulness of learned self-attention functions often correlates with how well it aligns with human attention (Das et al., 2016;Klerke et al., 2016;Barrett et al., 2018;Klerke and Plank, 2019). In this paper, we evaluate how well attention flow (Abnar and Zuidema, 2020) in large language models, namely BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020), aligns with human eye fixations during task-specific reading, compared to other shallow sequence labeling models (Lecun and Bengio, 1995;Vaswani et al., 2017) and a classic, heuristic model of human reading (Reichle et al., 2003). We compare the learned attention functions and the heuristic model across two task-specific English reading tasks, namely sentiment analysis (SST movie reviews) and relation extraction (Wikipedia), as well as natural reading, using a publicly available data set with eye-tracking recordings of native speakers of English (Hollenstein et al., 2018).\n\nContributions We compare human and model attention patterns on both sentiment reading and relation extraction tasks. In our analysis, we compare human attention to pre-trained Transformers (BERT, RoBERTa and T5), from-scratch training of two shallow sequence labeling architectures (Lecun and Bengio, 1995; Vaswani et al., 2017), as well as to a frequency baseline and a heuristic, cognitively inspired model of human reading called the E-Z Reader (Reichle et al., 2003). We find that the heuristic model correlates well with human reading, as has been reported in Sood et al. (2020b). However when we apply attention flow (Abnar and Zuidema, 2020), the pre-trained Transformer models also reach comparable levels of correlation strength. Further fine-tuning experiments on BERT did not result in increased correlation to human fixations. To understand what drives the differences between models, we perform an in-depth analysis of the effect of word predictability and POS tags on correlation strength. It reveals that Transformer models do not accurately capture tail phenomena for hard-to-predict words (in contrast to the E-Z Reader) and that Transformer attention flow shows comparably weak correlation on (proper) nouns while the E-Z Reader predicts importance of these more accurately, especially on the sentiment reading task. In addition we investigate a subset of the ZuCo corpus for which aligned task-specific and natural reading data is available and find that Transformers correlate stronger to natural reading patterns. But how faithful are these different attention patterns at producing correct task-classification on a state-of-the-art NLP model? We test this via an input reduction experiment on task-tuned BERT models which highlights the trade-off between a model\u2019s faithfulness and sparsity when comparing importance scores to human attention, i.e., less sparse (higher entropy) attention vectors seem to be less faithful with respect to model predictions. Our code is available at github.com/anon.\n\n References: \nAbdou, M., Kulmizev, A., Hill, F., Low, D. M., and S\u00f8gaard, A. 2019. Higher-order comparisons of sentence encoder representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5838--5845 10.18653/v1/D19-1593\nAbnar, S. and Zuidema, W. 2020. Quantifying attention flow in transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4190--4197 10.18653/v1/2020.acl-main.385\nArras, L., Horn, F., Montavon, G., M\u00fcller, K., and Samek, W. 2016. Explaining predictions of non-linear classifiers in NLP. In Proceedings of the 1st Workshop on Representation Learning for NLP. pp. 1--7 10.18653/v1/W16-1601\nArras, L., Horn, F., Montavon, G., M\u00fcller, K., and Samek, W. 2017. What is relevant in a text document?\": An interpretable machine learning approach. In PLOS ONE. pp. 1--23 10.1371/journal.pone.0181142\nBach, S., Binder, A., Montavon, G., Klauschen, F., M\u00fcller, K., and Samek, W. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. In PLoS ONE. pp. e0130140 10.1371/journal.pone.0130140\nBarrett, M., Bingel, J., Hollenstein, N., Rei, M., and S\u00f8gaard, A. 2018. Sequence classification with human attention. In Proceedings of the 22nd Conference on Computational Natural Language Learning. pp. 302--312 10.18653/v1/K18-1030\nBarrett, M. and S\u00f8gaard, A. 2015. Reading behavior predicts syntactic categories. In Proceedings of the Nineteenth Conference on Computational Natural Language Learning. pp. 345--349 10.18653/v1/K15-1038\nBarrett, M. and S\u00f8gaard, A. 2015. Using reading behavior to predict grammatical functions. In Proceedings of the Sixth Workshop on Cognitive Aspects of Computational Language Learning. pp. 1--5 10.18653/v1/W15-2401\nBorji, A. and Itti, L. 2014. Defending yarbus: eye movements reveal observers' task. In Journal of vision. pp. 29 10.1167/14.3.29\nChelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., and Robinson, T. 2013. One billion word benchmark for measuring progress in statistical language modeling. In One billion word benchmark for measuring progress in statistical language modeling.\nChoi, W., Rutvik, H., Desai, J.M., and Henderson 2014. The neural substrates of natural reading: a comparison of normal and nonword text using eyetracking and fmri. In Frontiers in human neuroscience. pp. 1024\nChrupa\u0142a, G. and Alishahi, A. 2019. Correlating neural and symbolic representations of language. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 2952--2962 10.18653/v1/P19-1283\nChurch, K. and Liberman, M. 2021. The future of computational linguistics: On beyond alchemy. In Frontiers in Artificial Intelligence. pp. 10 10.3389/frai.2021.625341\nClark, K., Khandelwal, U., Levy, O., and Manning, C. D. 2019. What does BERT look at? an analysis of BERT's attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. pp. 276--286 10.18653/v1/W19-4828\nCoutrot, A., Hsiao, J. H., and Chan, A. B. 2017. Scanpath modeling and classification with hidden markov models. In Scanpath modeling and classification with hidden markov models.\nCulotta, A., Mccallum, A., and Betz, J. 2006. Integrating probabilistic extraction models and data mining to discover relations and patterns in text. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference. pp. 296--303\nDas, A., Agrawal, H., Zitnick, L., Parikh, D., and Batra, D. 2016. Human attention in visual question answering: Do humans and deep networks look at the same regions?. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pp. 932--937 10.18653/v1/D16-1092\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423\nFeng, G. 2006. Eye movements as time-series random variables: A stochastic model of eye movement control in reading. In Cognitive Systems Research. pp. 70--95 10.1016/j.cogsys.2005.07.004\nShi Feng, E., Wallace, A., Grissom, I. I., Iyyer, M., Rodriguez, P., and Boyd-Graber, J. 2018. Pathologies of neural models make interpretations difficult. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 3719--3728 10.18653/v1/D18-1407\nGreene, M., Liu, T., and Wolfe, J. 2012. Reconsidering yarbus: A failure to predict observers' task from eye movement patterns. In Vision research. pp. 1--8 10.1016/j.visres.2012.03.019\nG\u00fcnther, F., Rinaldi, L., and Marelli, M. 2019. Vector-space models of semantic representation from a cognitive perspective: A discussion of common misconceptions. In Perspectives on Psychological Science. pp. 1006--1033 10.1177/1745691619861372\nHahn, M. and Keller, F. 2016. Modeling human reading with neural attention. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pp. 85--95 10.18653/v1/D16-1009\nHaji, A., Abolhassani, -., and Clark, J. J. 2014. An inverse yarbus process: Predicting observers' task from eye movement patterns. In Vision Research. pp. 127--142 10.1016/j.visres.2014.08.014\nHara, T., Mochihashi, D., Kano, Y., and Aizawa, A. 2012. Predicting word fixations in text with a CRF model for capturing general reading strategies among readers. In Proceedings of the First Workshop on Eye-tracking and Natural Language Processing. pp. 55--70\nHenderson, J., Shinkareva, S., Wang, J., Luke, S., and Olejarczyk, J. 2013. Predicting cognitive state from eye movements. In PloS one. pp. e64937 10.1371/journal.pone.0064937\nHillen, R., G\u00fcnther, T., Kohlen, C., Eckers, C., Van Ermingen-Marbach, M., Sass, K., Scharke, W., Vollmar, J., Radach, R., and Heim, S. 2013. Identifying brain systems for gaze orienting during reading: fmri investigation of the landolt paradigm. In Frontiers in human neuroscience. pp. 384\nHollenstein, N. and Beinborn, L. 2021. Relative importance in sentence processing. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 141--150 10.18653/v1/2021.acl-short.19\nHollenstein, N., De La Torre, A., Langer, N., and Zhang, C. 2019. CogniVal: A framework for cognitive word embedding evaluation. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL). pp. 538--549 10.18653/v1/K19-1050\nHollenstein, N., Pirovano, F., Zhang, C., J\u00e4ger, L., and Beinborn, L. 2021. Multilingual language models predict human reading behavior. In Multilingual language models predict human reading behavior. arXiv:2104.05433\nHollenstein, N., Rotsztejn, J., Troendle, M., Pedroni, A., Zhang, C., and Langer, N. 2018. Zuco, a simultaneous eeg and eye-tracking resource for natural sentence reading. In Scientific data. pp. 1--13\nHonnibal, M. and Montani, I. Sofie Van Landeghem, and Adriane Boyd. 2020. spaCy: Industrialstrength Natural Language Processing in Python. In Sofie Van Landeghem, and Adriane Boyd. 2020. spaCy: Industrialstrength Natural Language Processing in Python. 10.5281/zenodo.1212303\nJain, S. and Wallace, B. C. 2019. Attention is not Explanation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 3543--3556 10.18653/v1/N19-1357\nKilgarriff, A. 1995. BNC database and word frequency lists. In BNC database and word frequency lists. Accessed: 07/2020\nKim, Y. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 1746--1751 10.3115/v1/D14-1181\nKlerke, S., Goldberg, Y., and S\u00f8gaard, A. 2016. Improving sentence compression by learning to predict gaze. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1528--1533 10.18653/v1/N16-1179\nKlerke, S. and Plank, B. 2019. At a glance: The impact of gaze aggregation views on syntactic tagging. In Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN). pp. 51--61 10.18653/v1/D19-6408\nKliegl, R., Grabner, E., Rolfs, M., and Engbert, R. 2004. Length, frequency, and predictability effects of words on eye movements in reading. In European Journal of Cognitive Psychology. pp. 262--284 10.1080/09541440340000213\nKneser, R. and Ney, H. 1995. Improved backing-off for m-gram language modeling. In 1995 international conference on acoustics, speech, and signal processing. pp. 181--184\nKoch, C. and Ullman, S. 1985. Shifts in Selective Visual Attention: Towards the Underlying Neural Circuitry. In Human Neurobiology. pp. 219--227\nKovaleva, O., Romanov, A., Rogers, A., and Rumshisky, A. 2019. Revealing the dark secrets of bert. In Revealing the dark secrets of bert. arXiv:1908.08593\nK\u00fcmmerer, M., Thomas, S. A., Wallis, M., and Bethge 2016. DeepGaze II: Reading fixations from deep features trained on object recognition. In DeepGaze II: Reading fixations from deep features trained on object recognition. ArXiv: 1610.01563\nK\u00fcmmerer, M., Thomas, S. A., Wallis, L. A., Gatys, M., and Bethge 2017. Understanding low-and high-level contributions to fixation prediction. In 2017 IEEE International Conference on Computer Vision (ICCV). pp. 4799--4808 10.1109/ICCV.2017.513\nLecun, Y. and Bengio, Y. 1995. Convolutional Networks for Images, Speech and Time Series. In Convolutional Networks for Images, Speech and Time Series. pp. 255--258\nLin, Z., Feng, M., Nogueira, C., Santos, M., Yu, B., Xiang, B., Zhou, Y., and Bengio 2017. A structured self-attentive sentence embedding. In A structured self-attentive sentence embedding. abs/1703.03130\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nTomasz, D., Loboda, P., Brusilovsky, J., and Brunstein 2011. Inferring word relevance from eyemovements of readers. In Proceedings of the 16th international conference on Intelligent user interfaces. pp. 175--184\nMalmaud, J., Levy, R., and Berzak, Y. 2020. Bridging information-seeking human gaze and machine reading comprehension. In Proceedings of the 24th Conference on Computational Natural Language Learning. pp. 142--152 10.18653/v1/2020.conll-1.11\nMandera, P., Keuleers, E., and Brysbaert, M. 2017. Explaining human performance in psycholinguistic tasks with models of semantic similarity based on prediction and counting : a review and empirical validation. In Journal of Memory and Language. pp. 57--78 10.1016/j.jml.2016.04.001\nMatthies, F. and S\u00f8gaard, A. 2013. With blinkers on: Robust prediction of eye movements across readers. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 803--807\nSara, V., Milledge, H. I., and Blythe 2019. The changing role of phonology in reading development. In The changing role of phonology in reading development. 10.3390/vision3020023\nMiller, T. 2019. Explanation in artificial intelligence: Insights from the social sciences. In Artificial Intelligence. pp. 1--38 10.1016/j.artint.2018.07.007\nMishra, A., Dey, K., and Bhattacharyya, P. 2017. Learning cognitive features from gaze data for sentiment and sarcasm classification using convolutional neural network. In Proceedings of the 55th. 10.18653/v1/P17-1035\nAnnual Meeting of the Association for Computational Linguistics. In Annual Meeting of the Association for Computational Linguistics. pp. 377--387\nMishra, A., Kanojia, D., Nagar, S., Dey, K., and Bhattacharyya, P. 2016. Leveraging cognitive features for sentiment analysis. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning. pp. 156--166 10.18653/v1/K16-1016\nMontavon, G., Binder, A., Lapuschkin, S., Samek, W., and M\u00fcller, K. 2019. Layer-Wise Relevance Propagation: An Overview. In Layer-Wise Relevance Propagation: An Overview. pp. 193--209 10.1007/978-3-030-28954-6_10\nNiebur, E. and Koch, C. 1996. Control of selective visual attention: Modeling the \"where\" pathway. In Advances in Neural Information Processing Systems. pp. 802--808\nNilsson, M. and Nivre, J. 2009. Learning where to look: Modeling eye movements in reading. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning. pp. 93--101\nPennington, J., Socher, R., and Manning, C. D. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP). pp. 1532--1543\nPrasad, G., Nie, Y., Bansal, M., Jia, R., Kiela, D., and Williams, A. 2021. To what extent do human explanations of model behavior align with actual model behavior?. In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. pp. 1--14\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In Journal of Machine Learning Research. pp. 1--67\nRayner, K. 1998. Eye movements in reading and information processing: 20 years of research. In Psychological bulletin. pp. 372\nRayner, K. and Duffy, S. A. 1986. Lexical complexity and fixation times in reading: effects of word frequency, verb complexity, and lexical ambiguity. In Memory amp; cognition. pp. 191--201 10.3758/bf03197692\nRayner, K. and Reichle, E. D. 2010. Models of the reading process. In WIREs Cognitive Science. pp. 787--799 10.1002/wcs.68\nErik D Reichle, A., Pollatsek, Donald, L., Fisher, K., and Rayner 1998. Toward a model of eye movement control in reading. In Psychological review. pp. 125\nReichle, E. D., Rayner, K., and Pollatsek, A. 2003. The e-z reader model of eye-movement control in reading: comparisons to other models. The Behavioral and brain sciences. In The e-z reader model of eye-movement control in reading: comparisons to other models. The Behavioral and brain sciences. pp. 445--476 10.1017/S0140525X03000104\nRogers, T. and Wolmetz, M. 2016. Conceptual knowledge representation: A cross-section of current research. In Cognitive Neuropsychology. pp. 1--9 10.1080/02643294.2016.1188066\nSamek, W., Montavon, G., Lapuschkin, S., Anders, C. J., and M\u00fcller, K. 2021. Explaining deep neural networks and beyond: A review of methods and applications. Proceedings of the IEEE. In Explaining deep neural networks and beyond: A review of methods and applications. Proceedings of the IEEE. pp. 247--278 10.1109/JPROC.2021.3060483\nSchmidt, P. and Bie\u00dfmann, F. 2019. Quantifying interpretability and trust in machine learning systems. In CoRR. abs/1901.08558\nSerrano, S. and Smith, N. A. 2019. Is attention interpretable?. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 2931--2951 10.18653/v1/P19-1282\nSinha, K., Jia, R., Hupkes, D., Pineau, J., Williams, A., and Kiela, D. 2021. Masked language modeling and the distributional hypothesis: Order word matters pre-training for little. In Masked language modeling and the distributional hypothesis: Order word matters pre-training for little.\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Christopher, D., Manning, Andrew, Y., Ng, C., and Potts 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing. pp. 1631--1642\nS\u00f8gaard, A. 2016. Evaluating word embeddings with fMRI and eye-tracking. In Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP. pp. 116--121 10.18653/v1/W16-2521\nSood, E., Tannert, S., Frassinelli, D., Bulling, A., and Vu, N. T. 2020. Interpreting attention models with human visual attention in machine reading comprehension. In Proceedings of the 24th Conference on Computational Natural Language Learning. pp. 12--25 10.18653/v1/2020.conll-1.2\nSood, E., Tannert, S., Mueller, P., and Bulling, A. 2020. Improving natural language processing tasks with human gaze-guided neural attention. In Advances in Neural Information Processing Systems. pp. 6327--6341\nStolcke, A. 2002. Srilm -an extensible language modeling toolkit. In Proceedings of the 7th International Conference on Spoken Language Processing. pp. 901--904\nSugano, Y. and Bulling, A. 2016. Seeing with humans: Gaze-assisted neural image captioning. In Seeing with humans: Gaze-assisted neural image captioning. abs/1608.05203\nArun Balajee Vasudevan, D., Dai, L., and Van Gool 2018. Object referring in videos with language and human gaze. In 2018 IEEE Conference on Computer Vision and Pattern Recognition. pp. 4129--4138 10.1109/CVPR.2018.00434\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems. pp. 5998--6008\nWenzel, M. A., Bogojeski, M., and Blankertz, B. 2017. Real-time inference of word relevance from electroencephalogram and eye gaze. In Journal of neural engineering. pp. 56007\nWiegreffe, S. and Pinter, Y. 2019. Attention is not not explanation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 11--20 10.18653/v1/D19-1002\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., Patrick Von Platen, C., Ma, Y., Jernite, J., Plu, C., Xu, T. L., Scao, S., Gugger, M., Drame, Q., Lhoest, A., and Rush 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. pp. 38--45 10.18653/v1/2020.emnlp-demos.6\nYarbus, A. L. 1967. Eye Movements and Vision. Plenum. In Eye Movements and Vision. Plenum.\nZhang, Y. and Zhang, C. 2019. Using human attention to extract keyphrase from microblog post. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 5867--5872 10.18653/v1/P19-1588\nZhao, Y. and Bethard, S. 2020. How does BERT's attention change when you fine-tune? an analysis methodology and a case study in negation scope. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4729--4747 10.18653/v1/2020.acl-main.429"
            }
        ],
        "spans_b": [
            {
                "filename": "paper_5.txt",
                "start": 2361,
                "end": 2376,
                "label": "Unsupported claim",
                "text": "the ZuCo corpus",
                "full_text": "Introduction\n\nThe usefulness of learned self-attention functions often correlates with how well it aligns with human attention (Das et al., 2016;Klerke et al., 2016;Barrett et al., 2018;Klerke and Plank, 2019). In this paper, we evaluate how well attention flow (Abnar and Zuidema, 2020) in large language models, namely BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020), aligns with human eye fixations during task-specific reading, compared to other shallow sequence labeling models (Lecun and Bengio, 1995;Vaswani et al., 2017) and a classic, heuristic model of human reading (Reichle et al., 2003). We compare the learned attention functions and the heuristic model across two task-specific English reading tasks, namely sentiment analysis (SST movie reviews) and relation extraction (Wikipedia), as well as natural reading, using a publicly available data set with eye-tracking recordings of native speakers of English (Hollenstein et al., 2018).\n\nContributions We compare human and model attention patterns on both sentiment reading and relation extraction tasks. In our analysis, we compare human attention to pre-trained Transformers (BERT, RoBERTa and T5), from-scratch training of two shallow sequence labeling architectures (Lecun and Bengio, 1995; Vaswani et al., 2017), as well as to a frequency baseline and a heuristic, cognitively inspired model of human reading called the E-Z Reader (Reichle et al., 2003). We find that the heuristic model correlates well with human reading, as has been reported in Sood et al. (2020b). However when we apply attention flow (Abnar and Zuidema, 2020), the pre-trained Transformer models also reach comparable levels of correlation strength. Further fine-tuning experiments on BERT did not result in increased correlation to human fixations. To understand what drives the differences between models, we perform an in-depth analysis of the effect of word predictability and POS tags on correlation strength. It reveals that Transformer models do not accurately capture tail phenomena for hard-to-predict words (in contrast to the E-Z Reader) and that Transformer attention flow shows comparably weak correlation on (proper) nouns while the E-Z Reader predicts importance of these more accurately, especially on the sentiment reading task. In addition we investigate a subset of the ZuCo corpus for which aligned task-specific and natural reading data is available and find that Transformers correlate stronger to natural reading patterns. But how faithful are these different attention patterns at producing correct task-classification on a state-of-the-art NLP model? We test this via an input reduction experiment on task-tuned BERT models which highlights the trade-off between a model\u2019s faithfulness and sparsity when comparing importance scores to human attention, i.e., less sparse (higher entropy) attention vectors seem to be less faithful with respect to model predictions. Our code is available at github.com/anon.\n\n References: \nAbdou, M., Kulmizev, A., Hill, F., Low, D. M., and S\u00f8gaard, A. 2019. Higher-order comparisons of sentence encoder representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5838--5845 10.18653/v1/D19-1593\nAbnar, S. and Zuidema, W. 2020. Quantifying attention flow in transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4190--4197 10.18653/v1/2020.acl-main.385\nArras, L., Horn, F., Montavon, G., M\u00fcller, K., and Samek, W. 2016. Explaining predictions of non-linear classifiers in NLP. In Proceedings of the 1st Workshop on Representation Learning for NLP. pp. 1--7 10.18653/v1/W16-1601\nArras, L., Horn, F., Montavon, G., M\u00fcller, K., and Samek, W. 2017. What is relevant in a text document?\": An interpretable machine learning approach. In PLOS ONE. pp. 1--23 10.1371/journal.pone.0181142\nBach, S., Binder, A., Montavon, G., Klauschen, F., M\u00fcller, K., and Samek, W. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. In PLoS ONE. pp. e0130140 10.1371/journal.pone.0130140\nBarrett, M., Bingel, J., Hollenstein, N., Rei, M., and S\u00f8gaard, A. 2018. Sequence classification with human attention. In Proceedings of the 22nd Conference on Computational Natural Language Learning. pp. 302--312 10.18653/v1/K18-1030\nBarrett, M. and S\u00f8gaard, A. 2015. Reading behavior predicts syntactic categories. In Proceedings of the Nineteenth Conference on Computational Natural Language Learning. pp. 345--349 10.18653/v1/K15-1038\nBarrett, M. and S\u00f8gaard, A. 2015. Using reading behavior to predict grammatical functions. In Proceedings of the Sixth Workshop on Cognitive Aspects of Computational Language Learning. pp. 1--5 10.18653/v1/W15-2401\nBorji, A. and Itti, L. 2014. Defending yarbus: eye movements reveal observers' task. In Journal of vision. pp. 29 10.1167/14.3.29\nChelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., and Robinson, T. 2013. One billion word benchmark for measuring progress in statistical language modeling. In One billion word benchmark for measuring progress in statistical language modeling.\nChoi, W., Rutvik, H., Desai, J.M., and Henderson 2014. The neural substrates of natural reading: a comparison of normal and nonword text using eyetracking and fmri. In Frontiers in human neuroscience. pp. 1024\nChrupa\u0142a, G. and Alishahi, A. 2019. Correlating neural and symbolic representations of language. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 2952--2962 10.18653/v1/P19-1283\nChurch, K. and Liberman, M. 2021. The future of computational linguistics: On beyond alchemy. In Frontiers in Artificial Intelligence. pp. 10 10.3389/frai.2021.625341\nClark, K., Khandelwal, U., Levy, O., and Manning, C. D. 2019. What does BERT look at? an analysis of BERT's attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. pp. 276--286 10.18653/v1/W19-4828\nCoutrot, A., Hsiao, J. H., and Chan, A. B. 2017. Scanpath modeling and classification with hidden markov models. In Scanpath modeling and classification with hidden markov models.\nCulotta, A., Mccallum, A., and Betz, J. 2006. Integrating probabilistic extraction models and data mining to discover relations and patterns in text. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference. pp. 296--303\nDas, A., Agrawal, H., Zitnick, L., Parikh, D., and Batra, D. 2016. Human attention in visual question answering: Do humans and deep networks look at the same regions?. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pp. 932--937 10.18653/v1/D16-1092\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423\nFeng, G. 2006. Eye movements as time-series random variables: A stochastic model of eye movement control in reading. In Cognitive Systems Research. pp. 70--95 10.1016/j.cogsys.2005.07.004\nShi Feng, E., Wallace, A., Grissom, I. I., Iyyer, M., Rodriguez, P., and Boyd-Graber, J. 2018. Pathologies of neural models make interpretations difficult. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 3719--3728 10.18653/v1/D18-1407\nGreene, M., Liu, T., and Wolfe, J. 2012. Reconsidering yarbus: A failure to predict observers' task from eye movement patterns. In Vision research. pp. 1--8 10.1016/j.visres.2012.03.019\nG\u00fcnther, F., Rinaldi, L., and Marelli, M. 2019. Vector-space models of semantic representation from a cognitive perspective: A discussion of common misconceptions. In Perspectives on Psychological Science. pp. 1006--1033 10.1177/1745691619861372\nHahn, M. and Keller, F. 2016. Modeling human reading with neural attention. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pp. 85--95 10.18653/v1/D16-1009\nHaji, A., Abolhassani, -., and Clark, J. J. 2014. An inverse yarbus process: Predicting observers' task from eye movement patterns. In Vision Research. pp. 127--142 10.1016/j.visres.2014.08.014\nHara, T., Mochihashi, D., Kano, Y., and Aizawa, A. 2012. Predicting word fixations in text with a CRF model for capturing general reading strategies among readers. In Proceedings of the First Workshop on Eye-tracking and Natural Language Processing. pp. 55--70\nHenderson, J., Shinkareva, S., Wang, J., Luke, S., and Olejarczyk, J. 2013. Predicting cognitive state from eye movements. In PloS one. pp. e64937 10.1371/journal.pone.0064937\nHillen, R., G\u00fcnther, T., Kohlen, C., Eckers, C., Van Ermingen-Marbach, M., Sass, K., Scharke, W., Vollmar, J., Radach, R., and Heim, S. 2013. Identifying brain systems for gaze orienting during reading: fmri investigation of the landolt paradigm. In Frontiers in human neuroscience. pp. 384\nHollenstein, N. and Beinborn, L. 2021. Relative importance in sentence processing. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 141--150 10.18653/v1/2021.acl-short.19\nHollenstein, N., De La Torre, A., Langer, N., and Zhang, C. 2019. CogniVal: A framework for cognitive word embedding evaluation. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL). pp. 538--549 10.18653/v1/K19-1050\nHollenstein, N., Pirovano, F., Zhang, C., J\u00e4ger, L., and Beinborn, L. 2021. Multilingual language models predict human reading behavior. In Multilingual language models predict human reading behavior. arXiv:2104.05433\nHollenstein, N., Rotsztejn, J., Troendle, M., Pedroni, A., Zhang, C., and Langer, N. 2018. Zuco, a simultaneous eeg and eye-tracking resource for natural sentence reading. In Scientific data. pp. 1--13\nHonnibal, M. and Montani, I. Sofie Van Landeghem, and Adriane Boyd. 2020. spaCy: Industrialstrength Natural Language Processing in Python. In Sofie Van Landeghem, and Adriane Boyd. 2020. spaCy: Industrialstrength Natural Language Processing in Python. 10.5281/zenodo.1212303\nJain, S. and Wallace, B. C. 2019. Attention is not Explanation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 3543--3556 10.18653/v1/N19-1357\nKilgarriff, A. 1995. BNC database and word frequency lists. In BNC database and word frequency lists. Accessed: 07/2020\nKim, Y. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 1746--1751 10.3115/v1/D14-1181\nKlerke, S., Goldberg, Y., and S\u00f8gaard, A. 2016. Improving sentence compression by learning to predict gaze. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1528--1533 10.18653/v1/N16-1179\nKlerke, S. and Plank, B. 2019. At a glance: The impact of gaze aggregation views on syntactic tagging. In Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN). pp. 51--61 10.18653/v1/D19-6408\nKliegl, R., Grabner, E., Rolfs, M., and Engbert, R. 2004. Length, frequency, and predictability effects of words on eye movements in reading. In European Journal of Cognitive Psychology. pp. 262--284 10.1080/09541440340000213\nKneser, R. and Ney, H. 1995. Improved backing-off for m-gram language modeling. In 1995 international conference on acoustics, speech, and signal processing. pp. 181--184\nKoch, C. and Ullman, S. 1985. Shifts in Selective Visual Attention: Towards the Underlying Neural Circuitry. In Human Neurobiology. pp. 219--227\nKovaleva, O., Romanov, A., Rogers, A., and Rumshisky, A. 2019. Revealing the dark secrets of bert. In Revealing the dark secrets of bert. arXiv:1908.08593\nK\u00fcmmerer, M., Thomas, S. A., Wallis, M., and Bethge 2016. DeepGaze II: Reading fixations from deep features trained on object recognition. In DeepGaze II: Reading fixations from deep features trained on object recognition. ArXiv: 1610.01563\nK\u00fcmmerer, M., Thomas, S. A., Wallis, L. A., Gatys, M., and Bethge 2017. Understanding low-and high-level contributions to fixation prediction. In 2017 IEEE International Conference on Computer Vision (ICCV). pp. 4799--4808 10.1109/ICCV.2017.513\nLecun, Y. and Bengio, Y. 1995. Convolutional Networks for Images, Speech and Time Series. In Convolutional Networks for Images, Speech and Time Series. pp. 255--258\nLin, Z., Feng, M., Nogueira, C., Santos, M., Yu, B., Xiang, B., Zhou, Y., and Bengio 2017. A structured self-attentive sentence embedding. In A structured self-attentive sentence embedding. abs/1703.03130\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nTomasz, D., Loboda, P., Brusilovsky, J., and Brunstein 2011. Inferring word relevance from eyemovements of readers. In Proceedings of the 16th international conference on Intelligent user interfaces. pp. 175--184\nMalmaud, J., Levy, R., and Berzak, Y. 2020. Bridging information-seeking human gaze and machine reading comprehension. In Proceedings of the 24th Conference on Computational Natural Language Learning. pp. 142--152 10.18653/v1/2020.conll-1.11\nMandera, P., Keuleers, E., and Brysbaert, M. 2017. Explaining human performance in psycholinguistic tasks with models of semantic similarity based on prediction and counting : a review and empirical validation. In Journal of Memory and Language. pp. 57--78 10.1016/j.jml.2016.04.001\nMatthies, F. and S\u00f8gaard, A. 2013. With blinkers on: Robust prediction of eye movements across readers. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 803--807\nSara, V., Milledge, H. I., and Blythe 2019. The changing role of phonology in reading development. In The changing role of phonology in reading development. 10.3390/vision3020023\nMiller, T. 2019. Explanation in artificial intelligence: Insights from the social sciences. In Artificial Intelligence. pp. 1--38 10.1016/j.artint.2018.07.007\nMishra, A., Dey, K., and Bhattacharyya, P. 2017. Learning cognitive features from gaze data for sentiment and sarcasm classification using convolutional neural network. In Proceedings of the 55th. 10.18653/v1/P17-1035\nAnnual Meeting of the Association for Computational Linguistics. In Annual Meeting of the Association for Computational Linguistics. pp. 377--387\nMishra, A., Kanojia, D., Nagar, S., Dey, K., and Bhattacharyya, P. 2016. Leveraging cognitive features for sentiment analysis. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning. pp. 156--166 10.18653/v1/K16-1016\nMontavon, G., Binder, A., Lapuschkin, S., Samek, W., and M\u00fcller, K. 2019. Layer-Wise Relevance Propagation: An Overview. In Layer-Wise Relevance Propagation: An Overview. pp. 193--209 10.1007/978-3-030-28954-6_10\nNiebur, E. and Koch, C. 1996. Control of selective visual attention: Modeling the \"where\" pathway. In Advances in Neural Information Processing Systems. pp. 802--808\nNilsson, M. and Nivre, J. 2009. Learning where to look: Modeling eye movements in reading. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning. pp. 93--101\nPennington, J., Socher, R., and Manning, C. D. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP). pp. 1532--1543\nPrasad, G., Nie, Y., Bansal, M., Jia, R., Kiela, D., and Williams, A. 2021. To what extent do human explanations of model behavior align with actual model behavior?. In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. pp. 1--14\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In Journal of Machine Learning Research. pp. 1--67\nRayner, K. 1998. Eye movements in reading and information processing: 20 years of research. In Psychological bulletin. pp. 372\nRayner, K. and Duffy, S. A. 1986. Lexical complexity and fixation times in reading: effects of word frequency, verb complexity, and lexical ambiguity. In Memory amp; cognition. pp. 191--201 10.3758/bf03197692\nRayner, K. and Reichle, E. D. 2010. Models of the reading process. In WIREs Cognitive Science. pp. 787--799 10.1002/wcs.68\nErik D Reichle, A., Pollatsek, Donald, L., Fisher, K., and Rayner 1998. Toward a model of eye movement control in reading. In Psychological review. pp. 125\nReichle, E. D., Rayner, K., and Pollatsek, A. 2003. The e-z reader model of eye-movement control in reading: comparisons to other models. The Behavioral and brain sciences. In The e-z reader model of eye-movement control in reading: comparisons to other models. The Behavioral and brain sciences. pp. 445--476 10.1017/S0140525X03000104\nRogers, T. and Wolmetz, M. 2016. Conceptual knowledge representation: A cross-section of current research. In Cognitive Neuropsychology. pp. 1--9 10.1080/02643294.2016.1188066\nSamek, W., Montavon, G., Lapuschkin, S., Anders, C. J., and M\u00fcller, K. 2021. Explaining deep neural networks and beyond: A review of methods and applications. Proceedings of the IEEE. In Explaining deep neural networks and beyond: A review of methods and applications. Proceedings of the IEEE. pp. 247--278 10.1109/JPROC.2021.3060483\nSchmidt, P. and Bie\u00dfmann, F. 2019. Quantifying interpretability and trust in machine learning systems. In CoRR. abs/1901.08558\nSerrano, S. and Smith, N. A. 2019. Is attention interpretable?. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 2931--2951 10.18653/v1/P19-1282\nSinha, K., Jia, R., Hupkes, D., Pineau, J., Williams, A., and Kiela, D. 2021. Masked language modeling and the distributional hypothesis: Order word matters pre-training for little. In Masked language modeling and the distributional hypothesis: Order word matters pre-training for little.\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Christopher, D., Manning, Andrew, Y., Ng, C., and Potts 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing. pp. 1631--1642\nS\u00f8gaard, A. 2016. Evaluating word embeddings with fMRI and eye-tracking. In Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP. pp. 116--121 10.18653/v1/W16-2521\nSood, E., Tannert, S., Frassinelli, D., Bulling, A., and Vu, N. T. 2020. Interpreting attention models with human visual attention in machine reading comprehension. In Proceedings of the 24th Conference on Computational Natural Language Learning. pp. 12--25 10.18653/v1/2020.conll-1.2\nSood, E., Tannert, S., Mueller, P., and Bulling, A. 2020. Improving natural language processing tasks with human gaze-guided neural attention. In Advances in Neural Information Processing Systems. pp. 6327--6341\nStolcke, A. 2002. Srilm -an extensible language modeling toolkit. In Proceedings of the 7th International Conference on Spoken Language Processing. pp. 901--904\nSugano, Y. and Bulling, A. 2016. Seeing with humans: Gaze-assisted neural image captioning. In Seeing with humans: Gaze-assisted neural image captioning. abs/1608.05203\nArun Balajee Vasudevan, D., Dai, L., and Van Gool 2018. Object referring in videos with language and human gaze. In 2018 IEEE Conference on Computer Vision and Pattern Recognition. pp. 4129--4138 10.1109/CVPR.2018.00434\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems. pp. 5998--6008\nWenzel, M. A., Bogojeski, M., and Blankertz, B. 2017. Real-time inference of word relevance from electroencephalogram and eye gaze. In Journal of neural engineering. pp. 56007\nWiegreffe, S. and Pinter, Y. 2019. Attention is not not explanation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 11--20 10.18653/v1/D19-1002\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., Patrick Von Platen, C., Ma, Y., Jernite, J., Plu, C., Xu, T. L., Scao, S., Gugger, M., Drame, Q., Lhoest, A., and Rush 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. pp. 38--45 10.18653/v1/2020.emnlp-demos.6\nYarbus, A. L. 1967. Eye Movements and Vision. Plenum. In Eye Movements and Vision. Plenum.\nZhang, Y. and Zhang, C. 2019. Using human attention to extract keyphrase from microblog post. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 5867--5872 10.18653/v1/P19-1588\nZhao, Y. and Bethard, S. 2020. How does BERT's attention change when you fine-tune? an analysis methodology and a case study in negation scope. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4729--4747 10.18653/v1/2020.acl-main.429"
            }
        ]
    },
    {
        "filename": "paper_7.txt",
        "label": "Lacks synthesis",
        "text": "Compared to extractive models which extract existing words from the retrieved passages, generative models are able to produce new words out of the retrieved passages, and thus provide a more flexible modeling framework.  and Lewis et al. (2020a) concatenate the given question with top retrieved passages and feed the concatenation to the BART model (Lewis et al., 2019). Izacard and Grave (2020b) separately encodes the question with each top retrieved passage, then takes the concatenation of the encoder outputs as input to the decoder. Their method provide a way to better aggregate evidence from multiple passages and improve the performance significantly. FiD-KD (Izacard and Grave, 2020a) is an extension of FiD model that increases the accuracy of passage retrieval by training the dense retriever with the guidance of the FiD reader iteratively.",
        "start": 1003,
        "end": 1857,
        "spans_a": [
            {
                "filename": "paper_7.txt",
                "start": 1003,
                "end": 1857,
                "label": "Lacks synthesis",
                "text": "Compared to extractive models which extract existing words from the retrieved passages, generative models are able to produce new words out of the retrieved passages, and thus provide a more flexible modeling framework.  and Lewis et al. (2020a) concatenate the given question with top retrieved passages and feed the concatenation to the BART model (Lewis et al., 2019). Izacard and Grave (2020b) separately encodes the question with each top retrieved passage, then takes the concatenation of the encoder outputs as input to the decoder. Their method provide a way to better aggregate evidence from multiple passages and improve the performance significantly. FiD-KD (Izacard and Grave, 2020a) is an extension of FiD model that increases the accuracy of passage retrieval by training the dense retriever with the guidance of the FiD reader iteratively.",
                "full_text": "Related Work\n\nOpen-Domain Question Answering\n\nIn this era of data explosion, ODQA offers a way to rapidly and accurately fulfill user's information needs, and hence has recently received significant attention from both industry and academia (Min et al., 2021a). Following the work of DrQA (Chen et al., 2017), most recent works build a two-stage retriever-reader system to tackle the problem. The retriever aims at retrieving supportive passages to the given question from a large document corpus. The reader intends to find answer of the question from the first stage retrieved passages. Early work of Chen et al. (2017) adapts a BiLSTM architecture with various lexical and semantic features from the question and passages as inputs. Later, with the emergence of large-scale pre-trained language models, readers based on pre-trained models such as BERT and T5 Raffel et al., 2019) have become a common approach (Yang et al., 2019;Karpukhin et al., 2020;Izacard and Grave, 2020b).\n\nGenerative Readers\n\nCompared to extractive models which extract existing words from the retrieved passages, generative models are able to produce new words out of the retrieved passages, and thus provide a more flexible modeling framework.  and Lewis et al. (2020a) concatenate the given question with top retrieved passages and feed the concatenation to the BART model (Lewis et al., 2019). Izacard and Grave (2020b) separately encodes the question with each top retrieved passage, then takes the concatenation of the encoder outputs as input to the decoder. Their method provide a way to better aggregate evidence from multiple passages and improve the performance significantly. FiD-KD (Izacard and Grave, 2020a) is an extension of FiD model that increases the accuracy of passage retrieval by training the dense retriever with the guidance of the FiD reader iteratively.\n\nPointer-Generator Network\n\nPointer-Generator Network (See et al., 2017) is an extension of the sequence-to-sequence model by integrating a copy mechanism (Vinyals et al., 2017) into the generator. At each decoding stage, the model is able to either directly copy a word from the input or generate one with certain probability, and thus can be viewed as a combination of extractive and generative approaches. It has been frequently used in natural language tasks like summarization (Gu et al., 2016;See et al.,  Gehrmann et al., 2018) and neural machine translation (Luong et al., 2014;Gu et al., 2017), but its application to ODQA has been less explored.\n\n References: \nAsai, A., Hashimoto, K., Hajishirzi, H., Socher, R., and Xiong, C. 1911. Learning to retrieve reasoning paths over wikipedia graph for question answering. In Learning to retrieve reasoning paths over wikipedia graph for question answering.\nChen, D., Fisch, A., Weston, J., and Bordes, A. 2017. Reading wikipedia to answer opendomain questions. In Reading wikipedia to answer opendomain questions. abs/1704.00051\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 4171--4186 10.18653/v1/n19-1423\nGehrmann, S., Deng, Y., and Rush, A. M. 2018. Bottom-up abstractive summarization. In Bottom-up abstractive summarization. abs/1808.10792\nGu, J., Bradbury, J., Xiong, C., Victor, O. K., Li, R., and Socher 2017. Non-autoregressive neural machine translation. In Non-autoregressive neural machine translation. abs/1711.02281\nGu, J., Lu, Z., Li, H., Victor, O. K., and Li 2016. Incorporating copying mechanism in sequenceto-sequence learning. In Incorporating copying mechanism in sequenceto-sequence learning. abs/1603.06393\nIzacard, G. and Grave, E. 2020. Distilling knowledge from reader to retriever for question answering. In Distilling knowledge from reader to retriever for question answering. abs/2012.04584\nIzacard, G. and Grave, E. 1282. Leveraging passage retrieval with generative models for open domain question answering. CoRR, abs. In Leveraging passage retrieval with generative models for open domain question answering. CoRR, abs.\nJoshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. abs/1705.03551\nKarpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W. 2020. Dense passage retrieval for open-domain question answering. In Dense passage retrieval for open-domain question answering. arXiv:2004.04906\nKwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M., Dai, A., Uszkoreit, J., Le, Q., and Petrov, S. 2019. Natural questions: a benchmark for question answering research. In Transactions of the Association for Computational Linguistics. pp. 452--466\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2019.\n1910. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. CoRR, abs. In BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. CoRR, abs.\nPatrick, S. H., Lewis, E., Perez, A., Piktus, F., Petroni, V., Karpukhin, N., Goyal, H., K\u00fcttler, M., Lewis, W., Yih, T., Rockt\u00e4schel, S., Riedel, D., and Kiela 2005. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Retrieval-augmented generation for knowledge-intensive NLP tasks.\nPatrick, S. H., Lewis, P., Stenetorp, S., and Riedel 2008. Question and answer test-train overlap in open-domain question answering datasets. CoRR, abs. In Question and answer test-train overlap in open-domain question answering datasets. CoRR, abs.\nLoshchilov, I. and Hutter, F. 2017. Fixing weight decay regularization in adam. In Fixing weight decay regularization in adam. abs/1711.05101\nLuong, T., Sutskever, I., Le, Q. V., Vinyals, O., and Zaremba, W. 2014. Addressing the rare word problem in neural machine translation. In Addressing the rare word problem in neural machine translation.\nMaynez, J., Narayan, S., Bohnet, B., and Mcdonald, R. T. 0661. On faithfulness and factuality in abstractive summarization. CoRR, abs. In On faithfulness and factuality in abstractive summarization. CoRR, abs.\nMin, S., Boyd-Graber, J. L., Alberti, C., Chen, D., Choi, E., Collins, M., Guu, K., Hajishirzi, H., Lee, K., Palomaki, J., Raffel, C., Roberts, A., Kwiatkowski, T., Patrick, S. H., Lewis, Y., Wu, H., K\u00fcttler, L., Liu, P., Minervini, P., Stenetorp, S., Riedel, S., Yang, M., Seo, G., Izacard, F., Petroni, L., Hosseini, N. D., Cao, E., Grave, I., Yamada, S., Shimaoka, M., Suzuki, S., Miyawaki, S., Sato, R., Takahashi, J., Suzuki, M., Fajcik, M., and Docekal\nMin, S., Lee, K., Chang, M., Toutanova, K., and Hajishirzi, H. 2021. Joint passage ranking for diverse multi-answer retrieval. In Joint passage ranking for diverse multi-answer retrieval. abs/2104.08445\nMin, S., Michael, J., Hajishirzi, H., and Zettlemoyer, L. 2004. Ambigqa: Answering ambiguous open-domain questions. CoRR, abs. In Ambigqa: Answering ambiguous open-domain questions. CoRR, abs.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 1910. Exploring the limits of transfer learning with a unified text-to-text transformer. In Exploring the limits of transfer learning with a unified text-to-text transformer.\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. 2016. Squad: 100, 000+ questions for machine comprehension of text. In Squad: 100, 000+ questions for machine comprehension of text. abs/1606.05250\nRogers, A., Gardner, M., and Augenstein, I. 2021. QA dataset explosion: A taxonomy of NLP resources for question answering and reading comprehension. In QA dataset explosion: A taxonomy of NLP resources for question answering and reading comprehension. abs/2107.12708\nSee, A., Peter, J., Liu, C.D., and Manning 2017. Get to the point: Summarization with pointer-generator networks. In Get to the point: Summarization with pointer-generator networks. arXiv:1704.04368\nVinyals, O., Fortunato, M., and Jaitly, N. 2017.\nVoorhees, E. M. 1999. The TREC-8 question answering track report. In Proceedings of The Eighth Text REtrieval Conference. TREC 1999\nWu, Y., Minervini, P., Stenetorp, P., and Riedel, S. 2021. Training adaptive computation for open-domain question answering with computational constraints. In Training adaptive computation for open-domain question answering with computational constraints. abs/2107.02102\nYamada, I., Asai, A., and Hajishirzi, H. 2021. Efficient passage retrieval with hashing for open-domain question answering. In Efficient passage retrieval with hashing for open-domain question answering. abs/2106.00882\nYang, W., Xie, Y., Lin, A., Li, X., Tan, L., Xiong, K., Li, M., and Lin, J. 1718. End-to-end open-domain question answering with bertserini. CoRR, abs. In End-to-end open-domain question answering with bertserini. CoRR, abs.\nZhou, C., Neubig, G., Gu, J., Diab, M., Guzm\u00e1n, F., Zettlemoyer, L., and Ghazvininejad, M. 2021. Detecting hallucinated content in conditional neural sequence generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. pp. 1393--1404 10.18653/v1/2021.findings-acl.120"
            }
        ],
        "spans_b": [
            {
                "filename": "paper_7.txt",
                "start": 983,
                "end": 1857,
                "label": "Lacks synthesis",
                "text": "Generative Readers\n\nCompared to extractive models which extract existing words from the retrieved passages, generative models are able to produce new words out of the retrieved passages, and thus provide a more flexible modeling framework.  and Lewis et al. (2020a) concatenate the given question with top retrieved passages and feed the concatenation to the BART model (Lewis et al., 2019). Izacard and Grave (2020b) separately encodes the question with each top retrieved passage, then takes the concatenation of the encoder outputs as input to the decoder. Their method provide a way to better aggregate evidence from multiple passages and improve the performance significantly. FiD-KD (Izacard and Grave, 2020a) is an extension of FiD model that increases the accuracy of passage retrieval by training the dense retriever with the guidance of the FiD reader iteratively.",
                "full_text": "Related Work\n\nOpen-Domain Question Answering\n\nIn this era of data explosion, ODQA offers a way to rapidly and accurately fulfill user's information needs, and hence has recently received significant attention from both industry and academia (Min et al., 2021a). Following the work of DrQA (Chen et al., 2017), most recent works build a two-stage retriever-reader system to tackle the problem. The retriever aims at retrieving supportive passages to the given question from a large document corpus. The reader intends to find answer of the question from the first stage retrieved passages. Early work of Chen et al. (2017) adapts a BiLSTM architecture with various lexical and semantic features from the question and passages as inputs. Later, with the emergence of large-scale pre-trained language models, readers based on pre-trained models such as BERT and T5 Raffel et al., 2019) have become a common approach (Yang et al., 2019;Karpukhin et al., 2020;Izacard and Grave, 2020b).\n\nGenerative Readers\n\nCompared to extractive models which extract existing words from the retrieved passages, generative models are able to produce new words out of the retrieved passages, and thus provide a more flexible modeling framework.  and Lewis et al. (2020a) concatenate the given question with top retrieved passages and feed the concatenation to the BART model (Lewis et al., 2019). Izacard and Grave (2020b) separately encodes the question with each top retrieved passage, then takes the concatenation of the encoder outputs as input to the decoder. Their method provide a way to better aggregate evidence from multiple passages and improve the performance significantly. FiD-KD (Izacard and Grave, 2020a) is an extension of FiD model that increases the accuracy of passage retrieval by training the dense retriever with the guidance of the FiD reader iteratively.\n\nPointer-Generator Network\n\nPointer-Generator Network (See et al., 2017) is an extension of the sequence-to-sequence model by integrating a copy mechanism (Vinyals et al., 2017) into the generator. At each decoding stage, the model is able to either directly copy a word from the input or generate one with certain probability, and thus can be viewed as a combination of extractive and generative approaches. It has been frequently used in natural language tasks like summarization (Gu et al., 2016;See et al.,  Gehrmann et al., 2018) and neural machine translation (Luong et al., 2014;Gu et al., 2017), but its application to ODQA has been less explored.\n\n References: \nAsai, A., Hashimoto, K., Hajishirzi, H., Socher, R., and Xiong, C. 1911. Learning to retrieve reasoning paths over wikipedia graph for question answering. In Learning to retrieve reasoning paths over wikipedia graph for question answering.\nChen, D., Fisch, A., Weston, J., and Bordes, A. 2017. Reading wikipedia to answer opendomain questions. In Reading wikipedia to answer opendomain questions. abs/1704.00051\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 4171--4186 10.18653/v1/n19-1423\nGehrmann, S., Deng, Y., and Rush, A. M. 2018. Bottom-up abstractive summarization. In Bottom-up abstractive summarization. abs/1808.10792\nGu, J., Bradbury, J., Xiong, C., Victor, O. K., Li, R., and Socher 2017. Non-autoregressive neural machine translation. In Non-autoregressive neural machine translation. abs/1711.02281\nGu, J., Lu, Z., Li, H., Victor, O. K., and Li 2016. Incorporating copying mechanism in sequenceto-sequence learning. In Incorporating copying mechanism in sequenceto-sequence learning. abs/1603.06393\nIzacard, G. and Grave, E. 2020. Distilling knowledge from reader to retriever for question answering. In Distilling knowledge from reader to retriever for question answering. abs/2012.04584\nIzacard, G. and Grave, E. 1282. Leveraging passage retrieval with generative models for open domain question answering. CoRR, abs. In Leveraging passage retrieval with generative models for open domain question answering. CoRR, abs.\nJoshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. abs/1705.03551\nKarpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W. 2020. Dense passage retrieval for open-domain question answering. In Dense passage retrieval for open-domain question answering. arXiv:2004.04906\nKwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M., Dai, A., Uszkoreit, J., Le, Q., and Petrov, S. 2019. Natural questions: a benchmark for question answering research. In Transactions of the Association for Computational Linguistics. pp. 452--466\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2019.\n1910. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. CoRR, abs. In BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. CoRR, abs.\nPatrick, S. H., Lewis, E., Perez, A., Piktus, F., Petroni, V., Karpukhin, N., Goyal, H., K\u00fcttler, M., Lewis, W., Yih, T., Rockt\u00e4schel, S., Riedel, D., and Kiela 2005. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Retrieval-augmented generation for knowledge-intensive NLP tasks.\nPatrick, S. H., Lewis, P., Stenetorp, S., and Riedel 2008. Question and answer test-train overlap in open-domain question answering datasets. CoRR, abs. In Question and answer test-train overlap in open-domain question answering datasets. CoRR, abs.\nLoshchilov, I. and Hutter, F. 2017. Fixing weight decay regularization in adam. In Fixing weight decay regularization in adam. abs/1711.05101\nLuong, T., Sutskever, I., Le, Q. V., Vinyals, O., and Zaremba, W. 2014. Addressing the rare word problem in neural machine translation. In Addressing the rare word problem in neural machine translation.\nMaynez, J., Narayan, S., Bohnet, B., and Mcdonald, R. T. 0661. On faithfulness and factuality in abstractive summarization. CoRR, abs. In On faithfulness and factuality in abstractive summarization. CoRR, abs.\nMin, S., Boyd-Graber, J. L., Alberti, C., Chen, D., Choi, E., Collins, M., Guu, K., Hajishirzi, H., Lee, K., Palomaki, J., Raffel, C., Roberts, A., Kwiatkowski, T., Patrick, S. H., Lewis, Y., Wu, H., K\u00fcttler, L., Liu, P., Minervini, P., Stenetorp, S., Riedel, S., Yang, M., Seo, G., Izacard, F., Petroni, L., Hosseini, N. D., Cao, E., Grave, I., Yamada, S., Shimaoka, M., Suzuki, S., Miyawaki, S., Sato, R., Takahashi, J., Suzuki, M., Fajcik, M., and Docekal\nMin, S., Lee, K., Chang, M., Toutanova, K., and Hajishirzi, H. 2021. Joint passage ranking for diverse multi-answer retrieval. In Joint passage ranking for diverse multi-answer retrieval. abs/2104.08445\nMin, S., Michael, J., Hajishirzi, H., and Zettlemoyer, L. 2004. Ambigqa: Answering ambiguous open-domain questions. CoRR, abs. In Ambigqa: Answering ambiguous open-domain questions. CoRR, abs.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 1910. Exploring the limits of transfer learning with a unified text-to-text transformer. In Exploring the limits of transfer learning with a unified text-to-text transformer.\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. 2016. Squad: 100, 000+ questions for machine comprehension of text. In Squad: 100, 000+ questions for machine comprehension of text. abs/1606.05250\nRogers, A., Gardner, M., and Augenstein, I. 2021. QA dataset explosion: A taxonomy of NLP resources for question answering and reading comprehension. In QA dataset explosion: A taxonomy of NLP resources for question answering and reading comprehension. abs/2107.12708\nSee, A., Peter, J., Liu, C.D., and Manning 2017. Get to the point: Summarization with pointer-generator networks. In Get to the point: Summarization with pointer-generator networks. arXiv:1704.04368\nVinyals, O., Fortunato, M., and Jaitly, N. 2017.\nVoorhees, E. M. 1999. The TREC-8 question answering track report. In Proceedings of The Eighth Text REtrieval Conference. TREC 1999\nWu, Y., Minervini, P., Stenetorp, P., and Riedel, S. 2021. Training adaptive computation for open-domain question answering with computational constraints. In Training adaptive computation for open-domain question answering with computational constraints. abs/2107.02102\nYamada, I., Asai, A., and Hajishirzi, H. 2021. Efficient passage retrieval with hashing for open-domain question answering. In Efficient passage retrieval with hashing for open-domain question answering. abs/2106.00882\nYang, W., Xie, Y., Lin, A., Li, X., Tan, L., Xiong, K., Li, M., and Lin, J. 1718. End-to-end open-domain question answering with bertserini. CoRR, abs. In End-to-end open-domain question answering with bertserini. CoRR, abs.\nZhou, C., Neubig, G., Gu, J., Diab, M., Guzm\u00e1n, F., Zettlemoyer, L., and Ghazvininejad, M. 2021. Detecting hallucinated content in conditional neural sequence generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. pp. 1393--1404 10.18653/v1/2021.findings-acl.120"
            }
        ]
    },
    {
        "filename": "paper_7.txt",
        "label": "Lacks synthesis",
        "text": "Following the work of DrQA (Chen et al., 2017), most recent works build a two-stage retriever-reader system to tackle the problem. The retriever aims at retrieving supportive passages to the given question from a large document corpus. The reader intends to find answer of the question from the first stage retrieved passages. Early work of Chen et al. (2017) adapts a BiLSTM architecture with various lexical and semantic features from the question and passages as inputs. Later, with the emergence of large-scale pre-trained language models, readers based on pre-trained models such as BERT and T5 Raffel et al., 2019) have become a common approach (Yang et al., 2019;Karpukhin et al., 2020;Izacard and Grave, 2020b).",
        "start": 262,
        "end": 981,
        "spans_a": [
            {
                "filename": "paper_7.txt",
                "start": 262,
                "end": 982,
                "label": "Lacks synthesis",
                "text": "Following the work of DrQA (Chen et al., 2017), most recent works build a two-stage retriever-reader system to tackle the problem. The retriever aims at retrieving supportive passages to the given question from a large document corpus. The reader intends to find answer of the question from the first stage retrieved passages. Early work of Chen et al. (2017) adapts a BiLSTM architecture with various lexical and semantic features from the question and passages as inputs. Later, with the emergence of large-scale pre-trained language models, readers based on pre-trained models such as BERT and T5 Raffel et al., 2019) have become a common approach (Yang et al., 2019;Karpukhin et al., 2020;Izacard and Grave, 2020b).",
                "full_text": "Related Work\n\nOpen-Domain Question Answering\n\nIn this era of data explosion, ODQA offers a way to rapidly and accurately fulfill user's information needs, and hence has recently received significant attention from both industry and academia (Min et al., 2021a). Following the work of DrQA (Chen et al., 2017), most recent works build a two-stage retriever-reader system to tackle the problem. The retriever aims at retrieving supportive passages to the given question from a large document corpus. The reader intends to find answer of the question from the first stage retrieved passages. Early work of Chen et al. (2017) adapts a BiLSTM architecture with various lexical and semantic features from the question and passages as inputs. Later, with the emergence of large-scale pre-trained language models, readers based on pre-trained models such as BERT and T5 Raffel et al., 2019) have become a common approach (Yang et al., 2019;Karpukhin et al., 2020;Izacard and Grave, 2020b).\n\nGenerative Readers\n\nCompared to extractive models which extract existing words from the retrieved passages, generative models are able to produce new words out of the retrieved passages, and thus provide a more flexible modeling framework.  and Lewis et al. (2020a) concatenate the given question with top retrieved passages and feed the concatenation to the BART model (Lewis et al., 2019). Izacard and Grave (2020b) separately encodes the question with each top retrieved passage, then takes the concatenation of the encoder outputs as input to the decoder. Their method provide a way to better aggregate evidence from multiple passages and improve the performance significantly. FiD-KD (Izacard and Grave, 2020a) is an extension of FiD model that increases the accuracy of passage retrieval by training the dense retriever with the guidance of the FiD reader iteratively.\n\nPointer-Generator Network\n\nPointer-Generator Network (See et al., 2017) is an extension of the sequence-to-sequence model by integrating a copy mechanism (Vinyals et al., 2017) into the generator. At each decoding stage, the model is able to either directly copy a word from the input or generate one with certain probability, and thus can be viewed as a combination of extractive and generative approaches. It has been frequently used in natural language tasks like summarization (Gu et al., 2016;See et al.,  Gehrmann et al., 2018) and neural machine translation (Luong et al., 2014;Gu et al., 2017), but its application to ODQA has been less explored.\n\n References: \nAsai, A., Hashimoto, K., Hajishirzi, H., Socher, R., and Xiong, C. 1911. Learning to retrieve reasoning paths over wikipedia graph for question answering. In Learning to retrieve reasoning paths over wikipedia graph for question answering.\nChen, D., Fisch, A., Weston, J., and Bordes, A. 2017. Reading wikipedia to answer opendomain questions. In Reading wikipedia to answer opendomain questions. abs/1704.00051\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 4171--4186 10.18653/v1/n19-1423\nGehrmann, S., Deng, Y., and Rush, A. M. 2018. Bottom-up abstractive summarization. In Bottom-up abstractive summarization. abs/1808.10792\nGu, J., Bradbury, J., Xiong, C., Victor, O. K., Li, R., and Socher 2017. Non-autoregressive neural machine translation. In Non-autoregressive neural machine translation. abs/1711.02281\nGu, J., Lu, Z., Li, H., Victor, O. K., and Li 2016. Incorporating copying mechanism in sequenceto-sequence learning. In Incorporating copying mechanism in sequenceto-sequence learning. abs/1603.06393\nIzacard, G. and Grave, E. 2020. Distilling knowledge from reader to retriever for question answering. In Distilling knowledge from reader to retriever for question answering. abs/2012.04584\nIzacard, G. and Grave, E. 1282. Leveraging passage retrieval with generative models for open domain question answering. CoRR, abs. In Leveraging passage retrieval with generative models for open domain question answering. CoRR, abs.\nJoshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. abs/1705.03551\nKarpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W. 2020. Dense passage retrieval for open-domain question answering. In Dense passage retrieval for open-domain question answering. arXiv:2004.04906\nKwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M., Dai, A., Uszkoreit, J., Le, Q., and Petrov, S. 2019. Natural questions: a benchmark for question answering research. In Transactions of the Association for Computational Linguistics. pp. 452--466\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2019.\n1910. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. CoRR, abs. In BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. CoRR, abs.\nPatrick, S. H., Lewis, E., Perez, A., Piktus, F., Petroni, V., Karpukhin, N., Goyal, H., K\u00fcttler, M., Lewis, W., Yih, T., Rockt\u00e4schel, S., Riedel, D., and Kiela 2005. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Retrieval-augmented generation for knowledge-intensive NLP tasks.\nPatrick, S. H., Lewis, P., Stenetorp, S., and Riedel 2008. Question and answer test-train overlap in open-domain question answering datasets. CoRR, abs. In Question and answer test-train overlap in open-domain question answering datasets. CoRR, abs.\nLoshchilov, I. and Hutter, F. 2017. Fixing weight decay regularization in adam. In Fixing weight decay regularization in adam. abs/1711.05101\nLuong, T., Sutskever, I., Le, Q. V., Vinyals, O., and Zaremba, W. 2014. Addressing the rare word problem in neural machine translation. In Addressing the rare word problem in neural machine translation.\nMaynez, J., Narayan, S., Bohnet, B., and Mcdonald, R. T. 0661. On faithfulness and factuality in abstractive summarization. CoRR, abs. In On faithfulness and factuality in abstractive summarization. CoRR, abs.\nMin, S., Boyd-Graber, J. L., Alberti, C., Chen, D., Choi, E., Collins, M., Guu, K., Hajishirzi, H., Lee, K., Palomaki, J., Raffel, C., Roberts, A., Kwiatkowski, T., Patrick, S. H., Lewis, Y., Wu, H., K\u00fcttler, L., Liu, P., Minervini, P., Stenetorp, S., Riedel, S., Yang, M., Seo, G., Izacard, F., Petroni, L., Hosseini, N. D., Cao, E., Grave, I., Yamada, S., Shimaoka, M., Suzuki, S., Miyawaki, S., Sato, R., Takahashi, J., Suzuki, M., Fajcik, M., and Docekal\nMin, S., Lee, K., Chang, M., Toutanova, K., and Hajishirzi, H. 2021. Joint passage ranking for diverse multi-answer retrieval. In Joint passage ranking for diverse multi-answer retrieval. abs/2104.08445\nMin, S., Michael, J., Hajishirzi, H., and Zettlemoyer, L. 2004. Ambigqa: Answering ambiguous open-domain questions. CoRR, abs. In Ambigqa: Answering ambiguous open-domain questions. CoRR, abs.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 1910. Exploring the limits of transfer learning with a unified text-to-text transformer. In Exploring the limits of transfer learning with a unified text-to-text transformer.\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. 2016. Squad: 100, 000+ questions for machine comprehension of text. In Squad: 100, 000+ questions for machine comprehension of text. abs/1606.05250\nRogers, A., Gardner, M., and Augenstein, I. 2021. QA dataset explosion: A taxonomy of NLP resources for question answering and reading comprehension. In QA dataset explosion: A taxonomy of NLP resources for question answering and reading comprehension. abs/2107.12708\nSee, A., Peter, J., Liu, C.D., and Manning 2017. Get to the point: Summarization with pointer-generator networks. In Get to the point: Summarization with pointer-generator networks. arXiv:1704.04368\nVinyals, O., Fortunato, M., and Jaitly, N. 2017.\nVoorhees, E. M. 1999. The TREC-8 question answering track report. In Proceedings of The Eighth Text REtrieval Conference. TREC 1999\nWu, Y., Minervini, P., Stenetorp, P., and Riedel, S. 2021. Training adaptive computation for open-domain question answering with computational constraints. In Training adaptive computation for open-domain question answering with computational constraints. abs/2107.02102\nYamada, I., Asai, A., and Hajishirzi, H. 2021. Efficient passage retrieval with hashing for open-domain question answering. In Efficient passage retrieval with hashing for open-domain question answering. abs/2106.00882\nYang, W., Xie, Y., Lin, A., Li, X., Tan, L., Xiong, K., Li, M., and Lin, J. 1718. End-to-end open-domain question answering with bertserini. CoRR, abs. In End-to-end open-domain question answering with bertserini. CoRR, abs.\nZhou, C., Neubig, G., Gu, J., Diab, M., Guzm\u00e1n, F., Zettlemoyer, L., and Ghazvininejad, M. 2021. Detecting hallucinated content in conditional neural sequence generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. pp. 1393--1404 10.18653/v1/2021.findings-acl.120"
            }
        ],
        "spans_b": [
            {
                "filename": "paper_7.txt",
                "start": 14,
                "end": 981,
                "label": "Lacks synthesis",
                "text": "Open-Domain Question Answering\n\nIn this era of data explosion, ODQA offers a way to rapidly and accurately fulfill user's information needs, and hence has recently received significant attention from both industry and academia (Min et al., 2021a). Following the work of DrQA (Chen et al., 2017), most recent works build a two-stage retriever-reader system to tackle the problem. The retriever aims at retrieving supportive passages to the given question from a large document corpus. The reader intends to find answer of the question from the first stage retrieved passages. Early work of Chen et al. (2017) adapts a BiLSTM architecture with various lexical and semantic features from the question and passages as inputs. Later, with the emergence of large-scale pre-trained language models, readers based on pre-trained models such as BERT and T5 Raffel et al., 2019) have become a common approach (Yang et al., 2019;Karpukhin et al., 2020;Izacard and Grave, 2020b).",
                "full_text": "Related Work\n\nOpen-Domain Question Answering\n\nIn this era of data explosion, ODQA offers a way to rapidly and accurately fulfill user's information needs, and hence has recently received significant attention from both industry and academia (Min et al., 2021a). Following the work of DrQA (Chen et al., 2017), most recent works build a two-stage retriever-reader system to tackle the problem. The retriever aims at retrieving supportive passages to the given question from a large document corpus. The reader intends to find answer of the question from the first stage retrieved passages. Early work of Chen et al. (2017) adapts a BiLSTM architecture with various lexical and semantic features from the question and passages as inputs. Later, with the emergence of large-scale pre-trained language models, readers based on pre-trained models such as BERT and T5 Raffel et al., 2019) have become a common approach (Yang et al., 2019;Karpukhin et al., 2020;Izacard and Grave, 2020b).\n\nGenerative Readers\n\nCompared to extractive models which extract existing words from the retrieved passages, generative models are able to produce new words out of the retrieved passages, and thus provide a more flexible modeling framework.  and Lewis et al. (2020a) concatenate the given question with top retrieved passages and feed the concatenation to the BART model (Lewis et al., 2019). Izacard and Grave (2020b) separately encodes the question with each top retrieved passage, then takes the concatenation of the encoder outputs as input to the decoder. Their method provide a way to better aggregate evidence from multiple passages and improve the performance significantly. FiD-KD (Izacard and Grave, 2020a) is an extension of FiD model that increases the accuracy of passage retrieval by training the dense retriever with the guidance of the FiD reader iteratively.\n\nPointer-Generator Network\n\nPointer-Generator Network (See et al., 2017) is an extension of the sequence-to-sequence model by integrating a copy mechanism (Vinyals et al., 2017) into the generator. At each decoding stage, the model is able to either directly copy a word from the input or generate one with certain probability, and thus can be viewed as a combination of extractive and generative approaches. It has been frequently used in natural language tasks like summarization (Gu et al., 2016;See et al.,  Gehrmann et al., 2018) and neural machine translation (Luong et al., 2014;Gu et al., 2017), but its application to ODQA has been less explored.\n\n References: \nAsai, A., Hashimoto, K., Hajishirzi, H., Socher, R., and Xiong, C. 1911. Learning to retrieve reasoning paths over wikipedia graph for question answering. In Learning to retrieve reasoning paths over wikipedia graph for question answering.\nChen, D., Fisch, A., Weston, J., and Bordes, A. 2017. Reading wikipedia to answer opendomain questions. In Reading wikipedia to answer opendomain questions. abs/1704.00051\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 4171--4186 10.18653/v1/n19-1423\nGehrmann, S., Deng, Y., and Rush, A. M. 2018. Bottom-up abstractive summarization. In Bottom-up abstractive summarization. abs/1808.10792\nGu, J., Bradbury, J., Xiong, C., Victor, O. K., Li, R., and Socher 2017. Non-autoregressive neural machine translation. In Non-autoregressive neural machine translation. abs/1711.02281\nGu, J., Lu, Z., Li, H., Victor, O. K., and Li 2016. Incorporating copying mechanism in sequenceto-sequence learning. In Incorporating copying mechanism in sequenceto-sequence learning. abs/1603.06393\nIzacard, G. and Grave, E. 2020. Distilling knowledge from reader to retriever for question answering. In Distilling knowledge from reader to retriever for question answering. abs/2012.04584\nIzacard, G. and Grave, E. 1282. Leveraging passage retrieval with generative models for open domain question answering. CoRR, abs. In Leveraging passage retrieval with generative models for open domain question answering. CoRR, abs.\nJoshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. abs/1705.03551\nKarpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W. 2020. Dense passage retrieval for open-domain question answering. In Dense passage retrieval for open-domain question answering. arXiv:2004.04906\nKwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M., Dai, A., Uszkoreit, J., Le, Q., and Petrov, S. 2019. Natural questions: a benchmark for question answering research. In Transactions of the Association for Computational Linguistics. pp. 452--466\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2019.\n1910. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. CoRR, abs. In BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. CoRR, abs.\nPatrick, S. H., Lewis, E., Perez, A., Piktus, F., Petroni, V., Karpukhin, N., Goyal, H., K\u00fcttler, M., Lewis, W., Yih, T., Rockt\u00e4schel, S., Riedel, D., and Kiela 2005. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Retrieval-augmented generation for knowledge-intensive NLP tasks.\nPatrick, S. H., Lewis, P., Stenetorp, S., and Riedel 2008. Question and answer test-train overlap in open-domain question answering datasets. CoRR, abs. In Question and answer test-train overlap in open-domain question answering datasets. CoRR, abs.\nLoshchilov, I. and Hutter, F. 2017. Fixing weight decay regularization in adam. In Fixing weight decay regularization in adam. abs/1711.05101\nLuong, T., Sutskever, I., Le, Q. V., Vinyals, O., and Zaremba, W. 2014. Addressing the rare word problem in neural machine translation. In Addressing the rare word problem in neural machine translation.\nMaynez, J., Narayan, S., Bohnet, B., and Mcdonald, R. T. 0661. On faithfulness and factuality in abstractive summarization. CoRR, abs. In On faithfulness and factuality in abstractive summarization. CoRR, abs.\nMin, S., Boyd-Graber, J. L., Alberti, C., Chen, D., Choi, E., Collins, M., Guu, K., Hajishirzi, H., Lee, K., Palomaki, J., Raffel, C., Roberts, A., Kwiatkowski, T., Patrick, S. H., Lewis, Y., Wu, H., K\u00fcttler, L., Liu, P., Minervini, P., Stenetorp, S., Riedel, S., Yang, M., Seo, G., Izacard, F., Petroni, L., Hosseini, N. D., Cao, E., Grave, I., Yamada, S., Shimaoka, M., Suzuki, S., Miyawaki, S., Sato, R., Takahashi, J., Suzuki, M., Fajcik, M., and Docekal\nMin, S., Lee, K., Chang, M., Toutanova, K., and Hajishirzi, H. 2021. Joint passage ranking for diverse multi-answer retrieval. In Joint passage ranking for diverse multi-answer retrieval. abs/2104.08445\nMin, S., Michael, J., Hajishirzi, H., and Zettlemoyer, L. 2004. Ambigqa: Answering ambiguous open-domain questions. CoRR, abs. In Ambigqa: Answering ambiguous open-domain questions. CoRR, abs.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 1910. Exploring the limits of transfer learning with a unified text-to-text transformer. In Exploring the limits of transfer learning with a unified text-to-text transformer.\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. 2016. Squad: 100, 000+ questions for machine comprehension of text. In Squad: 100, 000+ questions for machine comprehension of text. abs/1606.05250\nRogers, A., Gardner, M., and Augenstein, I. 2021. QA dataset explosion: A taxonomy of NLP resources for question answering and reading comprehension. In QA dataset explosion: A taxonomy of NLP resources for question answering and reading comprehension. abs/2107.12708\nSee, A., Peter, J., Liu, C.D., and Manning 2017. Get to the point: Summarization with pointer-generator networks. In Get to the point: Summarization with pointer-generator networks. arXiv:1704.04368\nVinyals, O., Fortunato, M., and Jaitly, N. 2017.\nVoorhees, E. M. 1999. The TREC-8 question answering track report. In Proceedings of The Eighth Text REtrieval Conference. TREC 1999\nWu, Y., Minervini, P., Stenetorp, P., and Riedel, S. 2021. Training adaptive computation for open-domain question answering with computational constraints. In Training adaptive computation for open-domain question answering with computational constraints. abs/2107.02102\nYamada, I., Asai, A., and Hajishirzi, H. 2021. Efficient passage retrieval with hashing for open-domain question answering. In Efficient passage retrieval with hashing for open-domain question answering. abs/2106.00882\nYang, W., Xie, Y., Lin, A., Li, X., Tan, L., Xiong, K., Li, M., and Lin, J. 1718. End-to-end open-domain question answering with bertserini. CoRR, abs. In End-to-end open-domain question answering with bertserini. CoRR, abs.\nZhou, C., Neubig, G., Gu, J., Diab, M., Guzm\u00e1n, F., Zettlemoyer, L., and Ghazvininejad, M. 2021. Detecting hallucinated content in conditional neural sequence generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. pp. 1393--1404 10.18653/v1/2021.findings-acl.120"
            }
        ]
    },
    {
        "filename": "paper_8.txt",
        "label": "Unsupported claim",
        "text": "Early exiting is a widely used technique to accelerate inference of deep neural networks.",
        "start": 14,
        "end": 103,
        "spans_a": [
            {
                "filename": "paper_8.txt",
                "start": 14,
                "end": 103,
                "label": "Unsupported claim",
                "text": "Early exiting is a widely used technique to accelerate inference of deep neural networks.",
                "full_text": "Introduction\n\nEarly exiting is a widely used technique to accelerate inference of deep neural networks. With the rising of pre-trained language models (PLMs) (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;Qiu et al., 2020), early exiting is drawing increasing attention in the NLP community. At its core, early exiting allows simple instances to exit early while allowing hard instances to exit late. Thus, how to measure instance difficulty is a crucial problem.\n\nMost existing early exiting methods attach multiple internal classifiers to the PLM and adopt some heuristic metrics, such as entropy  (Xin et al., 2020;Liu et al., 2020a) or maximum softmax score (Schwartz et al., 2020) of internal outputs, to measure instance difficulty. However, these methods can not easily generalize to new tasks. On the one hand, these metrics are not accessible on some tasks such as regression. On the other hand, In order for these methods to perform well, one usually needs to fine-tune the threshold, which varies widely across different tasks and datasets.\n\nAnother way to measure instance difficulty is to directly learn it. Recent studies (Elbayad et al., 2020;Xin et al., 2021) that use the idea of \"learn-toexit\" have achieved promising results. They jointly train a neural model to predict for each instance the exiting layer. At their core, the learn-to-exit module is to estimate the difficulty for each instance. Compared with previous heuristically designed metrics for difficulty, learn-to-exit is task-agnostic and does not require threshold-tuning, therefore is a more promising way.\n\nDespite their success, it is still unknown whether or how well the instance difficulty can be learned. As a response, in this work, we construct datasets for two kinds of instance difficulty: (a) Humandefined difficulty, and (b) Model-defined difficulty. The dataset for human-defined difficulty has two labels, 0 for instances that can be annotated by human and 1 for instances that cannot. For modeldefined difficulty, we train a multi-exit BERT (Devlin et al., 2019), which is attached with an internal classifier at each layer, on a sentence-level classification task, SNLI (Bowman et al., 2015), and a token-level classification task, OntoNotes NER (Hovy et al., 2006). The trained multi-exit BERTs are then used to annotate for each development instance whether it can be correctly predicted by each internal classifier. Thus, our constructed sentence-level and token-level model-defined difficulty datasets are multi-label classification datasets. Experimental results demonstrate that, modern neu-ral networks perform poorly on predicting instance difficulty. This observation is consistent with previous work (Laverghetta et al., 2020) on estimating instance difficulty for curriculum learning.\n\nGiven that instance difficulty is hard to be predicted, then what works in the learn-to-exit modules? We hypothesis that the consistency between training and inference may play an important role. That is, for a training instance x i that is predicted to exit at layer l, an inference instance x j that is similar with x i should be predicted to exit at layer l, too. Since neural networks are usually smooth functions (Ziegel, 2003), this consistency can be easily satisfied by neural learn-to-exit modules. If this hypothesis holds, we can replace the learn-to-exit module with a simple hash function. In particular, we use hash functions to assign each token to a fixed exiting layer. This hash-based early exiting method is named HASHEE.\n\nCompared with previous methods that use heuristic metrics for difficulty or jointly learn to exit, HASHEE offers several advantages: (a) HASHEE requires no internal classifiers nor extra parameters, which are necessary in previous work. (b) HASHEE can perform token-level early exiting without supervision, therefore can be widely used on various tasks including language understanding and generation. (c) The speed-up ratio can be easily tuned by modifying the hash function. (d) HASHEE can significantly accelerate model inference on a per-batch basis instead of per-instance basis as in previous work Liu et al., 2020a;.\n\nWe conduct experiments on classification, regression, and generation tasks. Experimental results on ELUE (Liu et al., 2021a) demonstrate that HASHEE, despite its simplicity, can achieve higher performance with fewer FLOPs and inference time than previous state-of-the-art methods on various tasks. Besides, our experiments on several text summarization datasets show that HASHEE can reduce \u223c50% FLOPs of BART (Lewis et al., 2020) and CPT  while maintaining 97% ROUGE-1 score.\n\n References: \nSamuel, R., Bowman, G., Angeli, C., Potts, C. D., and Manning 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 632--642 10.18653/v1/d15-1075\nDaniel, M., Cer, M. T., Diab, E., Agirre, I., Lopez-Gazpio, L., and Specia 2017. Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. In Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. abs/1708.00055\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L. 2019. Universal transformers. In 7th International Conference on Learning Representations. ICLR 2019\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 4171--4186 10.18653/v1/n19-1423\nWilliam, B., Dolan, C., and Brockett 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005.\nElbayad, M., Gu, J., Grave, E., and Auli, M. 2020. Depth-adaptive transformer. In 8th International Conference on Learning Representations.\nGordon, M. A., Duh, K., and Andrews, N. 2020. Compressing BERT: studying the effects of weight pruning on transfer learning. In Proceedings of the 5th Workshop on Representation Learning for NLP, RepL4NLP@ACL 2020, Online. pp. 143--155 10.18653/v1/2020.repl4nlp-1.18\nMoritz Hermann, K., Kocisk\u00fd, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., and Blunsom, P. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. pp. 1693--1701\nHochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. In Neural Comput. pp. 1735--1780 10.1162/neco.1997.9.8.1735\nHovy, E. H., Marcus, M. P., Palmer, M., Ramshaw, L. A., and Weischedel, R. M. 2006. Ontonotes: The 90% solution. In Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings.\nHua, L., Wan, X., and Li, L. 2017. Overview of the nlpcc 2017 shared task: Single document summarization. In National CCF Conference on Natural Language Processing and Chinese Computing. pp. 942--947\nJiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., and Liu, Q. 2020. Tinybert: Distilling BERT for natural language understanding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020. pp. 4163--4174 10.18653/v1/2020.findings-emnlp.372\nKhot, T., Sabharwal, A., and Clark, P. 2018. Scitail: A textual entailment dataset from science question answering. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). pp. 5189--5197\nKim, B., Kim, H., and Kim, G. 2019. Abstractive summarization of reddit posts with multi-level memory networks. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 2519--2531 10.18653/v1/n19-1260\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In 8th International Conference on Learning Representations.\nLaverghetta, A., Mirzakhalov, J., and Licato, J. 2020. Towards a task-agnostic model of difficulty estimation for supervised learning tasks. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, AACL/IJCNLP 2021. pp. 16--23\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020.\n2020. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703\nLi, L., Lin, Y., Chen, D., Ren, S., Li, P., Zhou, J., and Sun, X. 2021. Cascadebert: Accelerating inference of pre-trained language models via calibrated complete models cascade. In Findings of EMNLP.\nLi, X., Shao, Y., Sun, T., Yan, H., Qiu, X., and Huang, X. 2021. Accelerating BERT inference for sequence labeling via earlyexit. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 189--199 10.18653/v1/2021.acl-long.16\nLiao, K., Zhang, Y., Ren, X., and Su, Q. 2021. A global past-future early exit method for accelerating inference of pretrained language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. pp. 2013--2023 10.18653/v1/2021.naacl-main.162\nLiu, W., Zhou, P., Wang, Z., Zhao, Z., Deng, H., and Ju, Q. 2020. Fastbert: a selfdistilling BERT with adaptive inference time. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. pp. 6035--6044\nLiu, X., Sun, T., He, J., Wu, L., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. In Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. abs/2110.07038\nLiu, X., Lai, H., Wong, D. F., and Chao, L. S. 2020. Norm-based curriculum learning for neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 427--436 10.18653/v1/2020.acl-main.41\nLiu, Y., Meng, F., Zhou, J., Chen, Y., and Xu, J. 2021. Faster depth-adaptive transformers. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. pp. 13424--13432\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. 2011. Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference. pp. 142--150\nMichel, P., Levy, O., and Neubig, G. 2019. Are sixteen heads really better than one?. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 14014--14024\nQiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., and Huang, X. 2020. Pre-trained models for natural language processing: A survey. In SCIENCE CHINA Technological Sciences. 10.1007/s11431-020-1647-3\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In J. Mach. Learn. Res. pp. 67\nReimers, N. and Gurevych, I. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 3980--3990 10.18653/v1/D19-1410\nRoller, S., Sukhbaatar, S., Szlam, A., and Weston, J. 2021. Hash layers for large sparse models. In Hash layers for large sparse models.\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. abs/1910.01108\nAdriaan, M. J., Schakel, B. J., and Wilson 2015. Measuring word significance using distributed representations of words. In Measuring word significance using distributed representations of words. abs/1508.02297\nSchwartz, R., Stanovsky, G., Swayamdipta, S., Dodge, J., and Smith, N. A. 2020. The right tool for the job: Matching model and instance complexities. In Proceedings of the 58th.\n2020. Annual Meeting of the Association for Computational Linguistics. In Annual Meeting of the Association for Computational Linguistics. pp. 6640--6651\nShao, Y., Geng, Z., Liu, Y., Dai, J., Yang, F., Zhe, L., Bao, H., and Qiu, X. 2021. CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. In CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. abs/2109.05729\nShen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer, K. 2020. Q-BERT: hessian based ultra low precision quantization of BERT. In The Thirty-Fourth AAAI Conference on Artificial Intelligence. pp. 8815--8821\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1631--1642\nSun, S., Cheng, Y., Gan, Z., and Liu, J. 2019. Patient knowledge distillation for BERT model compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 4322--4331 10.18653/v1/D19-1441\nSun, T., Zhou, Y., Liu, X., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. In Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. abs/2105.13792\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. pp. 5998--6008\nWang, R., Cheng, M., Chen, X., Tang, X., and Hsieh, C. 2021. Rethinking architecture selection in differentiable NAS. In 9th International Conference on Learning Representations.\nXin, J., Tang, R., Lee, J., Yu, Y., and Lin, J. 2020. Deebert: Dynamic early exiting for accelerating BERT inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 2246--2251\nXin, J., Tang, R., Yu, Y., and Lin, J. 2021. Berxit: Early exiting for BERT with better finetuning and extension to regression. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main. pp. 91--104\nXu, C., Zhou, W., Ge, T., Wei, F., and Zhou, M. 2020. Bert-of-theseus: Compressing BERT by progressive module replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. pp. 7859--7869 10.18653/v1/2020.emnlp-main.633\nXu, L., Hu, H., Zhang, X., Li, L., Cao, C., Li, Y., Xu, Y., Sun, K., Yu, D., Yu, C., Tian, Y., Dong, Q., Liu, W., Shi, B., Cui, Y., Li, J., Zeng, J., Wang, R., Xie, W., Li, Y., Patterson, Y., Tian, Z., Zhang, Y., Zhou, H., Liu, S., Zhao, Z., Zhao, Q., Yue, C., Zhang, X., Yang, Z., Richardson, K., and Lan, Z. 2020. CLUE: A chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics. pp. 4762--4772 10.18653/v1/2020.coling-main.419\nYang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., Quoc, V., and Le 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 5754--5764\nZhang, C., Yu, M., Wang, W., and Yan, F. 2019. Mark: Exploiting cloud services for costeffective, slo-aware machine learning inference serving. In 2019 USENIX Annual Technical Conference, USENIX ATC 2019. pp. 1049--1062\nZhou, W., Xu, C., Ge, T., Mcauley, J. J., Xu, K., and Wei, F. 2020. BERT loses patience: Fast and robust inference with early exit. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020.\nZhu, W. 2021. Leebert: Learned early exit for BERT with cross-level optimization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 2968--2980 10.18653/v1/2021.acl-long.231\nZiegel, E. R. 2003. The elements of statistical learning. In Technometrics. pp. 267--268 10.1198/tech.2003.s770"
            }
        ],
        "spans_b": [
            {
                "filename": "paper_8.txt",
                "start": 14,
                "end": 103,
                "label": "Unsupported claim",
                "text": "Early exiting is a widely used technique to accelerate inference of deep neural networks.",
                "full_text": "Introduction\n\nEarly exiting is a widely used technique to accelerate inference of deep neural networks. With the rising of pre-trained language models (PLMs) (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;Qiu et al., 2020), early exiting is drawing increasing attention in the NLP community. At its core, early exiting allows simple instances to exit early while allowing hard instances to exit late. Thus, how to measure instance difficulty is a crucial problem.\n\nMost existing early exiting methods attach multiple internal classifiers to the PLM and adopt some heuristic metrics, such as entropy  (Xin et al., 2020;Liu et al., 2020a) or maximum softmax score (Schwartz et al., 2020) of internal outputs, to measure instance difficulty. However, these methods can not easily generalize to new tasks. On the one hand, these metrics are not accessible on some tasks such as regression. On the other hand, In order for these methods to perform well, one usually needs to fine-tune the threshold, which varies widely across different tasks and datasets.\n\nAnother way to measure instance difficulty is to directly learn it. Recent studies (Elbayad et al., 2020;Xin et al., 2021) that use the idea of \"learn-toexit\" have achieved promising results. They jointly train a neural model to predict for each instance the exiting layer. At their core, the learn-to-exit module is to estimate the difficulty for each instance. Compared with previous heuristically designed metrics for difficulty, learn-to-exit is task-agnostic and does not require threshold-tuning, therefore is a more promising way.\n\nDespite their success, it is still unknown whether or how well the instance difficulty can be learned. As a response, in this work, we construct datasets for two kinds of instance difficulty: (a) Humandefined difficulty, and (b) Model-defined difficulty. The dataset for human-defined difficulty has two labels, 0 for instances that can be annotated by human and 1 for instances that cannot. For modeldefined difficulty, we train a multi-exit BERT (Devlin et al., 2019), which is attached with an internal classifier at each layer, on a sentence-level classification task, SNLI (Bowman et al., 2015), and a token-level classification task, OntoNotes NER (Hovy et al., 2006). The trained multi-exit BERTs are then used to annotate for each development instance whether it can be correctly predicted by each internal classifier. Thus, our constructed sentence-level and token-level model-defined difficulty datasets are multi-label classification datasets. Experimental results demonstrate that, modern neu-ral networks perform poorly on predicting instance difficulty. This observation is consistent with previous work (Laverghetta et al., 2020) on estimating instance difficulty for curriculum learning.\n\nGiven that instance difficulty is hard to be predicted, then what works in the learn-to-exit modules? We hypothesis that the consistency between training and inference may play an important role. That is, for a training instance x i that is predicted to exit at layer l, an inference instance x j that is similar with x i should be predicted to exit at layer l, too. Since neural networks are usually smooth functions (Ziegel, 2003), this consistency can be easily satisfied by neural learn-to-exit modules. If this hypothesis holds, we can replace the learn-to-exit module with a simple hash function. In particular, we use hash functions to assign each token to a fixed exiting layer. This hash-based early exiting method is named HASHEE.\n\nCompared with previous methods that use heuristic metrics for difficulty or jointly learn to exit, HASHEE offers several advantages: (a) HASHEE requires no internal classifiers nor extra parameters, which are necessary in previous work. (b) HASHEE can perform token-level early exiting without supervision, therefore can be widely used on various tasks including language understanding and generation. (c) The speed-up ratio can be easily tuned by modifying the hash function. (d) HASHEE can significantly accelerate model inference on a per-batch basis instead of per-instance basis as in previous work Liu et al., 2020a;.\n\nWe conduct experiments on classification, regression, and generation tasks. Experimental results on ELUE (Liu et al., 2021a) demonstrate that HASHEE, despite its simplicity, can achieve higher performance with fewer FLOPs and inference time than previous state-of-the-art methods on various tasks. Besides, our experiments on several text summarization datasets show that HASHEE can reduce \u223c50% FLOPs of BART (Lewis et al., 2020) and CPT  while maintaining 97% ROUGE-1 score.\n\n References: \nSamuel, R., Bowman, G., Angeli, C., Potts, C. D., and Manning 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 632--642 10.18653/v1/d15-1075\nDaniel, M., Cer, M. T., Diab, E., Agirre, I., Lopez-Gazpio, L., and Specia 2017. Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. In Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. abs/1708.00055\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L. 2019. Universal transformers. In 7th International Conference on Learning Representations. ICLR 2019\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 4171--4186 10.18653/v1/n19-1423\nWilliam, B., Dolan, C., and Brockett 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005.\nElbayad, M., Gu, J., Grave, E., and Auli, M. 2020. Depth-adaptive transformer. In 8th International Conference on Learning Representations.\nGordon, M. A., Duh, K., and Andrews, N. 2020. Compressing BERT: studying the effects of weight pruning on transfer learning. In Proceedings of the 5th Workshop on Representation Learning for NLP, RepL4NLP@ACL 2020, Online. pp. 143--155 10.18653/v1/2020.repl4nlp-1.18\nMoritz Hermann, K., Kocisk\u00fd, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., and Blunsom, P. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. pp. 1693--1701\nHochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. In Neural Comput. pp. 1735--1780 10.1162/neco.1997.9.8.1735\nHovy, E. H., Marcus, M. P., Palmer, M., Ramshaw, L. A., and Weischedel, R. M. 2006. Ontonotes: The 90% solution. In Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings.\nHua, L., Wan, X., and Li, L. 2017. Overview of the nlpcc 2017 shared task: Single document summarization. In National CCF Conference on Natural Language Processing and Chinese Computing. pp. 942--947\nJiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., and Liu, Q. 2020. Tinybert: Distilling BERT for natural language understanding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020. pp. 4163--4174 10.18653/v1/2020.findings-emnlp.372\nKhot, T., Sabharwal, A., and Clark, P. 2018. Scitail: A textual entailment dataset from science question answering. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). pp. 5189--5197\nKim, B., Kim, H., and Kim, G. 2019. Abstractive summarization of reddit posts with multi-level memory networks. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 2519--2531 10.18653/v1/n19-1260\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In 8th International Conference on Learning Representations.\nLaverghetta, A., Mirzakhalov, J., and Licato, J. 2020. Towards a task-agnostic model of difficulty estimation for supervised learning tasks. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, AACL/IJCNLP 2021. pp. 16--23\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020.\n2020. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703\nLi, L., Lin, Y., Chen, D., Ren, S., Li, P., Zhou, J., and Sun, X. 2021. Cascadebert: Accelerating inference of pre-trained language models via calibrated complete models cascade. In Findings of EMNLP.\nLi, X., Shao, Y., Sun, T., Yan, H., Qiu, X., and Huang, X. 2021. Accelerating BERT inference for sequence labeling via earlyexit. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 189--199 10.18653/v1/2021.acl-long.16\nLiao, K., Zhang, Y., Ren, X., and Su, Q. 2021. A global past-future early exit method for accelerating inference of pretrained language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. pp. 2013--2023 10.18653/v1/2021.naacl-main.162\nLiu, W., Zhou, P., Wang, Z., Zhao, Z., Deng, H., and Ju, Q. 2020. Fastbert: a selfdistilling BERT with adaptive inference time. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. pp. 6035--6044\nLiu, X., Sun, T., He, J., Wu, L., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. In Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. abs/2110.07038\nLiu, X., Lai, H., Wong, D. F., and Chao, L. S. 2020. Norm-based curriculum learning for neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 427--436 10.18653/v1/2020.acl-main.41\nLiu, Y., Meng, F., Zhou, J., Chen, Y., and Xu, J. 2021. Faster depth-adaptive transformers. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. pp. 13424--13432\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. 2011. Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference. pp. 142--150\nMichel, P., Levy, O., and Neubig, G. 2019. Are sixteen heads really better than one?. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 14014--14024\nQiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., and Huang, X. 2020. Pre-trained models for natural language processing: A survey. In SCIENCE CHINA Technological Sciences. 10.1007/s11431-020-1647-3\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In J. Mach. Learn. Res. pp. 67\nReimers, N. and Gurevych, I. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 3980--3990 10.18653/v1/D19-1410\nRoller, S., Sukhbaatar, S., Szlam, A., and Weston, J. 2021. Hash layers for large sparse models. In Hash layers for large sparse models.\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. abs/1910.01108\nAdriaan, M. J., Schakel, B. J., and Wilson 2015. Measuring word significance using distributed representations of words. In Measuring word significance using distributed representations of words. abs/1508.02297\nSchwartz, R., Stanovsky, G., Swayamdipta, S., Dodge, J., and Smith, N. A. 2020. The right tool for the job: Matching model and instance complexities. In Proceedings of the 58th.\n2020. Annual Meeting of the Association for Computational Linguistics. In Annual Meeting of the Association for Computational Linguistics. pp. 6640--6651\nShao, Y., Geng, Z., Liu, Y., Dai, J., Yang, F., Zhe, L., Bao, H., and Qiu, X. 2021. CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. In CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. abs/2109.05729\nShen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer, K. 2020. Q-BERT: hessian based ultra low precision quantization of BERT. In The Thirty-Fourth AAAI Conference on Artificial Intelligence. pp. 8815--8821\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1631--1642\nSun, S., Cheng, Y., Gan, Z., and Liu, J. 2019. Patient knowledge distillation for BERT model compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 4322--4331 10.18653/v1/D19-1441\nSun, T., Zhou, Y., Liu, X., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. In Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. abs/2105.13792\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. pp. 5998--6008\nWang, R., Cheng, M., Chen, X., Tang, X., and Hsieh, C. 2021. Rethinking architecture selection in differentiable NAS. In 9th International Conference on Learning Representations.\nXin, J., Tang, R., Lee, J., Yu, Y., and Lin, J. 2020. Deebert: Dynamic early exiting for accelerating BERT inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 2246--2251\nXin, J., Tang, R., Yu, Y., and Lin, J. 2021. Berxit: Early exiting for BERT with better finetuning and extension to regression. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main. pp. 91--104\nXu, C., Zhou, W., Ge, T., Wei, F., and Zhou, M. 2020. Bert-of-theseus: Compressing BERT by progressive module replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. pp. 7859--7869 10.18653/v1/2020.emnlp-main.633\nXu, L., Hu, H., Zhang, X., Li, L., Cao, C., Li, Y., Xu, Y., Sun, K., Yu, D., Yu, C., Tian, Y., Dong, Q., Liu, W., Shi, B., Cui, Y., Li, J., Zeng, J., Wang, R., Xie, W., Li, Y., Patterson, Y., Tian, Z., Zhang, Y., Zhou, H., Liu, S., Zhao, Z., Zhao, Q., Yue, C., Zhang, X., Yang, Z., Richardson, K., and Lan, Z. 2020. CLUE: A chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics. pp. 4762--4772 10.18653/v1/2020.coling-main.419\nYang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., Quoc, V., and Le 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 5754--5764\nZhang, C., Yu, M., Wang, W., and Yan, F. 2019. Mark: Exploiting cloud services for costeffective, slo-aware machine learning inference serving. In 2019 USENIX Annual Technical Conference, USENIX ATC 2019. pp. 1049--1062\nZhou, W., Xu, C., Ge, T., Mcauley, J. J., Xu, K., and Wei, F. 2020. BERT loses patience: Fast and robust inference with early exit. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020.\nZhu, W. 2021. Leebert: Learned early exit for BERT with cross-level optimization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 2968--2980 10.18653/v1/2021.acl-long.231\nZiegel, E. R. 2003. The elements of statistical learning. In Technometrics. pp. 267--268 10.1198/tech.2003.s770"
            }
        ]
    },
    {
        "filename": "paper_8.txt",
        "label": "Unsupported claim",
        "text": "However, these methods can not easily generalize to new tasks.",
        "start": 732,
        "end": 794,
        "spans_a": [
            {
                "filename": "paper_8.txt",
                "start": 732,
                "end": 794,
                "label": "Unsupported claim",
                "text": "However, these methods can not easily generalize to new tasks.",
                "full_text": "Introduction\n\nEarly exiting is a widely used technique to accelerate inference of deep neural networks. With the rising of pre-trained language models (PLMs) (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;Qiu et al., 2020), early exiting is drawing increasing attention in the NLP community. At its core, early exiting allows simple instances to exit early while allowing hard instances to exit late. Thus, how to measure instance difficulty is a crucial problem.\n\nMost existing early exiting methods attach multiple internal classifiers to the PLM and adopt some heuristic metrics, such as entropy  (Xin et al., 2020;Liu et al., 2020a) or maximum softmax score (Schwartz et al., 2020) of internal outputs, to measure instance difficulty. However, these methods can not easily generalize to new tasks. On the one hand, these metrics are not accessible on some tasks such as regression. On the other hand, In order for these methods to perform well, one usually needs to fine-tune the threshold, which varies widely across different tasks and datasets.\n\nAnother way to measure instance difficulty is to directly learn it. Recent studies (Elbayad et al., 2020;Xin et al., 2021) that use the idea of \"learn-toexit\" have achieved promising results. They jointly train a neural model to predict for each instance the exiting layer. At their core, the learn-to-exit module is to estimate the difficulty for each instance. Compared with previous heuristically designed metrics for difficulty, learn-to-exit is task-agnostic and does not require threshold-tuning, therefore is a more promising way.\n\nDespite their success, it is still unknown whether or how well the instance difficulty can be learned. As a response, in this work, we construct datasets for two kinds of instance difficulty: (a) Humandefined difficulty, and (b) Model-defined difficulty. The dataset for human-defined difficulty has two labels, 0 for instances that can be annotated by human and 1 for instances that cannot. For modeldefined difficulty, we train a multi-exit BERT (Devlin et al., 2019), which is attached with an internal classifier at each layer, on a sentence-level classification task, SNLI (Bowman et al., 2015), and a token-level classification task, OntoNotes NER (Hovy et al., 2006). The trained multi-exit BERTs are then used to annotate for each development instance whether it can be correctly predicted by each internal classifier. Thus, our constructed sentence-level and token-level model-defined difficulty datasets are multi-label classification datasets. Experimental results demonstrate that, modern neu-ral networks perform poorly on predicting instance difficulty. This observation is consistent with previous work (Laverghetta et al., 2020) on estimating instance difficulty for curriculum learning.\n\nGiven that instance difficulty is hard to be predicted, then what works in the learn-to-exit modules? We hypothesis that the consistency between training and inference may play an important role. That is, for a training instance x i that is predicted to exit at layer l, an inference instance x j that is similar with x i should be predicted to exit at layer l, too. Since neural networks are usually smooth functions (Ziegel, 2003), this consistency can be easily satisfied by neural learn-to-exit modules. If this hypothesis holds, we can replace the learn-to-exit module with a simple hash function. In particular, we use hash functions to assign each token to a fixed exiting layer. This hash-based early exiting method is named HASHEE.\n\nCompared with previous methods that use heuristic metrics for difficulty or jointly learn to exit, HASHEE offers several advantages: (a) HASHEE requires no internal classifiers nor extra parameters, which are necessary in previous work. (b) HASHEE can perform token-level early exiting without supervision, therefore can be widely used on various tasks including language understanding and generation. (c) The speed-up ratio can be easily tuned by modifying the hash function. (d) HASHEE can significantly accelerate model inference on a per-batch basis instead of per-instance basis as in previous work Liu et al., 2020a;.\n\nWe conduct experiments on classification, regression, and generation tasks. Experimental results on ELUE (Liu et al., 2021a) demonstrate that HASHEE, despite its simplicity, can achieve higher performance with fewer FLOPs and inference time than previous state-of-the-art methods on various tasks. Besides, our experiments on several text summarization datasets show that HASHEE can reduce \u223c50% FLOPs of BART (Lewis et al., 2020) and CPT  while maintaining 97% ROUGE-1 score.\n\n References: \nSamuel, R., Bowman, G., Angeli, C., Potts, C. D., and Manning 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 632--642 10.18653/v1/d15-1075\nDaniel, M., Cer, M. T., Diab, E., Agirre, I., Lopez-Gazpio, L., and Specia 2017. Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. In Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. abs/1708.00055\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L. 2019. Universal transformers. In 7th International Conference on Learning Representations. ICLR 2019\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 4171--4186 10.18653/v1/n19-1423\nWilliam, B., Dolan, C., and Brockett 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005.\nElbayad, M., Gu, J., Grave, E., and Auli, M. 2020. Depth-adaptive transformer. In 8th International Conference on Learning Representations.\nGordon, M. A., Duh, K., and Andrews, N. 2020. Compressing BERT: studying the effects of weight pruning on transfer learning. In Proceedings of the 5th Workshop on Representation Learning for NLP, RepL4NLP@ACL 2020, Online. pp. 143--155 10.18653/v1/2020.repl4nlp-1.18\nMoritz Hermann, K., Kocisk\u00fd, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., and Blunsom, P. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. pp. 1693--1701\nHochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. In Neural Comput. pp. 1735--1780 10.1162/neco.1997.9.8.1735\nHovy, E. H., Marcus, M. P., Palmer, M., Ramshaw, L. A., and Weischedel, R. M. 2006. Ontonotes: The 90% solution. In Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings.\nHua, L., Wan, X., and Li, L. 2017. Overview of the nlpcc 2017 shared task: Single document summarization. In National CCF Conference on Natural Language Processing and Chinese Computing. pp. 942--947\nJiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., and Liu, Q. 2020. Tinybert: Distilling BERT for natural language understanding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020. pp. 4163--4174 10.18653/v1/2020.findings-emnlp.372\nKhot, T., Sabharwal, A., and Clark, P. 2018. Scitail: A textual entailment dataset from science question answering. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). pp. 5189--5197\nKim, B., Kim, H., and Kim, G. 2019. Abstractive summarization of reddit posts with multi-level memory networks. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 2519--2531 10.18653/v1/n19-1260\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In 8th International Conference on Learning Representations.\nLaverghetta, A., Mirzakhalov, J., and Licato, J. 2020. Towards a task-agnostic model of difficulty estimation for supervised learning tasks. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, AACL/IJCNLP 2021. pp. 16--23\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020.\n2020. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703\nLi, L., Lin, Y., Chen, D., Ren, S., Li, P., Zhou, J., and Sun, X. 2021. Cascadebert: Accelerating inference of pre-trained language models via calibrated complete models cascade. In Findings of EMNLP.\nLi, X., Shao, Y., Sun, T., Yan, H., Qiu, X., and Huang, X. 2021. Accelerating BERT inference for sequence labeling via earlyexit. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 189--199 10.18653/v1/2021.acl-long.16\nLiao, K., Zhang, Y., Ren, X., and Su, Q. 2021. A global past-future early exit method for accelerating inference of pretrained language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. pp. 2013--2023 10.18653/v1/2021.naacl-main.162\nLiu, W., Zhou, P., Wang, Z., Zhao, Z., Deng, H., and Ju, Q. 2020. Fastbert: a selfdistilling BERT with adaptive inference time. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. pp. 6035--6044\nLiu, X., Sun, T., He, J., Wu, L., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. In Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. abs/2110.07038\nLiu, X., Lai, H., Wong, D. F., and Chao, L. S. 2020. Norm-based curriculum learning for neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 427--436 10.18653/v1/2020.acl-main.41\nLiu, Y., Meng, F., Zhou, J., Chen, Y., and Xu, J. 2021. Faster depth-adaptive transformers. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. pp. 13424--13432\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. 2011. Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference. pp. 142--150\nMichel, P., Levy, O., and Neubig, G. 2019. Are sixteen heads really better than one?. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 14014--14024\nQiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., and Huang, X. 2020. Pre-trained models for natural language processing: A survey. In SCIENCE CHINA Technological Sciences. 10.1007/s11431-020-1647-3\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In J. Mach. Learn. Res. pp. 67\nReimers, N. and Gurevych, I. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 3980--3990 10.18653/v1/D19-1410\nRoller, S., Sukhbaatar, S., Szlam, A., and Weston, J. 2021. Hash layers for large sparse models. In Hash layers for large sparse models.\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. abs/1910.01108\nAdriaan, M. J., Schakel, B. J., and Wilson 2015. Measuring word significance using distributed representations of words. In Measuring word significance using distributed representations of words. abs/1508.02297\nSchwartz, R., Stanovsky, G., Swayamdipta, S., Dodge, J., and Smith, N. A. 2020. The right tool for the job: Matching model and instance complexities. In Proceedings of the 58th.\n2020. Annual Meeting of the Association for Computational Linguistics. In Annual Meeting of the Association for Computational Linguistics. pp. 6640--6651\nShao, Y., Geng, Z., Liu, Y., Dai, J., Yang, F., Zhe, L., Bao, H., and Qiu, X. 2021. CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. In CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. abs/2109.05729\nShen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer, K. 2020. Q-BERT: hessian based ultra low precision quantization of BERT. In The Thirty-Fourth AAAI Conference on Artificial Intelligence. pp. 8815--8821\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1631--1642\nSun, S., Cheng, Y., Gan, Z., and Liu, J. 2019. Patient knowledge distillation for BERT model compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 4322--4331 10.18653/v1/D19-1441\nSun, T., Zhou, Y., Liu, X., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. In Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. abs/2105.13792\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. pp. 5998--6008\nWang, R., Cheng, M., Chen, X., Tang, X., and Hsieh, C. 2021. Rethinking architecture selection in differentiable NAS. In 9th International Conference on Learning Representations.\nXin, J., Tang, R., Lee, J., Yu, Y., and Lin, J. 2020. Deebert: Dynamic early exiting for accelerating BERT inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 2246--2251\nXin, J., Tang, R., Yu, Y., and Lin, J. 2021. Berxit: Early exiting for BERT with better finetuning and extension to regression. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main. pp. 91--104\nXu, C., Zhou, W., Ge, T., Wei, F., and Zhou, M. 2020. Bert-of-theseus: Compressing BERT by progressive module replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. pp. 7859--7869 10.18653/v1/2020.emnlp-main.633\nXu, L., Hu, H., Zhang, X., Li, L., Cao, C., Li, Y., Xu, Y., Sun, K., Yu, D., Yu, C., Tian, Y., Dong, Q., Liu, W., Shi, B., Cui, Y., Li, J., Zeng, J., Wang, R., Xie, W., Li, Y., Patterson, Y., Tian, Z., Zhang, Y., Zhou, H., Liu, S., Zhao, Z., Zhao, Q., Yue, C., Zhang, X., Yang, Z., Richardson, K., and Lan, Z. 2020. CLUE: A chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics. pp. 4762--4772 10.18653/v1/2020.coling-main.419\nYang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., Quoc, V., and Le 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 5754--5764\nZhang, C., Yu, M., Wang, W., and Yan, F. 2019. Mark: Exploiting cloud services for costeffective, slo-aware machine learning inference serving. In 2019 USENIX Annual Technical Conference, USENIX ATC 2019. pp. 1049--1062\nZhou, W., Xu, C., Ge, T., Mcauley, J. J., Xu, K., and Wei, F. 2020. BERT loses patience: Fast and robust inference with early exit. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020.\nZhu, W. 2021. Leebert: Learned early exit for BERT with cross-level optimization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 2968--2980 10.18653/v1/2021.acl-long.231\nZiegel, E. R. 2003. The elements of statistical learning. In Technometrics. pp. 267--268 10.1198/tech.2003.s770"
            }
        ],
        "spans_b": [
            {
                "filename": "paper_8.txt",
                "start": 732,
                "end": 794,
                "label": "Unsupported claim",
                "text": "However, these methods can not easily generalize to new tasks.",
                "full_text": "Introduction\n\nEarly exiting is a widely used technique to accelerate inference of deep neural networks. With the rising of pre-trained language models (PLMs) (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;Qiu et al., 2020), early exiting is drawing increasing attention in the NLP community. At its core, early exiting allows simple instances to exit early while allowing hard instances to exit late. Thus, how to measure instance difficulty is a crucial problem.\n\nMost existing early exiting methods attach multiple internal classifiers to the PLM and adopt some heuristic metrics, such as entropy  (Xin et al., 2020;Liu et al., 2020a) or maximum softmax score (Schwartz et al., 2020) of internal outputs, to measure instance difficulty. However, these methods can not easily generalize to new tasks. On the one hand, these metrics are not accessible on some tasks such as regression. On the other hand, In order for these methods to perform well, one usually needs to fine-tune the threshold, which varies widely across different tasks and datasets.\n\nAnother way to measure instance difficulty is to directly learn it. Recent studies (Elbayad et al., 2020;Xin et al., 2021) that use the idea of \"learn-toexit\" have achieved promising results. They jointly train a neural model to predict for each instance the exiting layer. At their core, the learn-to-exit module is to estimate the difficulty for each instance. Compared with previous heuristically designed metrics for difficulty, learn-to-exit is task-agnostic and does not require threshold-tuning, therefore is a more promising way.\n\nDespite their success, it is still unknown whether or how well the instance difficulty can be learned. As a response, in this work, we construct datasets for two kinds of instance difficulty: (a) Humandefined difficulty, and (b) Model-defined difficulty. The dataset for human-defined difficulty has two labels, 0 for instances that can be annotated by human and 1 for instances that cannot. For modeldefined difficulty, we train a multi-exit BERT (Devlin et al., 2019), which is attached with an internal classifier at each layer, on a sentence-level classification task, SNLI (Bowman et al., 2015), and a token-level classification task, OntoNotes NER (Hovy et al., 2006). The trained multi-exit BERTs are then used to annotate for each development instance whether it can be correctly predicted by each internal classifier. Thus, our constructed sentence-level and token-level model-defined difficulty datasets are multi-label classification datasets. Experimental results demonstrate that, modern neu-ral networks perform poorly on predicting instance difficulty. This observation is consistent with previous work (Laverghetta et al., 2020) on estimating instance difficulty for curriculum learning.\n\nGiven that instance difficulty is hard to be predicted, then what works in the learn-to-exit modules? We hypothesis that the consistency between training and inference may play an important role. That is, for a training instance x i that is predicted to exit at layer l, an inference instance x j that is similar with x i should be predicted to exit at layer l, too. Since neural networks are usually smooth functions (Ziegel, 2003), this consistency can be easily satisfied by neural learn-to-exit modules. If this hypothesis holds, we can replace the learn-to-exit module with a simple hash function. In particular, we use hash functions to assign each token to a fixed exiting layer. This hash-based early exiting method is named HASHEE.\n\nCompared with previous methods that use heuristic metrics for difficulty or jointly learn to exit, HASHEE offers several advantages: (a) HASHEE requires no internal classifiers nor extra parameters, which are necessary in previous work. (b) HASHEE can perform token-level early exiting without supervision, therefore can be widely used on various tasks including language understanding and generation. (c) The speed-up ratio can be easily tuned by modifying the hash function. (d) HASHEE can significantly accelerate model inference on a per-batch basis instead of per-instance basis as in previous work Liu et al., 2020a;.\n\nWe conduct experiments on classification, regression, and generation tasks. Experimental results on ELUE (Liu et al., 2021a) demonstrate that HASHEE, despite its simplicity, can achieve higher performance with fewer FLOPs and inference time than previous state-of-the-art methods on various tasks. Besides, our experiments on several text summarization datasets show that HASHEE can reduce \u223c50% FLOPs of BART (Lewis et al., 2020) and CPT  while maintaining 97% ROUGE-1 score.\n\n References: \nSamuel, R., Bowman, G., Angeli, C., Potts, C. D., and Manning 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 632--642 10.18653/v1/d15-1075\nDaniel, M., Cer, M. T., Diab, E., Agirre, I., Lopez-Gazpio, L., and Specia 2017. Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. In Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. abs/1708.00055\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L. 2019. Universal transformers. In 7th International Conference on Learning Representations. ICLR 2019\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 4171--4186 10.18653/v1/n19-1423\nWilliam, B., Dolan, C., and Brockett 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005.\nElbayad, M., Gu, J., Grave, E., and Auli, M. 2020. Depth-adaptive transformer. In 8th International Conference on Learning Representations.\nGordon, M. A., Duh, K., and Andrews, N. 2020. Compressing BERT: studying the effects of weight pruning on transfer learning. In Proceedings of the 5th Workshop on Representation Learning for NLP, RepL4NLP@ACL 2020, Online. pp. 143--155 10.18653/v1/2020.repl4nlp-1.18\nMoritz Hermann, K., Kocisk\u00fd, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., and Blunsom, P. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. pp. 1693--1701\nHochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. In Neural Comput. pp. 1735--1780 10.1162/neco.1997.9.8.1735\nHovy, E. H., Marcus, M. P., Palmer, M., Ramshaw, L. A., and Weischedel, R. M. 2006. Ontonotes: The 90% solution. In Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings.\nHua, L., Wan, X., and Li, L. 2017. Overview of the nlpcc 2017 shared task: Single document summarization. In National CCF Conference on Natural Language Processing and Chinese Computing. pp. 942--947\nJiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., and Liu, Q. 2020. Tinybert: Distilling BERT for natural language understanding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020. pp. 4163--4174 10.18653/v1/2020.findings-emnlp.372\nKhot, T., Sabharwal, A., and Clark, P. 2018. Scitail: A textual entailment dataset from science question answering. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). pp. 5189--5197\nKim, B., Kim, H., and Kim, G. 2019. Abstractive summarization of reddit posts with multi-level memory networks. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 2519--2531 10.18653/v1/n19-1260\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In 8th International Conference on Learning Representations.\nLaverghetta, A., Mirzakhalov, J., and Licato, J. 2020. Towards a task-agnostic model of difficulty estimation for supervised learning tasks. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, AACL/IJCNLP 2021. pp. 16--23\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020.\n2020. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703\nLi, L., Lin, Y., Chen, D., Ren, S., Li, P., Zhou, J., and Sun, X. 2021. Cascadebert: Accelerating inference of pre-trained language models via calibrated complete models cascade. In Findings of EMNLP.\nLi, X., Shao, Y., Sun, T., Yan, H., Qiu, X., and Huang, X. 2021. Accelerating BERT inference for sequence labeling via earlyexit. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 189--199 10.18653/v1/2021.acl-long.16\nLiao, K., Zhang, Y., Ren, X., and Su, Q. 2021. A global past-future early exit method for accelerating inference of pretrained language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. pp. 2013--2023 10.18653/v1/2021.naacl-main.162\nLiu, W., Zhou, P., Wang, Z., Zhao, Z., Deng, H., and Ju, Q. 2020. Fastbert: a selfdistilling BERT with adaptive inference time. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. pp. 6035--6044\nLiu, X., Sun, T., He, J., Wu, L., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. In Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. abs/2110.07038\nLiu, X., Lai, H., Wong, D. F., and Chao, L. S. 2020. Norm-based curriculum learning for neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 427--436 10.18653/v1/2020.acl-main.41\nLiu, Y., Meng, F., Zhou, J., Chen, Y., and Xu, J. 2021. Faster depth-adaptive transformers. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. pp. 13424--13432\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. 2011. Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference. pp. 142--150\nMichel, P., Levy, O., and Neubig, G. 2019. Are sixteen heads really better than one?. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 14014--14024\nQiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., and Huang, X. 2020. Pre-trained models for natural language processing: A survey. In SCIENCE CHINA Technological Sciences. 10.1007/s11431-020-1647-3\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In J. Mach. Learn. Res. pp. 67\nReimers, N. and Gurevych, I. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 3980--3990 10.18653/v1/D19-1410\nRoller, S., Sukhbaatar, S., Szlam, A., and Weston, J. 2021. Hash layers for large sparse models. In Hash layers for large sparse models.\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. abs/1910.01108\nAdriaan, M. J., Schakel, B. J., and Wilson 2015. Measuring word significance using distributed representations of words. In Measuring word significance using distributed representations of words. abs/1508.02297\nSchwartz, R., Stanovsky, G., Swayamdipta, S., Dodge, J., and Smith, N. A. 2020. The right tool for the job: Matching model and instance complexities. In Proceedings of the 58th.\n2020. Annual Meeting of the Association for Computational Linguistics. In Annual Meeting of the Association for Computational Linguistics. pp. 6640--6651\nShao, Y., Geng, Z., Liu, Y., Dai, J., Yang, F., Zhe, L., Bao, H., and Qiu, X. 2021. CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. In CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. abs/2109.05729\nShen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer, K. 2020. Q-BERT: hessian based ultra low precision quantization of BERT. In The Thirty-Fourth AAAI Conference on Artificial Intelligence. pp. 8815--8821\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1631--1642\nSun, S., Cheng, Y., Gan, Z., and Liu, J. 2019. Patient knowledge distillation for BERT model compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 4322--4331 10.18653/v1/D19-1441\nSun, T., Zhou, Y., Liu, X., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. In Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. abs/2105.13792\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. pp. 5998--6008\nWang, R., Cheng, M., Chen, X., Tang, X., and Hsieh, C. 2021. Rethinking architecture selection in differentiable NAS. In 9th International Conference on Learning Representations.\nXin, J., Tang, R., Lee, J., Yu, Y., and Lin, J. 2020. Deebert: Dynamic early exiting for accelerating BERT inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 2246--2251\nXin, J., Tang, R., Yu, Y., and Lin, J. 2021. Berxit: Early exiting for BERT with better finetuning and extension to regression. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main. pp. 91--104\nXu, C., Zhou, W., Ge, T., Wei, F., and Zhou, M. 2020. Bert-of-theseus: Compressing BERT by progressive module replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. pp. 7859--7869 10.18653/v1/2020.emnlp-main.633\nXu, L., Hu, H., Zhang, X., Li, L., Cao, C., Li, Y., Xu, Y., Sun, K., Yu, D., Yu, C., Tian, Y., Dong, Q., Liu, W., Shi, B., Cui, Y., Li, J., Zeng, J., Wang, R., Xie, W., Li, Y., Patterson, Y., Tian, Z., Zhang, Y., Zhou, H., Liu, S., Zhao, Z., Zhao, Q., Yue, C., Zhang, X., Yang, Z., Richardson, K., and Lan, Z. 2020. CLUE: A chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics. pp. 4762--4772 10.18653/v1/2020.coling-main.419\nYang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., Quoc, V., and Le 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 5754--5764\nZhang, C., Yu, M., Wang, W., and Yan, F. 2019. Mark: Exploiting cloud services for costeffective, slo-aware machine learning inference serving. In 2019 USENIX Annual Technical Conference, USENIX ATC 2019. pp. 1049--1062\nZhou, W., Xu, C., Ge, T., Mcauley, J. J., Xu, K., and Wei, F. 2020. BERT loses patience: Fast and robust inference with early exit. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020.\nZhu, W. 2021. Leebert: Learned early exit for BERT with cross-level optimization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 2968--2980 10.18653/v1/2021.acl-long.231\nZiegel, E. R. 2003. The elements of statistical learning. In Technometrics. pp. 267--268 10.1198/tech.2003.s770"
            }
        ]
    },
    {
        "filename": "paper_9.txt",
        "label": "Unsupported claim",
        "text": "Several approaches to zero-shot cross-lingual transfer exist and can broadly be divided into: a) Databased Transfer, which focuses on training data transformation and b) Model-based Transfer that centres around modifying models' training routine.",
        "start": 14,
        "end": 260,
        "spans_a": [
            {
                "filename": "paper_9.txt",
                "start": 14,
                "end": 260,
                "label": "Unsupported claim",
                "text": "Several approaches to zero-shot cross-lingual transfer exist and can broadly be divided into: a) Databased Transfer, which focuses on training data transformation and b) Model-based Transfer that centres around modifying models' training routine.",
                "full_text": "Related Work\n\nSeveral approaches to zero-shot cross-lingual transfer exist and can broadly be divided into: a) Databased Transfer, which focuses on training data transformation and b) Model-based Transfer that centres around modifying models' training routine.\n\nData-based Transfer Translating utterances for the intent classification task is relatively straightforward so previous works focused on projecting and/or aligning the entity labels between translated utterances (Dou and Neubig, 2021). This technique is followed by standard supervised training with those pseudo-labels and is commonly known as a translate-train method. One of the earliest works still being used for this purpose is fastalign (Dyer et al., 2013). It's an unsupervised word aligner trained on a parallel corpus to map each word (thus its entity label) in the source utterance to the word(s) in the target user utterance. Projecting the entity labels can also be done with word-by-word translation and source label copying (Yi and Cheng, 2021). A teacher model then weakly labels the target data, which is used to train the final student model. Sometimes, this type of label projection is complemented with an additional entity alignment step (Li et al., 2021a). Better performance can be achieved by using machine translation with entity matching and distributional statistics (Jain et al., 2019) though this can be a costly process for each language. A category of 'word substitution' methods such as code-switching (Qin et al., 2020;Kuwanto et al., 2021) or dictionary-enhanced pretraining  have also been shown to improve cross-lingual transfer.\n\nModel-based Transfer Prior to the adoption of multilingual transformers (Lample and Conneau, 2019), task-oriented XNLU methods employed a BiLSTM encoder combined with different multilingual embeddings (Schuster et al., 2019). Newer approaches usually involve a pretrained XLM and the addition of some new training component with the inference routine remaining mostly unchanged. Xu et al. (2020) learn to jointly align and predict entity labels by fusing the source and target language embeddings with attention and using the resulting cross-lingual representation for entity prediction. Qi and Du (2020) include an adversarial language detector in training whose loss encourages the model to generate language-agnostic sentence representations for improved zero-shot trans-fer. Pan et al. (2020) and Chi et al. (2020) added a contrastive loss to pretraining that treats translated sentences as positive examples and unrelated sentences as negative samples. This training step helps the XLM produce similar embeddings in different languages. However, these methods require large annotated datasets and expensive model pretraining (Chi et al., 2020). Our proposed methods only use the English task data (which is relatively limited) and its translations for each language.\n\nThe most related prior works are Arivazhagan et al. (2019) for machine translation and  for task-oriented XNLU. Both of these are cross-lingual alignment methods that use translated training data to zero-shot transfer the source language model to the target language. We focus on the latter work, called XeroAlign, which reported the most recent SOTA scores on our evaluation datasets. XeroAlign works by generating a sentence embedding of the user utterance for each language, e.g. English (source) and Thai (target) using the CLS token of the XLM. A Mean Squared Error loss function minimises the difference between the multilingual sentence embeddings and is backpropagated along with the main task loss. XeroAlign aims to bring sentence embeddings in different languages closer together with a bias towards intent classification due to the CLS embedding, which is the standard input to the intent classifier. We reproduce this method for analysis and comparisons but add a small post-processing step that distinctly improves the reported scores.\n\n References: \nArivazhagan, N., Bapna, A., Firat, O., Aharoni, R., Johnson, M., and Macherey, W. 2019. The missing ingredient in zero-shot neural machine translation. In The missing ingredient in zero-shot neural machine translation. arXiv:1903.07091\nArtetxe, M., Ruder, S., and Yogatama, D. 2020. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th.\nAnnual Meeting of the Association for Computational Linguistics. In Annual Meeting of the Association for Computational Linguistics. pp. 4623--4637\nArtetxe, M. and Schwenk, H. 2019. Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond. In Transactions of the Association for Computational Linguistics. pp. 597--610\nCasanueva, I., Budzianowski, P., Su, P., Mrk\u0161i\u0107, N., Wen, T., Ultes, S., Rojas-Barahona, L., Young, S., and Ga\u0161i\u0107, M. 2017. A benchmarking environment for reinforcement learning based task oriented dialogue management. In A benchmarking environment for reinforcement learning based task oriented dialogue management. arXiv:1711.11023\nChaudhary, A., Raman, K., Srinivasan, K., and Chen, J. 2020. Dict-mlm: Improved multilingual pre-training using bilingual dictionaries. In Dict-mlm: Improved multilingual pre-training using bilingual dictionaries. arXiv:2010.12566\nChen, Q., Zhuo, Z., and Wang, W. 2019. Bert for joint intent classification and slot filling. In Bert for joint intent classification and slot filling. arXiv:1902.10909\nChi, Z., Dong, L., Wei, F., Yang, N., Singhal, S., Wang, W., Song, X., Mao, X., Huang, H., and Zhou, M. 2020. foxlm: An information-theoretic framework for cross-lingual language model pre-training. In foxlm: An information-theoretic framework for cross-lingual language model pre-training. arXiv:2007.07834\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzm\u00e1n, F., Grave, \u00c9., Ott, M., Zettlemoyer, L., and Stoyanov, V. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 8440--8451\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186\nZi, Y., Dou, G., and Neubig 2021. Word alignment by fine-tuning embeddings on parallel corpora. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. pp. 2112--2128\nDyer, C., Chahuneau, V., and Smith, N.A. 2013. A simple, fast, and effective reparameterization of ibm model 2. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 644--648\nGritta, M. and Iacobacci, I. 2021. Xeroalign: Zero-shot cross-lingual transformer alignment. In Xeroalign: Zero-shot cross-lingual transformer alignment. arXiv:2105.02472\nGritta, M., Lampouras, G., and Iacobacci, I. 2021. Conversation graph: Data augmentation, training, and evaluation for non-deterministic dialogue management. In Transactions of the Association for Computational Linguistics. pp. 36--52\nGroenendijk, R., Karaoglu, S., Gevers, T., and Mensink, T. 2021. Multi-loss weighting with coefficient of variations. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 1469--1478\nHu, J., Ruder, S., Siddhant, A., Neubig, G., Firat, O., and Johnson, M. 2020. Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In International Conference on Machine Learning. pp. 4411--4421\nJain, A., Paranjape, B., and ZacharyC 2019. Entity projection via machine translation for cross-lingual NER. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 1083--1092 10.18653/v1/D19-1100\nKuwanto, G., Feyza Aky\u00fcrek, A., Tourni, I. C., Li, S., and Wijaya, D. 2021. Low-resource machine translation for low-resource languages: Leveraging comparable data, codeswitching and compute resources. In Low-resource machine translation for low-resource languages: Leveraging comparable data, codeswitching and compute resources. arXiv:2103.13272\nLample, G. and Conneau, A. 2019. Crosslingual language model pretraining. In Crosslingual language model pretraining. arXiv:1901.07291\nLi, B., He, Y., and Xu, W. 2021. Cross-lingual named entity recognition using parallel corpus: A new approach using xlm-roberta alignment. In Cross-lingual named entity recognition using parallel corpus: A new approach using xlm-roberta alignment. arXiv:2101.11112\nLi, H., Arora, A., Chen, S., Gupta, A., Gupta, S., and Mehdad, Y. 2021. MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. pp. 2950--2962\nLiang, Y., Duan, N., Gong, Y., Wu, N., Guo, F., Qi, W., Gong, M., Shou, L., Jiang, D., and Cao, G. 2020. Xglue: A new benchmark datasetfor cross-lingual pre-training, understanding and generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 6008--6018\nVan Den Oord, A., Li, Y., and Vinyals, O. 2018. Representation learning with contrastive predictive coding. In Representation learning with contrastive predictive coding. arXiv:1807.03748\nPan, L., Hang, C., Qi, H., Shah, A., Yu, M., and Potdar, S. 2020. Multilingual bert post-pretraining alignment. In Multilingual bert post-pretraining alignment. arXiv:2010.12547\nQi, K. and Du, J. 2020. Translation-based matching adversarial network for cross-lingual natural language inference. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8632--8639\nQin, L., Ni, M., Zhang, Y., and Che, W. 2020. Cosda-ml: Multi-lingual code-switching data augmentation for zero-shot cross-lingual nlp. In Cosda-ml: Multi-lingual code-switching data augmentation for zero-shot cross-lingual nlp. arXiv:2006.06402\nRazumovskaia, E., Glava\u0161, G., Majewska, O., Korhonen, A., and Vuli\u0107, I. 2021. Crossing the conversational chasm: A primer on multilingual task-oriented dialogue systems. In Crossing the conversational chasm: A primer on multilingual task-oriented dialogue systems. arXiv:2104.08570\nErikTjong, Sang, K., and De Meulder, F. 2003. Introduction to the conll-2003 shared task: Languageindependent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003. pp. 142--147\nSchuster, S., Gupta, S., Shah, R., and Lewis, M. 2019. Cross-lingual transfer learning for multilingual task oriented dialog. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 3795--3805\nSiddhant, A., Johnson, M., Tsai, H., Ari, N., Riesa, J., Bapna, A., Firat, O., and Raman, K. 2020. Evaluating the cross-lingual effectiveness of massively multilingual neural machine translation. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8854--8861\nWang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.R. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems. pp. 32\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.R. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv:1804.07461\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., and Funtowicz, M. 2019. Huggingface's transformers: State-ofthe-art natural language processing. In ArXiv. pp. 1910\nWu, C., Wu, F., and Huang, Y. 2021. Rethinking infonce: How many negative samples do you need? arXiv preprint. In Rethinking infonce: How many negative samples do you need? arXiv preprint. arXiv:2105.13003\nXia, M., Zheng, G., Mukherjee, S., and Shokouhi, M. Graham Neubig, and Ahmed Hassan Awadallah. 2021. Metaxl: Meta representation transformation for low-resource cross-lingual learning. In Graham Neubig, and Ahmed Hassan Awadallah. 2021. Metaxl: Meta representation transformation for low-resource cross-lingual learning. arXiv:2104.07908\nXu, W., Haider, B., and Mansour, S. 2020. End-to-end slot alignment and recognition for crosslingual nlu. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 5052--5063\nYi, H. and Cheng, J. 2021. Zero-shot entity recognition via multi-source projection and unlabeled data. In IOP Conference Series: Earth and Environmental Science. pp. 12084"
            }
        ],
        "spans_b": [
            {
                "filename": "paper_9.txt",
                "start": 14,
                "end": 260,
                "label": "Unsupported claim",
                "text": "Several approaches to zero-shot cross-lingual transfer exist and can broadly be divided into: a) Databased Transfer, which focuses on training data transformation and b) Model-based Transfer that centres around modifying models' training routine.",
                "full_text": "Related Work\n\nSeveral approaches to zero-shot cross-lingual transfer exist and can broadly be divided into: a) Databased Transfer, which focuses on training data transformation and b) Model-based Transfer that centres around modifying models' training routine.\n\nData-based Transfer Translating utterances for the intent classification task is relatively straightforward so previous works focused on projecting and/or aligning the entity labels between translated utterances (Dou and Neubig, 2021). This technique is followed by standard supervised training with those pseudo-labels and is commonly known as a translate-train method. One of the earliest works still being used for this purpose is fastalign (Dyer et al., 2013). It's an unsupervised word aligner trained on a parallel corpus to map each word (thus its entity label) in the source utterance to the word(s) in the target user utterance. Projecting the entity labels can also be done with word-by-word translation and source label copying (Yi and Cheng, 2021). A teacher model then weakly labels the target data, which is used to train the final student model. Sometimes, this type of label projection is complemented with an additional entity alignment step (Li et al., 2021a). Better performance can be achieved by using machine translation with entity matching and distributional statistics (Jain et al., 2019) though this can be a costly process for each language. A category of 'word substitution' methods such as code-switching (Qin et al., 2020;Kuwanto et al., 2021) or dictionary-enhanced pretraining  have also been shown to improve cross-lingual transfer.\n\nModel-based Transfer Prior to the adoption of multilingual transformers (Lample and Conneau, 2019), task-oriented XNLU methods employed a BiLSTM encoder combined with different multilingual embeddings (Schuster et al., 2019). Newer approaches usually involve a pretrained XLM and the addition of some new training component with the inference routine remaining mostly unchanged. Xu et al. (2020) learn to jointly align and predict entity labels by fusing the source and target language embeddings with attention and using the resulting cross-lingual representation for entity prediction. Qi and Du (2020) include an adversarial language detector in training whose loss encourages the model to generate language-agnostic sentence representations for improved zero-shot trans-fer. Pan et al. (2020) and Chi et al. (2020) added a contrastive loss to pretraining that treats translated sentences as positive examples and unrelated sentences as negative samples. This training step helps the XLM produce similar embeddings in different languages. However, these methods require large annotated datasets and expensive model pretraining (Chi et al., 2020). Our proposed methods only use the English task data (which is relatively limited) and its translations for each language.\n\nThe most related prior works are Arivazhagan et al. (2019) for machine translation and  for task-oriented XNLU. Both of these are cross-lingual alignment methods that use translated training data to zero-shot transfer the source language model to the target language. We focus on the latter work, called XeroAlign, which reported the most recent SOTA scores on our evaluation datasets. XeroAlign works by generating a sentence embedding of the user utterance for each language, e.g. English (source) and Thai (target) using the CLS token of the XLM. A Mean Squared Error loss function minimises the difference between the multilingual sentence embeddings and is backpropagated along with the main task loss. XeroAlign aims to bring sentence embeddings in different languages closer together with a bias towards intent classification due to the CLS embedding, which is the standard input to the intent classifier. We reproduce this method for analysis and comparisons but add a small post-processing step that distinctly improves the reported scores.\n\n References: \nArivazhagan, N., Bapna, A., Firat, O., Aharoni, R., Johnson, M., and Macherey, W. 2019. The missing ingredient in zero-shot neural machine translation. In The missing ingredient in zero-shot neural machine translation. arXiv:1903.07091\nArtetxe, M., Ruder, S., and Yogatama, D. 2020. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th.\nAnnual Meeting of the Association for Computational Linguistics. In Annual Meeting of the Association for Computational Linguistics. pp. 4623--4637\nArtetxe, M. and Schwenk, H. 2019. Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond. In Transactions of the Association for Computational Linguistics. pp. 597--610\nCasanueva, I., Budzianowski, P., Su, P., Mrk\u0161i\u0107, N., Wen, T., Ultes, S., Rojas-Barahona, L., Young, S., and Ga\u0161i\u0107, M. 2017. A benchmarking environment for reinforcement learning based task oriented dialogue management. In A benchmarking environment for reinforcement learning based task oriented dialogue management. arXiv:1711.11023\nChaudhary, A., Raman, K., Srinivasan, K., and Chen, J. 2020. Dict-mlm: Improved multilingual pre-training using bilingual dictionaries. In Dict-mlm: Improved multilingual pre-training using bilingual dictionaries. arXiv:2010.12566\nChen, Q., Zhuo, Z., and Wang, W. 2019. Bert for joint intent classification and slot filling. In Bert for joint intent classification and slot filling. arXiv:1902.10909\nChi, Z., Dong, L., Wei, F., Yang, N., Singhal, S., Wang, W., Song, X., Mao, X., Huang, H., and Zhou, M. 2020. foxlm: An information-theoretic framework for cross-lingual language model pre-training. In foxlm: An information-theoretic framework for cross-lingual language model pre-training. arXiv:2007.07834\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzm\u00e1n, F., Grave, \u00c9., Ott, M., Zettlemoyer, L., and Stoyanov, V. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 8440--8451\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186\nZi, Y., Dou, G., and Neubig 2021. Word alignment by fine-tuning embeddings on parallel corpora. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. pp. 2112--2128\nDyer, C., Chahuneau, V., and Smith, N.A. 2013. A simple, fast, and effective reparameterization of ibm model 2. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 644--648\nGritta, M. and Iacobacci, I. 2021. Xeroalign: Zero-shot cross-lingual transformer alignment. In Xeroalign: Zero-shot cross-lingual transformer alignment. arXiv:2105.02472\nGritta, M., Lampouras, G., and Iacobacci, I. 2021. Conversation graph: Data augmentation, training, and evaluation for non-deterministic dialogue management. In Transactions of the Association for Computational Linguistics. pp. 36--52\nGroenendijk, R., Karaoglu, S., Gevers, T., and Mensink, T. 2021. Multi-loss weighting with coefficient of variations. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 1469--1478\nHu, J., Ruder, S., Siddhant, A., Neubig, G., Firat, O., and Johnson, M. 2020. Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In International Conference on Machine Learning. pp. 4411--4421\nJain, A., Paranjape, B., and ZacharyC 2019. Entity projection via machine translation for cross-lingual NER. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 1083--1092 10.18653/v1/D19-1100\nKuwanto, G., Feyza Aky\u00fcrek, A., Tourni, I. C., Li, S., and Wijaya, D. 2021. Low-resource machine translation for low-resource languages: Leveraging comparable data, codeswitching and compute resources. In Low-resource machine translation for low-resource languages: Leveraging comparable data, codeswitching and compute resources. arXiv:2103.13272\nLample, G. and Conneau, A. 2019. Crosslingual language model pretraining. In Crosslingual language model pretraining. arXiv:1901.07291\nLi, B., He, Y., and Xu, W. 2021. Cross-lingual named entity recognition using parallel corpus: A new approach using xlm-roberta alignment. In Cross-lingual named entity recognition using parallel corpus: A new approach using xlm-roberta alignment. arXiv:2101.11112\nLi, H., Arora, A., Chen, S., Gupta, A., Gupta, S., and Mehdad, Y. 2021. MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. pp. 2950--2962\nLiang, Y., Duan, N., Gong, Y., Wu, N., Guo, F., Qi, W., Gong, M., Shou, L., Jiang, D., and Cao, G. 2020. Xglue: A new benchmark datasetfor cross-lingual pre-training, understanding and generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 6008--6018\nVan Den Oord, A., Li, Y., and Vinyals, O. 2018. Representation learning with contrastive predictive coding. In Representation learning with contrastive predictive coding. arXiv:1807.03748\nPan, L., Hang, C., Qi, H., Shah, A., Yu, M., and Potdar, S. 2020. Multilingual bert post-pretraining alignment. In Multilingual bert post-pretraining alignment. arXiv:2010.12547\nQi, K. and Du, J. 2020. Translation-based matching adversarial network for cross-lingual natural language inference. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8632--8639\nQin, L., Ni, M., Zhang, Y., and Che, W. 2020. Cosda-ml: Multi-lingual code-switching data augmentation for zero-shot cross-lingual nlp. In Cosda-ml: Multi-lingual code-switching data augmentation for zero-shot cross-lingual nlp. arXiv:2006.06402\nRazumovskaia, E., Glava\u0161, G., Majewska, O., Korhonen, A., and Vuli\u0107, I. 2021. Crossing the conversational chasm: A primer on multilingual task-oriented dialogue systems. In Crossing the conversational chasm: A primer on multilingual task-oriented dialogue systems. arXiv:2104.08570\nErikTjong, Sang, K., and De Meulder, F. 2003. Introduction to the conll-2003 shared task: Languageindependent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003. pp. 142--147\nSchuster, S., Gupta, S., Shah, R., and Lewis, M. 2019. Cross-lingual transfer learning for multilingual task oriented dialog. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 3795--3805\nSiddhant, A., Johnson, M., Tsai, H., Ari, N., Riesa, J., Bapna, A., Firat, O., and Raman, K. 2020. Evaluating the cross-lingual effectiveness of massively multilingual neural machine translation. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8854--8861\nWang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.R. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. In Advances in Neural Information Processing Systems. pp. 32\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.R. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv:1804.07461\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., and Funtowicz, M. 2019. Huggingface's transformers: State-ofthe-art natural language processing. In ArXiv. pp. 1910\nWu, C., Wu, F., and Huang, Y. 2021. Rethinking infonce: How many negative samples do you need? arXiv preprint. In Rethinking infonce: How many negative samples do you need? arXiv preprint. arXiv:2105.13003\nXia, M., Zheng, G., Mukherjee, S., and Shokouhi, M. Graham Neubig, and Ahmed Hassan Awadallah. 2021. Metaxl: Meta representation transformation for low-resource cross-lingual learning. In Graham Neubig, and Ahmed Hassan Awadallah. 2021. Metaxl: Meta representation transformation for low-resource cross-lingual learning. arXiv:2104.07908\nXu, W., Haider, B., and Mansour, S. 2020. End-to-end slot alignment and recognition for crosslingual nlu. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 5052--5063\nYi, H. and Cheng, J. 2021. Zero-shot entity recognition via multi-source projection and unlabeled data. In IOP Conference Series: Earth and Environmental Science. pp. 12084"
            }
        ]
    },
    {
        "filename": "paper_10.txt",
        "label": "Unsupported claim",
        "text": "This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way.",
        "start": 1785,
        "end": 1992,
        "spans_a": [
            {
                "filename": "paper_10.txt",
                "start": 1785,
                "end": 1992,
                "label": "Unsupported claim",
                "text": "This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way.",
                "full_text": "Introduction\n\nReading comprehension is a complex, multidimensional cognitive process (Kim, 2017). Question answering (QA) are fundamental for supporting humans\u2019 development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning. To achieve this goal, comprehension questions should be valid and reliable, meaning that all items are designed to cohesively assess comprehension rather than some other skills (e.g., text matching, paraphrasing, or memorization) (Roberts and Priest, 2006). Moreover, from the educational perspective, given that reading comprehension is a multicomponent skill, it is ideal for comprehension questions to be able to identify students\u2019 performance in specific sub-skills, thus allowing teachers to provide tailored guidance (Francis et al., 2005). This kind of high-quality questions is also valuable for improving machine reading comprehension.\n\nHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding. Some researchers have proposed to develop models to automatically generate questions to satisfy the need for a continuous supply of new questions (Kurdi et al., 2020). However, existing datasets are not particularly suitable for training question generation (QG) models for educational purposes (Das et al., 2021). This is primarily because the datasets are not typically structured around the specific dimensions of reading comprehension sub-skills, nor do they provide sufficient information on what sub-skills are tested. As a consequence, QG  models built on these datasets only yield one single \"comprehension\" score without a more detailed breakdown of performance on comprehension sub-skills. This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way. To bridge the gap, we constructed FairytaleQA, an open-source dataset focusing on comprehension of narratives, targeting students from kindergarten to eighth grade (Table 1). We focus on narrative comprehension for two reasons. First, narrative comprehension is a high-level comprehension skill strongly predictive of reading achievement (Lynch et al., 2008) and plays a central role in daily life as people frequently encounter narratives in different forms (Goldie, 2003). Second, narrative stories have a clear structure of specific elements and relations among these elements, and there are existing validated narrative comprehension frameworks around this structure, which provides a basis for developing the annotation schema for our dataset. We employed education experts who generated 10,580 question-answer pairs based on a collection of 278 fairytale stories for young readers, following evidence-based narrative comprehension frameworks (Paris and Paris, 2003; Alonzo et al., 2009). Thereby, FairytaleQA contains questions that focus on seven narrative elements and relations, namely character, setting, feeling, action, causal relationship, outcome resolution, and prediction (Paris and Paris, 2003), thus increasing the validity and reliability of the assessment. In addition, FairytaleQA also contains both explicit questions that involve answers found directly in the text and implicit questions that require inference making and high-level summarization, thus representing a relatively balanced assessment with questions of varying difficulty levels. Most importantly, our selection of annotators with education domain knoweldge as well as the training and quality control process ensured that the aforementioned annotation protocol was consistently implemented. A subset of questions in our dataset has been validated with 120 kindergarten students, proving the questions\u2019 reliability and validity.\nWe show the utility of FairytaleQA through two benchmarking experiments. First, we used our data to train and evaluate state-of-the-art QA models and demonstrated that (1) FairytaleQA contains challenging phenonmena for existing models, and (2) it can support finer-grained analysis on the aforementioned seven types of comprehension sub-skills,\neven for models trained on standard QA datasets (NarrativeQA (Ko\u02c7cisk `y et al., 2018)). We further calibrated model performances with human baseline, highlighting the most visible gap on models\u2019 reasoning capabilities on recognizing casual relationships and predicting event outcomes. Second, we used FairytaleQA to power question generation and showed that the QG model trained on ours was more capable of asking diverse questions and generating questions with higher quality.\n\n References: \nAlonzo, J., Basaraba, D., Tindal, G., and Carriveau, R. S. 2009. They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. In They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. pp. 34--44\nBajgar, O. and Kadlec, R. 2016. Embracing data abundance: Booktest dataset for reading comprehension. In Embracing data abundance: Booktest dataset for reading comprehension. arXiv:1610.00956\nBrahman, F., Huang, M., Tafjord, O., Zhao, C., Sachan, M., and Chaturvedi, S. 2021. let your characters tell their story. In A dataset for character-centric narrative understanding. arXiv:2109.05438\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. In Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457\nDalvi, B., Huang, L., Tandon, N., Wen-Tau, Y., and Clark, P. 2018. Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1595--1604\nDas, B. and Majumder, M. 2021. Automatic question generation and answer assessment: a survey. In Technology Enhanced Learning. pp. 1--15\nCarolyn A Denton, M., Enos, M. J., York, D. J., Francis, M. A., Barnes, P. A., Kulesz, Jack, M., Fletcher, S., and Carter 2015. Text-processing differences in adolescent adequate and poor comprehenders reading accessible and challenging narrative and informational text. In Reading Research Quarterly. pp. 393--416\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. In Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805\nDavid, J., Francis, Jack, M., Fletcher, H. W., Catts, and Bruce Tomblin 2005. Dimensions affecting the assessment of reading comprehension. In Children's reading comprehension and assessment. pp. 387--412\nAnna, S., Gellert, C., and Elbro 2013. Cloze tests may be quick, but are they dirty? development and preliminary validation of a cloze test of reading comprehension. In Journal of Psychoeducational Assessment. pp. 16--28\nGoldie, P. 2003. One's remembered past: Narrative thinking, emotion, and the external perspective. In Philosophical Papers. pp. 301--319\nMartha, H., Head, J. E., Readence, R. R., and Buss 1989. An examination of summary writing as a measure of reading comprehension. In Literacy Research and Instruction. pp. 1--11\nKim, Y.G. 2017. Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. In Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. pp. 310--333\nKlufa, J. 2015. Multiple choice question testsadvantages and disadvantages. In 3rd International Conference on Education and Modern Educational Technologies (EMET). pp. 39--42\nKo\u010disk\u1ef3, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. 2018. The narrativeqa reading comprehension challenge. In Transactions of the Association for Computational Linguistics. pp. 317--328\nKry\u015bci\u0144ski, W. and Rajani, N. 2021. Booksum: A collection of datasets for longform narrative summarization. In Booksum: A collection of datasets for longform narrative summarization. arXiv:2105.08209\nKurdi, G., Leo, J., Parsia, B., Sattler, U., and Al-Emari, S. 2020. A systematic review of automatic question generation for educational purposes. In International Journal of Artificial Intelligence in Education. pp. 121--204\nLadhak, F., Li, B., Al-Onaizan, Y., and Mckeown, K. 2020. Exploring content selection in summarization of novel chapters. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 5043--5054\nLai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. 2017. Race: Large-scale reading comprehension dataset from examinations. In Race: Large-scale reading comprehension dataset from examinations. arXiv:1704.04683\nKumar Lal, Y., Chambers, N., Mooney, R., and Balasubramanian, N. 2021. Tellmewhy: A dataset for answering why-questions in narratives. In Tellmewhy: A dataset for answering why-questions in narratives. arXiv:2106.06132\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2019. Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv:1910.13461\nMeghan, D. and Liebfreund 2021. Cognitive and motivational predictors of narrative and informational text comprehension. In Reading Psychology. pp. 177--196\nJulie, S., Lynch, P., Van Den, Broek, K. E., Kremer, P., Kendeou, M. J., White, E. P., and Lorch 2008. The development of narrative comprehension and its relation to other early reading skills. In Reading Psychology. pp. 327--365\nNancy, A., Martin, R., and Brownell 2011. Expressive one-word picture vocabulary test-4 (EOWPVT. In Expressive one-word picture vocabulary test-4 (EOWPVT.\nMostafazadeh, N., Chambers, N., He, X., Parikh, D., Batra, D., Vanderwende, L., Kohli, P., and Allen, J. 2016. A corpus and evaluation framework for deeper understanding of commonsense stories. In A corpus and evaluation framework for deeper understanding of commonsense stories. arXiv:1604.01696\nMou, X., Yang, C., Yu, M., Yao, B., Guo, X., Potdar, S., and Su, H. 2021. Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. In Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. arXiv:2106.03826\nEzgi \u00c7etinkaya \u00d6zdemir, H. and Akyol 2019. The development of a reading comprehension test. In Universal Journal of Educational Research. pp. 563--570\nAlison, H., Paris, G., and Paris 2003. Assessing narrative comprehension in young children. In Reading Research Quarterly. pp. 36--76\nTaffy, E. and Raphael 1986. Teaching question answer relationships, revisited. The reading teacher. In Teaching question answer relationships, revisited. The reading teacher. pp. 516--522\nRoberts, P. and Priest, H. 2006. Reliability and validity in research. In Nursing standard. pp. 41--46\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv:1910.01108\nSims, M., Park, J. H., and Bamman, D. 2019. Literary event detection. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 3623--3634\nXie, Q., Lai, G., Dai, Z., and Hovy, E. 2017. Large-scale cloze test dataset created by teachers. In Large-scale cloze test dataset created by teachers. arXiv:1711.03225\nYao, B., Wang, D., Wu, T., Hoang, T., Sun, B., Li, T.J., Yu, M., and Xu, Y. 2021. It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. In It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. arXiv:2109.03423"
            }
        ],
        "spans_b": [
            {
                "filename": "paper_10.txt",
                "start": 1785,
                "end": 1992,
                "label": "Unsupported claim",
                "text": "This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way.",
                "full_text": "Introduction\n\nReading comprehension is a complex, multidimensional cognitive process (Kim, 2017). Question answering (QA) are fundamental for supporting humans\u2019 development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning. To achieve this goal, comprehension questions should be valid and reliable, meaning that all items are designed to cohesively assess comprehension rather than some other skills (e.g., text matching, paraphrasing, or memorization) (Roberts and Priest, 2006). Moreover, from the educational perspective, given that reading comprehension is a multicomponent skill, it is ideal for comprehension questions to be able to identify students\u2019 performance in specific sub-skills, thus allowing teachers to provide tailored guidance (Francis et al., 2005). This kind of high-quality questions is also valuable for improving machine reading comprehension.\n\nHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding. Some researchers have proposed to develop models to automatically generate questions to satisfy the need for a continuous supply of new questions (Kurdi et al., 2020). However, existing datasets are not particularly suitable for training question generation (QG) models for educational purposes (Das et al., 2021). This is primarily because the datasets are not typically structured around the specific dimensions of reading comprehension sub-skills, nor do they provide sufficient information on what sub-skills are tested. As a consequence, QG  models built on these datasets only yield one single \"comprehension\" score without a more detailed breakdown of performance on comprehension sub-skills. This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way. To bridge the gap, we constructed FairytaleQA, an open-source dataset focusing on comprehension of narratives, targeting students from kindergarten to eighth grade (Table 1). We focus on narrative comprehension for two reasons. First, narrative comprehension is a high-level comprehension skill strongly predictive of reading achievement (Lynch et al., 2008) and plays a central role in daily life as people frequently encounter narratives in different forms (Goldie, 2003). Second, narrative stories have a clear structure of specific elements and relations among these elements, and there are existing validated narrative comprehension frameworks around this structure, which provides a basis for developing the annotation schema for our dataset. We employed education experts who generated 10,580 question-answer pairs based on a collection of 278 fairytale stories for young readers, following evidence-based narrative comprehension frameworks (Paris and Paris, 2003; Alonzo et al., 2009). Thereby, FairytaleQA contains questions that focus on seven narrative elements and relations, namely character, setting, feeling, action, causal relationship, outcome resolution, and prediction (Paris and Paris, 2003), thus increasing the validity and reliability of the assessment. In addition, FairytaleQA also contains both explicit questions that involve answers found directly in the text and implicit questions that require inference making and high-level summarization, thus representing a relatively balanced assessment with questions of varying difficulty levels. Most importantly, our selection of annotators with education domain knoweldge as well as the training and quality control process ensured that the aforementioned annotation protocol was consistently implemented. A subset of questions in our dataset has been validated with 120 kindergarten students, proving the questions\u2019 reliability and validity.\nWe show the utility of FairytaleQA through two benchmarking experiments. First, we used our data to train and evaluate state-of-the-art QA models and demonstrated that (1) FairytaleQA contains challenging phenonmena for existing models, and (2) it can support finer-grained analysis on the aforementioned seven types of comprehension sub-skills,\neven for models trained on standard QA datasets (NarrativeQA (Ko\u02c7cisk `y et al., 2018)). We further calibrated model performances with human baseline, highlighting the most visible gap on models\u2019 reasoning capabilities on recognizing casual relationships and predicting event outcomes. Second, we used FairytaleQA to power question generation and showed that the QG model trained on ours was more capable of asking diverse questions and generating questions with higher quality.\n\n References: \nAlonzo, J., Basaraba, D., Tindal, G., and Carriveau, R. S. 2009. They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. In They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. pp. 34--44\nBajgar, O. and Kadlec, R. 2016. Embracing data abundance: Booktest dataset for reading comprehension. In Embracing data abundance: Booktest dataset for reading comprehension. arXiv:1610.00956\nBrahman, F., Huang, M., Tafjord, O., Zhao, C., Sachan, M., and Chaturvedi, S. 2021. let your characters tell their story. In A dataset for character-centric narrative understanding. arXiv:2109.05438\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. In Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457\nDalvi, B., Huang, L., Tandon, N., Wen-Tau, Y., and Clark, P. 2018. Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1595--1604\nDas, B. and Majumder, M. 2021. Automatic question generation and answer assessment: a survey. In Technology Enhanced Learning. pp. 1--15\nCarolyn A Denton, M., Enos, M. J., York, D. J., Francis, M. A., Barnes, P. A., Kulesz, Jack, M., Fletcher, S., and Carter 2015. Text-processing differences in adolescent adequate and poor comprehenders reading accessible and challenging narrative and informational text. In Reading Research Quarterly. pp. 393--416\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. In Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805\nDavid, J., Francis, Jack, M., Fletcher, H. W., Catts, and Bruce Tomblin 2005. Dimensions affecting the assessment of reading comprehension. In Children's reading comprehension and assessment. pp. 387--412\nAnna, S., Gellert, C., and Elbro 2013. Cloze tests may be quick, but are they dirty? development and preliminary validation of a cloze test of reading comprehension. In Journal of Psychoeducational Assessment. pp. 16--28\nGoldie, P. 2003. One's remembered past: Narrative thinking, emotion, and the external perspective. In Philosophical Papers. pp. 301--319\nMartha, H., Head, J. E., Readence, R. R., and Buss 1989. An examination of summary writing as a measure of reading comprehension. In Literacy Research and Instruction. pp. 1--11\nKim, Y.G. 2017. Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. In Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. pp. 310--333\nKlufa, J. 2015. Multiple choice question testsadvantages and disadvantages. In 3rd International Conference on Education and Modern Educational Technologies (EMET). pp. 39--42\nKo\u010disk\u1ef3, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. 2018. The narrativeqa reading comprehension challenge. In Transactions of the Association for Computational Linguistics. pp. 317--328\nKry\u015bci\u0144ski, W. and Rajani, N. 2021. Booksum: A collection of datasets for longform narrative summarization. In Booksum: A collection of datasets for longform narrative summarization. arXiv:2105.08209\nKurdi, G., Leo, J., Parsia, B., Sattler, U., and Al-Emari, S. 2020. A systematic review of automatic question generation for educational purposes. In International Journal of Artificial Intelligence in Education. pp. 121--204\nLadhak, F., Li, B., Al-Onaizan, Y., and Mckeown, K. 2020. Exploring content selection in summarization of novel chapters. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 5043--5054\nLai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. 2017. Race: Large-scale reading comprehension dataset from examinations. In Race: Large-scale reading comprehension dataset from examinations. arXiv:1704.04683\nKumar Lal, Y., Chambers, N., Mooney, R., and Balasubramanian, N. 2021. Tellmewhy: A dataset for answering why-questions in narratives. In Tellmewhy: A dataset for answering why-questions in narratives. arXiv:2106.06132\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2019. Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv:1910.13461\nMeghan, D. and Liebfreund 2021. Cognitive and motivational predictors of narrative and informational text comprehension. In Reading Psychology. pp. 177--196\nJulie, S., Lynch, P., Van Den, Broek, K. E., Kremer, P., Kendeou, M. J., White, E. P., and Lorch 2008. The development of narrative comprehension and its relation to other early reading skills. In Reading Psychology. pp. 327--365\nNancy, A., Martin, R., and Brownell 2011. Expressive one-word picture vocabulary test-4 (EOWPVT. In Expressive one-word picture vocabulary test-4 (EOWPVT.\nMostafazadeh, N., Chambers, N., He, X., Parikh, D., Batra, D., Vanderwende, L., Kohli, P., and Allen, J. 2016. A corpus and evaluation framework for deeper understanding of commonsense stories. In A corpus and evaluation framework for deeper understanding of commonsense stories. arXiv:1604.01696\nMou, X., Yang, C., Yu, M., Yao, B., Guo, X., Potdar, S., and Su, H. 2021. Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. In Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. arXiv:2106.03826\nEzgi \u00c7etinkaya \u00d6zdemir, H. and Akyol 2019. The development of a reading comprehension test. In Universal Journal of Educational Research. pp. 563--570\nAlison, H., Paris, G., and Paris 2003. Assessing narrative comprehension in young children. In Reading Research Quarterly. pp. 36--76\nTaffy, E. and Raphael 1986. Teaching question answer relationships, revisited. The reading teacher. In Teaching question answer relationships, revisited. The reading teacher. pp. 516--522\nRoberts, P. and Priest, H. 2006. Reliability and validity in research. In Nursing standard. pp. 41--46\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv:1910.01108\nSims, M., Park, J. H., and Bamman, D. 2019. Literary event detection. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 3623--3634\nXie, Q., Lai, G., Dai, Z., and Hovy, E. 2017. Large-scale cloze test dataset created by teachers. In Large-scale cloze test dataset created by teachers. arXiv:1711.03225\nYao, B., Wang, D., Wu, T., Hoang, T., Sun, B., Li, T.J., Yu, M., and Xu, Y. 2021. It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. In It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. arXiv:2109.03423"
            }
        ]
    }
]