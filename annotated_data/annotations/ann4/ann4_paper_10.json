[
    {
        "filename": "paper_10.txt",
        "start": 153,
        "end": 172,
        "label": "Unsupported Claim",
        "text": "humans\u2019 development",
        "full_text": "Introduction\n\nReading comprehension is a complex, multidimensional cognitive process (Kim, 2017). Question answering (QA) are fundamental for supporting humans\u2019 development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning. To achieve this goal, comprehension questions should be valid and reliable, meaning that all items are designed to cohesively assess comprehension rather than some other skills (e.g., text matching, paraphrasing, or memorization) (Roberts and Priest, 2006). Moreover, from the educational perspective, given that reading comprehension is a multicomponent skill, it is ideal for comprehension questions to be able to identify students\u2019 performance in specific sub-skills, thus allowing teachers to provide tailored guidance (Francis et al., 2005). This kind of high-quality questions is also valuable for improving machine reading comprehension.\n\nHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding. Some researchers have proposed to develop models to automatically generate questions to satisfy the need for a continuous supply of new questions (Kurdi et al., 2020). However, existing datasets are not particularly suitable for training question generation (QG) models for educational purposes (Das et al., 2021). This is primarily because the datasets are not typically structured around the specific dimensions of reading comprehension sub-skills, nor do they provide sufficient information on what sub-skills are tested. As a consequence, QG  models built on these datasets only yield one single \"comprehension\" score without a more detailed breakdown of performance on comprehension sub-skills. This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way. To bridge the gap, we constructed FairytaleQA, an open-source dataset focusing on comprehension of narratives, targeting students from kindergarten to eighth grade (Table 1). We focus on narrative comprehension for two reasons. First, narrative comprehension is a high-level comprehension skill strongly predictive of reading achievement (Lynch et al., 2008) and plays a central role in daily life as people frequently encounter narratives in different forms (Goldie, 2003). Second, narrative stories have a clear structure of specific elements and relations among these elements, and there are existing validated narrative comprehension frameworks around this structure, which provides a basis for developing the annotation schema for our dataset. We employed education experts who generated 10,580 question-answer pairs based on a collection of 278 fairytale stories for young readers, following evidence-based narrative comprehension frameworks (Paris and Paris, 2003; Alonzo et al., 2009). Thereby, FairytaleQA contains questions that focus on seven narrative elements and relations, namely character, setting, feeling, action, causal relationship, outcome resolution, and prediction (Paris and Paris, 2003), thus increasing the validity and reliability of the assessment. In addition, FairytaleQA also contains both explicit questions that involve answers found directly in the text and implicit questions that require inference making and high-level summarization, thus representing a relatively balanced assessment with questions of varying difficulty levels. Most importantly, our selection of annotators with education domain knoweldge as well as the training and quality control process ensured that the aforementioned annotation protocol was consistently implemented. A subset of questions in our dataset has been validated with 120 kindergarten students, proving the questions\u2019 reliability and validity.\nWe show the utility of FairytaleQA through two benchmarking experiments. First, we used our data to train and evaluate state-of-the-art QA models and demonstrated that (1) FairytaleQA contains challenging phenonmena for existing models, and (2) it can support finer-grained analysis on the aforementioned seven types of comprehension sub-skills,\neven for models trained on standard QA datasets (NarrativeQA (Ko\u02c7cisk `y et al., 2018)). We further calibrated model performances with human baseline, highlighting the most visible gap on models\u2019 reasoning capabilities on recognizing casual relationships and predicting event outcomes. Second, we used FairytaleQA to power question generation and showed that the QG model trained on ours was more capable of asking diverse questions and generating questions with higher quality.\n\n References: \nAlonzo, J., Basaraba, D., Tindal, G., and Carriveau, R. S. 2009. They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. In They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. pp. 34--44\nBajgar, O. and Kadlec, R. 2016. Embracing data abundance: Booktest dataset for reading comprehension. In Embracing data abundance: Booktest dataset for reading comprehension. arXiv:1610.00956\nBrahman, F., Huang, M., Tafjord, O., Zhao, C., Sachan, M., and Chaturvedi, S. 2021. let your characters tell their story. In A dataset for character-centric narrative understanding. arXiv:2109.05438\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. In Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457\nDalvi, B., Huang, L., Tandon, N., Wen-Tau, Y., and Clark, P. 2018. Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1595--1604\nDas, B. and Majumder, M. 2021. Automatic question generation and answer assessment: a survey. In Technology Enhanced Learning. pp. 1--15\nCarolyn A Denton, M., Enos, M. J., York, D. J., Francis, M. A., Barnes, P. A., Kulesz, Jack, M., Fletcher, S., and Carter 2015. Text-processing differences in adolescent adequate and poor comprehenders reading accessible and challenging narrative and informational text. In Reading Research Quarterly. pp. 393--416\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. In Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805\nDavid, J., Francis, Jack, M., Fletcher, H. W., Catts, and Bruce Tomblin 2005. Dimensions affecting the assessment of reading comprehension. In Children's reading comprehension and assessment. pp. 387--412\nAnna, S., Gellert, C., and Elbro 2013. Cloze tests may be quick, but are they dirty? development and preliminary validation of a cloze test of reading comprehension. In Journal of Psychoeducational Assessment. pp. 16--28\nGoldie, P. 2003. One's remembered past: Narrative thinking, emotion, and the external perspective. In Philosophical Papers. pp. 301--319\nMartha, H., Head, J. E., Readence, R. R., and Buss 1989. An examination of summary writing as a measure of reading comprehension. In Literacy Research and Instruction. pp. 1--11\nKim, Y.G. 2017. Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. In Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. pp. 310--333\nKlufa, J. 2015. Multiple choice question testsadvantages and disadvantages. In 3rd International Conference on Education and Modern Educational Technologies (EMET). pp. 39--42\nKo\u010disk\u1ef3, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. 2018. The narrativeqa reading comprehension challenge. In Transactions of the Association for Computational Linguistics. pp. 317--328\nKry\u015bci\u0144ski, W. and Rajani, N. 2021. Booksum: A collection of datasets for longform narrative summarization. In Booksum: A collection of datasets for longform narrative summarization. arXiv:2105.08209\nKurdi, G., Leo, J., Parsia, B., Sattler, U., and Al-Emari, S. 2020. A systematic review of automatic question generation for educational purposes. In International Journal of Artificial Intelligence in Education. pp. 121--204\nLadhak, F., Li, B., Al-Onaizan, Y., and Mckeown, K. 2020. Exploring content selection in summarization of novel chapters. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 5043--5054\nLai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. 2017. Race: Large-scale reading comprehension dataset from examinations. In Race: Large-scale reading comprehension dataset from examinations. arXiv:1704.04683\nKumar Lal, Y., Chambers, N., Mooney, R., and Balasubramanian, N. 2021. Tellmewhy: A dataset for answering why-questions in narratives. In Tellmewhy: A dataset for answering why-questions in narratives. arXiv:2106.06132\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2019. Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv:1910.13461\nMeghan, D. and Liebfreund 2021. Cognitive and motivational predictors of narrative and informational text comprehension. In Reading Psychology. pp. 177--196\nJulie, S., Lynch, P., Van Den, Broek, K. E., Kremer, P., Kendeou, M. J., White, E. P., and Lorch 2008. The development of narrative comprehension and its relation to other early reading skills. In Reading Psychology. pp. 327--365\nNancy, A., Martin, R., and Brownell 2011. Expressive one-word picture vocabulary test-4 (EOWPVT. In Expressive one-word picture vocabulary test-4 (EOWPVT.\nMostafazadeh, N., Chambers, N., He, X., Parikh, D., Batra, D., Vanderwende, L., Kohli, P., and Allen, J. 2016. A corpus and evaluation framework for deeper understanding of commonsense stories. In A corpus and evaluation framework for deeper understanding of commonsense stories. arXiv:1604.01696\nMou, X., Yang, C., Yu, M., Yao, B., Guo, X., Potdar, S., and Su, H. 2021. Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. In Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. arXiv:2106.03826\nEzgi \u00c7etinkaya \u00d6zdemir, H. and Akyol 2019. The development of a reading comprehension test. In Universal Journal of Educational Research. pp. 563--570\nAlison, H., Paris, G., and Paris 2003. Assessing narrative comprehension in young children. In Reading Research Quarterly. pp. 36--76\nTaffy, E. and Raphael 1986. Teaching question answer relationships, revisited. The reading teacher. In Teaching question answer relationships, revisited. The reading teacher. pp. 516--522\nRoberts, P. and Priest, H. 2006. Reliability and validity in research. In Nursing standard. pp. 41--46\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv:1910.01108\nSims, M., Park, J. H., and Bamman, D. 2019. Literary event detection. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 3623--3634\nXie, Q., Lai, G., Dai, Z., and Hovy, E. 2017. Large-scale cloze test dataset created by teachers. In Large-scale cloze test dataset created by teachers. arXiv:1711.03225\nYao, B., Wang, D., Wu, T., Hoang, T., Sun, B., Li, T.J., Yu, M., and Xu, Y. 2021. It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. In It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. arXiv:2109.03423"
    },
    {
        "filename": "paper_10.txt",
        "start": 273,
        "end": 294,
        "label": "Unsupported Claim",
        "text": "facilitate learning.",
        "full_text": "Introduction\n\nReading comprehension is a complex, multidimensional cognitive process (Kim, 2017). Question answering (QA) are fundamental for supporting humans\u2019 development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning. To achieve this goal, comprehension questions should be valid and reliable, meaning that all items are designed to cohesively assess comprehension rather than some other skills (e.g., text matching, paraphrasing, or memorization) (Roberts and Priest, 2006). Moreover, from the educational perspective, given that reading comprehension is a multicomponent skill, it is ideal for comprehension questions to be able to identify students\u2019 performance in specific sub-skills, thus allowing teachers to provide tailored guidance (Francis et al., 2005). This kind of high-quality questions is also valuable for improving machine reading comprehension.\n\nHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding. Some researchers have proposed to develop models to automatically generate questions to satisfy the need for a continuous supply of new questions (Kurdi et al., 2020). However, existing datasets are not particularly suitable for training question generation (QG) models for educational purposes (Das et al., 2021). This is primarily because the datasets are not typically structured around the specific dimensions of reading comprehension sub-skills, nor do they provide sufficient information on what sub-skills are tested. As a consequence, QG  models built on these datasets only yield one single \"comprehension\" score without a more detailed breakdown of performance on comprehension sub-skills. This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way. To bridge the gap, we constructed FairytaleQA, an open-source dataset focusing on comprehension of narratives, targeting students from kindergarten to eighth grade (Table 1). We focus on narrative comprehension for two reasons. First, narrative comprehension is a high-level comprehension skill strongly predictive of reading achievement (Lynch et al., 2008) and plays a central role in daily life as people frequently encounter narratives in different forms (Goldie, 2003). Second, narrative stories have a clear structure of specific elements and relations among these elements, and there are existing validated narrative comprehension frameworks around this structure, which provides a basis for developing the annotation schema for our dataset. We employed education experts who generated 10,580 question-answer pairs based on a collection of 278 fairytale stories for young readers, following evidence-based narrative comprehension frameworks (Paris and Paris, 2003; Alonzo et al., 2009). Thereby, FairytaleQA contains questions that focus on seven narrative elements and relations, namely character, setting, feeling, action, causal relationship, outcome resolution, and prediction (Paris and Paris, 2003), thus increasing the validity and reliability of the assessment. In addition, FairytaleQA also contains both explicit questions that involve answers found directly in the text and implicit questions that require inference making and high-level summarization, thus representing a relatively balanced assessment with questions of varying difficulty levels. Most importantly, our selection of annotators with education domain knoweldge as well as the training and quality control process ensured that the aforementioned annotation protocol was consistently implemented. A subset of questions in our dataset has been validated with 120 kindergarten students, proving the questions\u2019 reliability and validity.\nWe show the utility of FairytaleQA through two benchmarking experiments. First, we used our data to train and evaluate state-of-the-art QA models and demonstrated that (1) FairytaleQA contains challenging phenonmena for existing models, and (2) it can support finer-grained analysis on the aforementioned seven types of comprehension sub-skills,\neven for models trained on standard QA datasets (NarrativeQA (Ko\u02c7cisk `y et al., 2018)). We further calibrated model performances with human baseline, highlighting the most visible gap on models\u2019 reasoning capabilities on recognizing casual relationships and predicting event outcomes. Second, we used FairytaleQA to power question generation and showed that the QG model trained on ours was more capable of asking diverse questions and generating questions with higher quality.\n\n References: \nAlonzo, J., Basaraba, D., Tindal, G., and Carriveau, R. S. 2009. They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. In They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. pp. 34--44\nBajgar, O. and Kadlec, R. 2016. Embracing data abundance: Booktest dataset for reading comprehension. In Embracing data abundance: Booktest dataset for reading comprehension. arXiv:1610.00956\nBrahman, F., Huang, M., Tafjord, O., Zhao, C., Sachan, M., and Chaturvedi, S. 2021. let your characters tell their story. In A dataset for character-centric narrative understanding. arXiv:2109.05438\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. In Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457\nDalvi, B., Huang, L., Tandon, N., Wen-Tau, Y., and Clark, P. 2018. Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1595--1604\nDas, B. and Majumder, M. 2021. Automatic question generation and answer assessment: a survey. In Technology Enhanced Learning. pp. 1--15\nCarolyn A Denton, M., Enos, M. J., York, D. J., Francis, M. A., Barnes, P. A., Kulesz, Jack, M., Fletcher, S., and Carter 2015. Text-processing differences in adolescent adequate and poor comprehenders reading accessible and challenging narrative and informational text. In Reading Research Quarterly. pp. 393--416\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. In Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805\nDavid, J., Francis, Jack, M., Fletcher, H. W., Catts, and Bruce Tomblin 2005. Dimensions affecting the assessment of reading comprehension. In Children's reading comprehension and assessment. pp. 387--412\nAnna, S., Gellert, C., and Elbro 2013. Cloze tests may be quick, but are they dirty? development and preliminary validation of a cloze test of reading comprehension. In Journal of Psychoeducational Assessment. pp. 16--28\nGoldie, P. 2003. One's remembered past: Narrative thinking, emotion, and the external perspective. In Philosophical Papers. pp. 301--319\nMartha, H., Head, J. E., Readence, R. R., and Buss 1989. An examination of summary writing as a measure of reading comprehension. In Literacy Research and Instruction. pp. 1--11\nKim, Y.G. 2017. Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. In Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. pp. 310--333\nKlufa, J. 2015. Multiple choice question testsadvantages and disadvantages. In 3rd International Conference on Education and Modern Educational Technologies (EMET). pp. 39--42\nKo\u010disk\u1ef3, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. 2018. The narrativeqa reading comprehension challenge. In Transactions of the Association for Computational Linguistics. pp. 317--328\nKry\u015bci\u0144ski, W. and Rajani, N. 2021. Booksum: A collection of datasets for longform narrative summarization. In Booksum: A collection of datasets for longform narrative summarization. arXiv:2105.08209\nKurdi, G., Leo, J., Parsia, B., Sattler, U., and Al-Emari, S. 2020. A systematic review of automatic question generation for educational purposes. In International Journal of Artificial Intelligence in Education. pp. 121--204\nLadhak, F., Li, B., Al-Onaizan, Y., and Mckeown, K. 2020. Exploring content selection in summarization of novel chapters. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 5043--5054\nLai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. 2017. Race: Large-scale reading comprehension dataset from examinations. In Race: Large-scale reading comprehension dataset from examinations. arXiv:1704.04683\nKumar Lal, Y., Chambers, N., Mooney, R., and Balasubramanian, N. 2021. Tellmewhy: A dataset for answering why-questions in narratives. In Tellmewhy: A dataset for answering why-questions in narratives. arXiv:2106.06132\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2019. Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv:1910.13461\nMeghan, D. and Liebfreund 2021. Cognitive and motivational predictors of narrative and informational text comprehension. In Reading Psychology. pp. 177--196\nJulie, S., Lynch, P., Van Den, Broek, K. E., Kremer, P., Kendeou, M. J., White, E. P., and Lorch 2008. The development of narrative comprehension and its relation to other early reading skills. In Reading Psychology. pp. 327--365\nNancy, A., Martin, R., and Brownell 2011. Expressive one-word picture vocabulary test-4 (EOWPVT. In Expressive one-word picture vocabulary test-4 (EOWPVT.\nMostafazadeh, N., Chambers, N., He, X., Parikh, D., Batra, D., Vanderwende, L., Kohli, P., and Allen, J. 2016. A corpus and evaluation framework for deeper understanding of commonsense stories. In A corpus and evaluation framework for deeper understanding of commonsense stories. arXiv:1604.01696\nMou, X., Yang, C., Yu, M., Yao, B., Guo, X., Potdar, S., and Su, H. 2021. Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. In Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. arXiv:2106.03826\nEzgi \u00c7etinkaya \u00d6zdemir, H. and Akyol 2019. The development of a reading comprehension test. In Universal Journal of Educational Research. pp. 563--570\nAlison, H., Paris, G., and Paris 2003. Assessing narrative comprehension in young children. In Reading Research Quarterly. pp. 36--76\nTaffy, E. and Raphael 1986. Teaching question answer relationships, revisited. The reading teacher. In Teaching question answer relationships, revisited. The reading teacher. pp. 516--522\nRoberts, P. and Priest, H. 2006. Reliability and validity in research. In Nursing standard. pp. 41--46\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv:1910.01108\nSims, M., Park, J. H., and Bamman, D. 2019. Literary event detection. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 3623--3634\nXie, Q., Lai, G., Dai, Z., and Hovy, E. 2017. Large-scale cloze test dataset created by teachers. In Large-scale cloze test dataset created by teachers. arXiv:1711.03225\nYao, B., Wang, D., Wu, T., Hoang, T., Sun, B., Li, T.J., Yu, M., and Xu, Y. 2021. It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. In It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. arXiv:2109.03423"
    },
    {
        "filename": "paper_10.txt",
        "start": 2596,
        "end": 2641,
        "label": "Unsupported Claim",
        "text": "validated narrative comprehension frameworks",
        "full_text": "Introduction\n\nReading comprehension is a complex, multidimensional cognitive process (Kim, 2017). Question answering (QA) are fundamental for supporting humans\u2019 development of reading comprehension skills, as questions serve as both instruments for evaluation and tools to facilitate learning. To achieve this goal, comprehension questions should be valid and reliable, meaning that all items are designed to cohesively assess comprehension rather than some other skills (e.g., text matching, paraphrasing, or memorization) (Roberts and Priest, 2006). Moreover, from the educational perspective, given that reading comprehension is a multicomponent skill, it is ideal for comprehension questions to be able to identify students\u2019 performance in specific sub-skills, thus allowing teachers to provide tailored guidance (Francis et al., 2005). This kind of high-quality questions is also valuable for improving machine reading comprehension.\n\nHowever, creating a large and suitable set of questions for supporting narrative comprehension is both time-consuming and cognitively demanding. Some researchers have proposed to develop models to automatically generate questions to satisfy the need for a continuous supply of new questions (Kurdi et al., 2020). However, existing datasets are not particularly suitable for training question generation (QG) models for educational purposes (Das et al., 2021). This is primarily because the datasets are not typically structured around the specific dimensions of reading comprehension sub-skills, nor do they provide sufficient information on what sub-skills are tested. As a consequence, QG  models built on these datasets only yield one single \"comprehension\" score without a more detailed breakdown of performance on comprehension sub-skills. This issue is compounded by the fact that many benchmarks rely on crowd-sourced workers who may not have sufficient training or education domain knowledge needed to create valid questions in a onsistent way. To bridge the gap, we constructed FairytaleQA, an open-source dataset focusing on comprehension of narratives, targeting students from kindergarten to eighth grade (Table 1). We focus on narrative comprehension for two reasons. First, narrative comprehension is a high-level comprehension skill strongly predictive of reading achievement (Lynch et al., 2008) and plays a central role in daily life as people frequently encounter narratives in different forms (Goldie, 2003). Second, narrative stories have a clear structure of specific elements and relations among these elements, and there are existing validated narrative comprehension frameworks around this structure, which provides a basis for developing the annotation schema for our dataset. We employed education experts who generated 10,580 question-answer pairs based on a collection of 278 fairytale stories for young readers, following evidence-based narrative comprehension frameworks (Paris and Paris, 2003; Alonzo et al., 2009). Thereby, FairytaleQA contains questions that focus on seven narrative elements and relations, namely character, setting, feeling, action, causal relationship, outcome resolution, and prediction (Paris and Paris, 2003), thus increasing the validity and reliability of the assessment. In addition, FairytaleQA also contains both explicit questions that involve answers found directly in the text and implicit questions that require inference making and high-level summarization, thus representing a relatively balanced assessment with questions of varying difficulty levels. Most importantly, our selection of annotators with education domain knoweldge as well as the training and quality control process ensured that the aforementioned annotation protocol was consistently implemented. A subset of questions in our dataset has been validated with 120 kindergarten students, proving the questions\u2019 reliability and validity.\nWe show the utility of FairytaleQA through two benchmarking experiments. First, we used our data to train and evaluate state-of-the-art QA models and demonstrated that (1) FairytaleQA contains challenging phenonmena for existing models, and (2) it can support finer-grained analysis on the aforementioned seven types of comprehension sub-skills,\neven for models trained on standard QA datasets (NarrativeQA (Ko\u02c7cisk `y et al., 2018)). We further calibrated model performances with human baseline, highlighting the most visible gap on models\u2019 reasoning capabilities on recognizing casual relationships and predicting event outcomes. Second, we used FairytaleQA to power question generation and showed that the QG model trained on ours was more capable of asking diverse questions and generating questions with higher quality.\n\n References: \nAlonzo, J., Basaraba, D., Tindal, G., and Carriveau, R. S. 2009. They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. In They read, but how well do they understand? an empirical look at the nuances of measuring reading comprehension. Assessment for Effective Intervention. pp. 34--44\nBajgar, O. and Kadlec, R. 2016. Embracing data abundance: Booktest dataset for reading comprehension. In Embracing data abundance: Booktest dataset for reading comprehension. arXiv:1610.00956\nBrahman, F., Huang, M., Tafjord, O., Zhao, C., Sachan, M., and Chaturvedi, S. 2021. let your characters tell their story. In A dataset for character-centric narrative understanding. arXiv:2109.05438\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. In Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457\nDalvi, B., Huang, L., Tandon, N., Wen-Tau, Y., and Clark, P. 2018. Tracking state changes in procedural text: a challenge dataset and models for process paragraph comprehension. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1595--1604\nDas, B. and Majumder, M. 2021. Automatic question generation and answer assessment: a survey. In Technology Enhanced Learning. pp. 1--15\nCarolyn A Denton, M., Enos, M. J., York, D. J., Francis, M. A., Barnes, P. A., Kulesz, Jack, M., Fletcher, S., and Carter 2015. Text-processing differences in adolescent adequate and poor comprehenders reading accessible and challenging narrative and informational text. In Reading Research Quarterly. pp. 393--416\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. In Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805\nDavid, J., Francis, Jack, M., Fletcher, H. W., Catts, and Bruce Tomblin 2005. Dimensions affecting the assessment of reading comprehension. In Children's reading comprehension and assessment. pp. 387--412\nAnna, S., Gellert, C., and Elbro 2013. Cloze tests may be quick, but are they dirty? development and preliminary validation of a cloze test of reading comprehension. In Journal of Psychoeducational Assessment. pp. 16--28\nGoldie, P. 2003. One's remembered past: Narrative thinking, emotion, and the external perspective. In Philosophical Papers. pp. 301--319\nMartha, H., Head, J. E., Readence, R. R., and Buss 1989. An examination of summary writing as a measure of reading comprehension. In Literacy Research and Instruction. pp. 1--11\nKim, Y.G. 2017. Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. In Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier). Scientific Studies of Reading. pp. 310--333\nKlufa, J. 2015. Multiple choice question testsadvantages and disadvantages. In 3rd International Conference on Education and Modern Educational Technologies (EMET). pp. 39--42\nKo\u010disk\u1ef3, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. 2018. The narrativeqa reading comprehension challenge. In Transactions of the Association for Computational Linguistics. pp. 317--328\nKry\u015bci\u0144ski, W. and Rajani, N. 2021. Booksum: A collection of datasets for longform narrative summarization. In Booksum: A collection of datasets for longform narrative summarization. arXiv:2105.08209\nKurdi, G., Leo, J., Parsia, B., Sattler, U., and Al-Emari, S. 2020. A systematic review of automatic question generation for educational purposes. In International Journal of Artificial Intelligence in Education. pp. 121--204\nLadhak, F., Li, B., Al-Onaizan, Y., and Mckeown, K. 2020. Exploring content selection in summarization of novel chapters. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 5043--5054\nLai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. 2017. Race: Large-scale reading comprehension dataset from examinations. In Race: Large-scale reading comprehension dataset from examinations. arXiv:1704.04683\nKumar Lal, Y., Chambers, N., Mooney, R., and Balasubramanian, N. 2021. Tellmewhy: A dataset for answering why-questions in narratives. In Tellmewhy: A dataset for answering why-questions in narratives. arXiv:2106.06132\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2019. Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv:1910.13461\nMeghan, D. and Liebfreund 2021. Cognitive and motivational predictors of narrative and informational text comprehension. In Reading Psychology. pp. 177--196\nJulie, S., Lynch, P., Van Den, Broek, K. E., Kremer, P., Kendeou, M. J., White, E. P., and Lorch 2008. The development of narrative comprehension and its relation to other early reading skills. In Reading Psychology. pp. 327--365\nNancy, A., Martin, R., and Brownell 2011. Expressive one-word picture vocabulary test-4 (EOWPVT. In Expressive one-word picture vocabulary test-4 (EOWPVT.\nMostafazadeh, N., Chambers, N., He, X., Parikh, D., Batra, D., Vanderwende, L., Kohli, P., and Allen, J. 2016. A corpus and evaluation framework for deeper understanding of commonsense stories. In A corpus and evaluation framework for deeper understanding of commonsense stories. arXiv:1604.01696\nMou, X., Yang, C., Yu, M., Yao, B., Guo, X., Potdar, S., and Su, H. 2021. Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. In Narrative question answering with cuttingedge open-domain qa techniques: A comprehensive study. arXiv:2106.03826\nEzgi \u00c7etinkaya \u00d6zdemir, H. and Akyol 2019. The development of a reading comprehension test. In Universal Journal of Educational Research. pp. 563--570\nAlison, H., Paris, G., and Paris 2003. Assessing narrative comprehension in young children. In Reading Research Quarterly. pp. 36--76\nTaffy, E. and Raphael 1986. Teaching question answer relationships, revisited. The reading teacher. In Teaching question answer relationships, revisited. The reading teacher. pp. 516--522\nRoberts, P. and Priest, H. 2006. Reliability and validity in research. In Nursing standard. pp. 41--46\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv:1910.01108\nSims, M., Park, J. H., and Bamman, D. 2019. Literary event detection. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 3623--3634\nXie, Q., Lai, G., Dai, Z., and Hovy, E. 2017. Large-scale cloze test dataset created by teachers. In Large-scale cloze test dataset created by teachers. arXiv:1711.03225\nYao, B., Wang, D., Wu, T., Hoang, T., Sun, B., Li, T.J., Yu, M., and Xu, Y. 2021. It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. In It is ai's turn to ask human a question: Question and answer pair generation for children storybooks in fairytaleqa dataset. arXiv:2109.03423"
    }
]