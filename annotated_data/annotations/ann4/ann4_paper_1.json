[
    {
        "filename": "paper_1.txt",
        "start": 741,
        "end": 843,
        "label": "Coherence",
        "text": "For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets.",
        "full_text": "Related Work\n\nLarge-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Sch\u00fctze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge.\n\nIntroducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.\n\nThere are various types of knowledge resources that can be used to enhance language models, focusing on different aspects. For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 . We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge align with our need.\n\n References: \nBanerjee, S. and Lavie, A. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. pp. 65--72\nBodenreider, O. 2004. The unified medical language system (umls): integrating biomedical terminology. In Nucleic acids research. pp. D267--D270\nBosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celikyilmaz, A., and Choi, Y. 2019. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4762--4779 10.18653/v1/P19-1470\nChakrabarty, T., Ghosh, D., Muresan, S., and Peng, N. 2020. R\u02c63: Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7976--7986 10.18653/v1/2020.acl-main.711\nDo, N. and Pavlick, E. 2021. Are rotten apples edible? challenging commonsense inference ability with exceptions. In FINDINGS.\nHendrickx, I., Kim, S. N., Kozareva, Z., Nakov, P., Diarmuid, '., O S'eaghdha, S., Pad'o, M., Pennacchiotti, L., Romano, S., and Szpakowicz 2010. SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation. pp. 33--38\nElizabeth M Huffman, D. I., Athanasiadis, N. E., Anton, Lindsay, A., Haskett, D. L., Doster, D., Stefanidis, N. K., and Lee 2021. How resilient is your team? exploring healthcare providers' well-being during the covid-19 pandemic. In The American Journal of Surgery. pp. 277--284\nHwang, J. D., Bhagavatula, C., Ronan Le Bras, J., Da, K., Sakaguchi, A., Bosselut, Y., and Choi 2020. Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs. In ArXiv. abs/2010.05953\nJi, H., Ke, P., Huang, S., Wei, F., Zhu, X., and Huang, M. 2020. Language generation with multi-hop reasoning on commonsense knowledge graph. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 725--736 10.18653/v1/2020.emnlp-main.54\nKassner, N. and Sch\u00fctze, H. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In ACL.\nKatz, N. and Mcnulty, K. 1994. Reflective listening. In Reflective listening.\nKitaev, N. and Klein, D. 2018. Constituency parsing with a self-attentive encoder. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 2676--2686 10.18653/v1/P18-1249\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 1910. Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs. In Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs.\nLi, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 110--119 10.18653/v1/N16-1014\nLin, C. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out. pp. 74--81\nLiu, W., Zhou, P., Zhao, Z., Wang, Z., Ju, Q., Deng, H., and Wang, P. 2020. K-bert: Enabling language representation with knowledge graph. In ArXiv. abs/1909.07606\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nHuanru Henry Mao, B. P., Majumder, J., Mcauley, G., and Cottrell 2019. Improving neural story generation by targeted common sense grounding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5988--5993 10.18653/v1/D19-1615\nWilliam, R., Miller, S., and Rollnick 2012. Motivational interviewing: Helping people change. In Motivational interviewing: Helping people change.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135\nMario R Paredes, V., Apaolaza, C., Fernandez-Robin, P., Hartmann, D., and Ya\u00f1ez-Martinez 2021. The impact of the covid-19 pandemic on subjective mental well-being: The interplay of perceived threat, future anxiety and resilience. In Personality and Individual Differences. pp. 110455\nP\u00e9rez-Rosas, V., Mihalcea, R., Resnicow, K., Singh, S., and An, L. 2016. Building a motivational interviewing dataset. In Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology. pp. 42--51 10.18653/v1/W16-0305\nPetroni, F., Rockt\u00e4schel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. 2019. Language models as knowledge bases? arXiv preprint. In Language models as knowledge bases? arXiv preprint. arXiv:1909.01066\nPetroni, F., Rockt\u00e4schel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. 2019. Association for Computational Linguistics. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2463--2473 10.18653/v1/D19-1250\nRadford, A. and Narasimhan, K. 2018. Improving language understanding by generative pre-training. In Improving language understanding by generative pre-training.\nReimers, N. and Gurevych, I. 1908. Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs. In Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs.\nReimers, N. and Gurevych, I. 2019. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3982--3992 10.18653/v1/D19-1410\nRen, P., Chen, Z., Monz, C., Ma, J., and De Rijke, M. 2020. Thinking globally, acting locally: Distantly supervised global-to-local knowledge selection for background based conversation. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8697--8704\nSap, M., Ronan Le Bras, E., Allaway, C., Bhagavatula, N., Lourie, H., Rashkin, B., Roof, Noah, A., Smith, Y., and Choi 2019. Atomic: An atlas of machine commonsense for ifthen reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 3027--3035\nShwartz, V., West, P., Ronan, L., Bras, C., Bhagavatula, Y., and Choi 2020. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 4615--4629 10.18653/v1/2020.emnlp-main.373\nSong, H., Zhang, W., Cui, Y., Wang, D., and Liu, T. 2019. Exploiting persona information for diverse generation of conversational responses. In IJCAI.\nSpeer, R., Chin, J., and Havasi, C. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence.\nWolf, T., Sanh, V., Chaumond, J., and Delangue, C. 2019. Transfertransfo: A transfer learning approach for neural network based conversational agents. In Transfertransfo: A transfer learning approach for neural network based conversational agents. arXiv:1901.08149\nZhang, H., Khashabi, D., Song, Y., and Roth, D. 2020. Transomcs: From linguistic graphs to commonsense knowledge. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI). pp. 2020\nZhang, T., Kishore, V., Wu, F., Kilian, Q., Weinberger, Y., and Artzi 2019. Bertscore: Evaluating text generation with bert. In Bertscore: Evaluating text generation with bert. arXiv:1904.09675\nZhao, X., Wu, W., Xu, C., Tao, C., Zhao, D., and Yan, R. 2020. Knowledgegrounded dialogue generation with pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 3377--3390 10.18653/v1/2020.emnlp-main.272"
    },
    {
        "filename": "paper_1.txt",
        "start": 1022,
        "end": 1101,
        "label": "Coherence",
        "text": "Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet.",
        "full_text": "Related Work\n\nLarge-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Sch\u00fctze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge.\n\nIntroducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.\n\nThere are various types of knowledge resources that can be used to enhance language models, focusing on different aspects. For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 . We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge align with our need.\n\n References: \nBanerjee, S. and Lavie, A. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. pp. 65--72\nBodenreider, O. 2004. The unified medical language system (umls): integrating biomedical terminology. In Nucleic acids research. pp. D267--D270\nBosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celikyilmaz, A., and Choi, Y. 2019. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4762--4779 10.18653/v1/P19-1470\nChakrabarty, T., Ghosh, D., Muresan, S., and Peng, N. 2020. R\u02c63: Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7976--7986 10.18653/v1/2020.acl-main.711\nDo, N. and Pavlick, E. 2021. Are rotten apples edible? challenging commonsense inference ability with exceptions. In FINDINGS.\nHendrickx, I., Kim, S. N., Kozareva, Z., Nakov, P., Diarmuid, '., O S'eaghdha, S., Pad'o, M., Pennacchiotti, L., Romano, S., and Szpakowicz 2010. SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation. pp. 33--38\nElizabeth M Huffman, D. I., Athanasiadis, N. E., Anton, Lindsay, A., Haskett, D. L., Doster, D., Stefanidis, N. K., and Lee 2021. How resilient is your team? exploring healthcare providers' well-being during the covid-19 pandemic. In The American Journal of Surgery. pp. 277--284\nHwang, J. D., Bhagavatula, C., Ronan Le Bras, J., Da, K., Sakaguchi, A., Bosselut, Y., and Choi 2020. Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs. In ArXiv. abs/2010.05953\nJi, H., Ke, P., Huang, S., Wei, F., Zhu, X., and Huang, M. 2020. Language generation with multi-hop reasoning on commonsense knowledge graph. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 725--736 10.18653/v1/2020.emnlp-main.54\nKassner, N. and Sch\u00fctze, H. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In ACL.\nKatz, N. and Mcnulty, K. 1994. Reflective listening. In Reflective listening.\nKitaev, N. and Klein, D. 2018. Constituency parsing with a self-attentive encoder. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 2676--2686 10.18653/v1/P18-1249\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 1910. Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs. In Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs.\nLi, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 110--119 10.18653/v1/N16-1014\nLin, C. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out. pp. 74--81\nLiu, W., Zhou, P., Zhao, Z., Wang, Z., Ju, Q., Deng, H., and Wang, P. 2020. K-bert: Enabling language representation with knowledge graph. In ArXiv. abs/1909.07606\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nHuanru Henry Mao, B. P., Majumder, J., Mcauley, G., and Cottrell 2019. Improving neural story generation by targeted common sense grounding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5988--5993 10.18653/v1/D19-1615\nWilliam, R., Miller, S., and Rollnick 2012. Motivational interviewing: Helping people change. In Motivational interviewing: Helping people change.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135\nMario R Paredes, V., Apaolaza, C., Fernandez-Robin, P., Hartmann, D., and Ya\u00f1ez-Martinez 2021. The impact of the covid-19 pandemic on subjective mental well-being: The interplay of perceived threat, future anxiety and resilience. In Personality and Individual Differences. pp. 110455\nP\u00e9rez-Rosas, V., Mihalcea, R., Resnicow, K., Singh, S., and An, L. 2016. Building a motivational interviewing dataset. In Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology. pp. 42--51 10.18653/v1/W16-0305\nPetroni, F., Rockt\u00e4schel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. 2019. Language models as knowledge bases? arXiv preprint. In Language models as knowledge bases? arXiv preprint. arXiv:1909.01066\nPetroni, F., Rockt\u00e4schel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. 2019. Association for Computational Linguistics. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2463--2473 10.18653/v1/D19-1250\nRadford, A. and Narasimhan, K. 2018. Improving language understanding by generative pre-training. In Improving language understanding by generative pre-training.\nReimers, N. and Gurevych, I. 1908. Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs. In Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs.\nReimers, N. and Gurevych, I. 2019. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3982--3992 10.18653/v1/D19-1410\nRen, P., Chen, Z., Monz, C., Ma, J., and De Rijke, M. 2020. Thinking globally, acting locally: Distantly supervised global-to-local knowledge selection for background based conversation. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8697--8704\nSap, M., Ronan Le Bras, E., Allaway, C., Bhagavatula, N., Lourie, H., Rashkin, B., Roof, Noah, A., Smith, Y., and Choi 2019. Atomic: An atlas of machine commonsense for ifthen reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 3027--3035\nShwartz, V., West, P., Ronan, L., Bras, C., Bhagavatula, Y., and Choi 2020. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 4615--4629 10.18653/v1/2020.emnlp-main.373\nSong, H., Zhang, W., Cui, Y., Wang, D., and Liu, T. 2019. Exploiting persona information for diverse generation of conversational responses. In IJCAI.\nSpeer, R., Chin, J., and Havasi, C. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence.\nWolf, T., Sanh, V., Chaumond, J., and Delangue, C. 2019. Transfertransfo: A transfer learning approach for neural network based conversational agents. In Transfertransfo: A transfer learning approach for neural network based conversational agents. arXiv:1901.08149\nZhang, H., Khashabi, D., Song, Y., and Roth, D. 2020. Transomcs: From linguistic graphs to commonsense knowledge. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI). pp. 2020\nZhang, T., Kishore, V., Wu, F., Kilian, Q., Weinberger, Y., and Artzi 2019. Bertscore: Evaluating text generation with bert. In Bertscore: Evaluating text generation with bert. arXiv:1904.09675\nZhao, X., Wu, W., Xu, C., Tao, C., Zhao, D., and Yan, R. 2020. Knowledgegrounded dialogue generation with pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 3377--3390 10.18653/v1/2020.emnlp-main.272"
    },
    {
        "filename": "paper_1.txt",
        "start": 1259,
        "end": 1295,
        "label": "Format",
        "text": "various types of knowledge resources",
        "full_text": "Related Work\n\nLarge-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Sch\u00fctze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge.\n\nIntroducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.\n\nThere are various types of knowledge resources that can be used to enhance language models, focusing on different aspects. For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 . We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge align with our need.\n\n References: \nBanerjee, S. and Lavie, A. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. pp. 65--72\nBodenreider, O. 2004. The unified medical language system (umls): integrating biomedical terminology. In Nucleic acids research. pp. D267--D270\nBosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celikyilmaz, A., and Choi, Y. 2019. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4762--4779 10.18653/v1/P19-1470\nChakrabarty, T., Ghosh, D., Muresan, S., and Peng, N. 2020. R\u02c63: Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7976--7986 10.18653/v1/2020.acl-main.711\nDo, N. and Pavlick, E. 2021. Are rotten apples edible? challenging commonsense inference ability with exceptions. In FINDINGS.\nHendrickx, I., Kim, S. N., Kozareva, Z., Nakov, P., Diarmuid, '., O S'eaghdha, S., Pad'o, M., Pennacchiotti, L., Romano, S., and Szpakowicz 2010. SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation. pp. 33--38\nElizabeth M Huffman, D. I., Athanasiadis, N. E., Anton, Lindsay, A., Haskett, D. L., Doster, D., Stefanidis, N. K., and Lee 2021. How resilient is your team? exploring healthcare providers' well-being during the covid-19 pandemic. In The American Journal of Surgery. pp. 277--284\nHwang, J. D., Bhagavatula, C., Ronan Le Bras, J., Da, K., Sakaguchi, A., Bosselut, Y., and Choi 2020. Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs. In ArXiv. abs/2010.05953\nJi, H., Ke, P., Huang, S., Wei, F., Zhu, X., and Huang, M. 2020. Language generation with multi-hop reasoning on commonsense knowledge graph. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 725--736 10.18653/v1/2020.emnlp-main.54\nKassner, N. and Sch\u00fctze, H. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In ACL.\nKatz, N. and Mcnulty, K. 1994. Reflective listening. In Reflective listening.\nKitaev, N. and Klein, D. 2018. Constituency parsing with a self-attentive encoder. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 2676--2686 10.18653/v1/P18-1249\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 1910. Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs. In Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs.\nLi, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 110--119 10.18653/v1/N16-1014\nLin, C. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out. pp. 74--81\nLiu, W., Zhou, P., Zhao, Z., Wang, Z., Ju, Q., Deng, H., and Wang, P. 2020. K-bert: Enabling language representation with knowledge graph. In ArXiv. abs/1909.07606\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nHuanru Henry Mao, B. P., Majumder, J., Mcauley, G., and Cottrell 2019. Improving neural story generation by targeted common sense grounding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5988--5993 10.18653/v1/D19-1615\nWilliam, R., Miller, S., and Rollnick 2012. Motivational interviewing: Helping people change. In Motivational interviewing: Helping people change.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135\nMario R Paredes, V., Apaolaza, C., Fernandez-Robin, P., Hartmann, D., and Ya\u00f1ez-Martinez 2021. The impact of the covid-19 pandemic on subjective mental well-being: The interplay of perceived threat, future anxiety and resilience. In Personality and Individual Differences. pp. 110455\nP\u00e9rez-Rosas, V., Mihalcea, R., Resnicow, K., Singh, S., and An, L. 2016. Building a motivational interviewing dataset. In Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology. pp. 42--51 10.18653/v1/W16-0305\nPetroni, F., Rockt\u00e4schel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. 2019. Language models as knowledge bases? arXiv preprint. In Language models as knowledge bases? arXiv preprint. arXiv:1909.01066\nPetroni, F., Rockt\u00e4schel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. 2019. Association for Computational Linguistics. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2463--2473 10.18653/v1/D19-1250\nRadford, A. and Narasimhan, K. 2018. Improving language understanding by generative pre-training. In Improving language understanding by generative pre-training.\nReimers, N. and Gurevych, I. 1908. Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs. In Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs.\nReimers, N. and Gurevych, I. 2019. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3982--3992 10.18653/v1/D19-1410\nRen, P., Chen, Z., Monz, C., Ma, J., and De Rijke, M. 2020. Thinking globally, acting locally: Distantly supervised global-to-local knowledge selection for background based conversation. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8697--8704\nSap, M., Ronan Le Bras, E., Allaway, C., Bhagavatula, N., Lourie, H., Rashkin, B., Roof, Noah, A., Smith, Y., and Choi 2019. Atomic: An atlas of machine commonsense for ifthen reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 3027--3035\nShwartz, V., West, P., Ronan, L., Bras, C., Bhagavatula, Y., and Choi 2020. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 4615--4629 10.18653/v1/2020.emnlp-main.373\nSong, H., Zhang, W., Cui, Y., Wang, D., and Liu, T. 2019. Exploiting persona information for diverse generation of conversational responses. In IJCAI.\nSpeer, R., Chin, J., and Havasi, C. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence.\nWolf, T., Sanh, V., Chaumond, J., and Delangue, C. 2019. Transfertransfo: A transfer learning approach for neural network based conversational agents. In Transfertransfo: A transfer learning approach for neural network based conversational agents. arXiv:1901.08149\nZhang, H., Khashabi, D., Song, Y., and Roth, D. 2020. Transomcs: From linguistic graphs to commonsense knowledge. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI). pp. 2020\nZhang, T., Kishore, V., Wu, F., Kilian, Q., Weinberger, Y., and Artzi 2019. Bertscore: Evaluating text generation with bert. In Bertscore: Evaluating text generation with bert. arXiv:1904.09675\nZhao, X., Wu, W., Xu, C., Tao, C., Zhao, D., and Yan, R. 2020. Knowledgegrounded dialogue generation with pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 3377--3390 10.18653/v1/2020.emnlp-main.272"
    },
    {
        "filename": "paper_1.txt",
        "start": 1384,
        "end": 1432,
        "label": "Format",
        "text": "large-scale commonsense knowledge graphs (CSKG)",
        "full_text": "Related Work\n\nLarge-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Sch\u00fctze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge.\n\nIntroducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.\n\nThere are various types of knowledge resources that can be used to enhance language models, focusing on different aspects. For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 . We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge align with our need.\n\n References: \nBanerjee, S. and Lavie, A. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. pp. 65--72\nBodenreider, O. 2004. The unified medical language system (umls): integrating biomedical terminology. In Nucleic acids research. pp. D267--D270\nBosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celikyilmaz, A., and Choi, Y. 2019. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4762--4779 10.18653/v1/P19-1470\nChakrabarty, T., Ghosh, D., Muresan, S., and Peng, N. 2020. R\u02c63: Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7976--7986 10.18653/v1/2020.acl-main.711\nDo, N. and Pavlick, E. 2021. Are rotten apples edible? challenging commonsense inference ability with exceptions. In FINDINGS.\nHendrickx, I., Kim, S. N., Kozareva, Z., Nakov, P., Diarmuid, '., O S'eaghdha, S., Pad'o, M., Pennacchiotti, L., Romano, S., and Szpakowicz 2010. SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation. pp. 33--38\nElizabeth M Huffman, D. I., Athanasiadis, N. E., Anton, Lindsay, A., Haskett, D. L., Doster, D., Stefanidis, N. K., and Lee 2021. How resilient is your team? exploring healthcare providers' well-being during the covid-19 pandemic. In The American Journal of Surgery. pp. 277--284\nHwang, J. D., Bhagavatula, C., Ronan Le Bras, J., Da, K., Sakaguchi, A., Bosselut, Y., and Choi 2020. Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs. In ArXiv. abs/2010.05953\nJi, H., Ke, P., Huang, S., Wei, F., Zhu, X., and Huang, M. 2020. Language generation with multi-hop reasoning on commonsense knowledge graph. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 725--736 10.18653/v1/2020.emnlp-main.54\nKassner, N. and Sch\u00fctze, H. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In ACL.\nKatz, N. and Mcnulty, K. 1994. Reflective listening. In Reflective listening.\nKitaev, N. and Klein, D. 2018. Constituency parsing with a self-attentive encoder. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 2676--2686 10.18653/v1/P18-1249\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 1910. Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs. In Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs.\nLi, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 110--119 10.18653/v1/N16-1014\nLin, C. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out. pp. 74--81\nLiu, W., Zhou, P., Zhao, Z., Wang, Z., Ju, Q., Deng, H., and Wang, P. 2020. K-bert: Enabling language representation with knowledge graph. In ArXiv. abs/1909.07606\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nHuanru Henry Mao, B. P., Majumder, J., Mcauley, G., and Cottrell 2019. Improving neural story generation by targeted common sense grounding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5988--5993 10.18653/v1/D19-1615\nWilliam, R., Miller, S., and Rollnick 2012. Motivational interviewing: Helping people change. In Motivational interviewing: Helping people change.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135\nMario R Paredes, V., Apaolaza, C., Fernandez-Robin, P., Hartmann, D., and Ya\u00f1ez-Martinez 2021. The impact of the covid-19 pandemic on subjective mental well-being: The interplay of perceived threat, future anxiety and resilience. In Personality and Individual Differences. pp. 110455\nP\u00e9rez-Rosas, V., Mihalcea, R., Resnicow, K., Singh, S., and An, L. 2016. Building a motivational interviewing dataset. In Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology. pp. 42--51 10.18653/v1/W16-0305\nPetroni, F., Rockt\u00e4schel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. 2019. Language models as knowledge bases? arXiv preprint. In Language models as knowledge bases? arXiv preprint. arXiv:1909.01066\nPetroni, F., Rockt\u00e4schel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. 2019. Association for Computational Linguistics. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2463--2473 10.18653/v1/D19-1250\nRadford, A. and Narasimhan, K. 2018. Improving language understanding by generative pre-training. In Improving language understanding by generative pre-training.\nReimers, N. and Gurevych, I. 1908. Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs. In Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs.\nReimers, N. and Gurevych, I. 2019. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3982--3992 10.18653/v1/D19-1410\nRen, P., Chen, Z., Monz, C., Ma, J., and De Rijke, M. 2020. Thinking globally, acting locally: Distantly supervised global-to-local knowledge selection for background based conversation. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8697--8704\nSap, M., Ronan Le Bras, E., Allaway, C., Bhagavatula, N., Lourie, H., Rashkin, B., Roof, Noah, A., Smith, Y., and Choi 2019. Atomic: An atlas of machine commonsense for ifthen reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 3027--3035\nShwartz, V., West, P., Ronan, L., Bras, C., Bhagavatula, Y., and Choi 2020. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 4615--4629 10.18653/v1/2020.emnlp-main.373\nSong, H., Zhang, W., Cui, Y., Wang, D., and Liu, T. 2019. Exploiting persona information for diverse generation of conversational responses. In IJCAI.\nSpeer, R., Chin, J., and Havasi, C. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence.\nWolf, T., Sanh, V., Chaumond, J., and Delangue, C. 2019. Transfertransfo: A transfer learning approach for neural network based conversational agents. In Transfertransfo: A transfer learning approach for neural network based conversational agents. arXiv:1901.08149\nZhang, H., Khashabi, D., Song, Y., and Roth, D. 2020. Transomcs: From linguistic graphs to commonsense knowledge. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI). pp. 2020\nZhang, T., Kishore, V., Wu, F., Kilian, Q., Weinberger, Y., and Artzi 2019. Bertscore: Evaluating text generation with bert. In Bertscore: Evaluating text generation with bert. arXiv:1904.09675\nZhao, X., Wu, W., Xu, C., Tao, C., Zhao, D., and Yan, R. 2020. Knowledgegrounded dialogue generation with pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 3377--3390 10.18653/v1/2020.emnlp-main.272"
    },
    {
        "filename": "paper_1.txt",
        "start": 1583,
        "end": 1585,
        "label": "Format",
        "text": ",",
        "full_text": "Related Work\n\nLarge-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Sch\u00fctze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge.\n\nIntroducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.\n\nThere are various types of knowledge resources that can be used to enhance language models, focusing on different aspects. For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 . We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge align with our need.\n\n References: \nBanerjee, S. and Lavie, A. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. pp. 65--72\nBodenreider, O. 2004. The unified medical language system (umls): integrating biomedical terminology. In Nucleic acids research. pp. D267--D270\nBosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celikyilmaz, A., and Choi, Y. 2019. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4762--4779 10.18653/v1/P19-1470\nChakrabarty, T., Ghosh, D., Muresan, S., and Peng, N. 2020. R\u02c63: Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7976--7986 10.18653/v1/2020.acl-main.711\nDo, N. and Pavlick, E. 2021. Are rotten apples edible? challenging commonsense inference ability with exceptions. In FINDINGS.\nHendrickx, I., Kim, S. N., Kozareva, Z., Nakov, P., Diarmuid, '., O S'eaghdha, S., Pad'o, M., Pennacchiotti, L., Romano, S., and Szpakowicz 2010. SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation. pp. 33--38\nElizabeth M Huffman, D. I., Athanasiadis, N. E., Anton, Lindsay, A., Haskett, D. L., Doster, D., Stefanidis, N. K., and Lee 2021. How resilient is your team? exploring healthcare providers' well-being during the covid-19 pandemic. In The American Journal of Surgery. pp. 277--284\nHwang, J. D., Bhagavatula, C., Ronan Le Bras, J., Da, K., Sakaguchi, A., Bosselut, Y., and Choi 2020. Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs. In ArXiv. abs/2010.05953\nJi, H., Ke, P., Huang, S., Wei, F., Zhu, X., and Huang, M. 2020. Language generation with multi-hop reasoning on commonsense knowledge graph. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 725--736 10.18653/v1/2020.emnlp-main.54\nKassner, N. and Sch\u00fctze, H. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In ACL.\nKatz, N. and Mcnulty, K. 1994. Reflective listening. In Reflective listening.\nKitaev, N. and Klein, D. 2018. Constituency parsing with a self-attentive encoder. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 2676--2686 10.18653/v1/P18-1249\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 1910. Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs. In Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs.\nLi, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 110--119 10.18653/v1/N16-1014\nLin, C. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out. pp. 74--81\nLiu, W., Zhou, P., Zhao, Z., Wang, Z., Ju, Q., Deng, H., and Wang, P. 2020. K-bert: Enabling language representation with knowledge graph. In ArXiv. abs/1909.07606\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nHuanru Henry Mao, B. P., Majumder, J., Mcauley, G., and Cottrell 2019. Improving neural story generation by targeted common sense grounding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5988--5993 10.18653/v1/D19-1615\nWilliam, R., Miller, S., and Rollnick 2012. Motivational interviewing: Helping people change. In Motivational interviewing: Helping people change.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135\nMario R Paredes, V., Apaolaza, C., Fernandez-Robin, P., Hartmann, D., and Ya\u00f1ez-Martinez 2021. The impact of the covid-19 pandemic on subjective mental well-being: The interplay of perceived threat, future anxiety and resilience. In Personality and Individual Differences. pp. 110455\nP\u00e9rez-Rosas, V., Mihalcea, R., Resnicow, K., Singh, S., and An, L. 2016. Building a motivational interviewing dataset. In Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology. pp. 42--51 10.18653/v1/W16-0305\nPetroni, F., Rockt\u00e4schel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. 2019. Language models as knowledge bases? arXiv preprint. In Language models as knowledge bases? arXiv preprint. arXiv:1909.01066\nPetroni, F., Rockt\u00e4schel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. 2019. Association for Computational Linguistics. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2463--2473 10.18653/v1/D19-1250\nRadford, A. and Narasimhan, K. 2018. Improving language understanding by generative pre-training. In Improving language understanding by generative pre-training.\nReimers, N. and Gurevych, I. 1908. Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs. In Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs.\nReimers, N. and Gurevych, I. 2019. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3982--3992 10.18653/v1/D19-1410\nRen, P., Chen, Z., Monz, C., Ma, J., and De Rijke, M. 2020. Thinking globally, acting locally: Distantly supervised global-to-local knowledge selection for background based conversation. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8697--8704\nSap, M., Ronan Le Bras, E., Allaway, C., Bhagavatula, N., Lourie, H., Rashkin, B., Roof, Noah, A., Smith, Y., and Choi 2019. Atomic: An atlas of machine commonsense for ifthen reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 3027--3035\nShwartz, V., West, P., Ronan, L., Bras, C., Bhagavatula, Y., and Choi 2020. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 4615--4629 10.18653/v1/2020.emnlp-main.373\nSong, H., Zhang, W., Cui, Y., Wang, D., and Liu, T. 2019. Exploiting persona information for diverse generation of conversational responses. In IJCAI.\nSpeer, R., Chin, J., and Havasi, C. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence.\nWolf, T., Sanh, V., Chaumond, J., and Delangue, C. 2019. Transfertransfo: A transfer learning approach for neural network based conversational agents. In Transfertransfo: A transfer learning approach for neural network based conversational agents. arXiv:1901.08149\nZhang, H., Khashabi, D., Song, Y., and Roth, D. 2020. Transomcs: From linguistic graphs to commonsense knowledge. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI). pp. 2020\nZhang, T., Kishore, V., Wu, F., Kilian, Q., Weinberger, Y., and Artzi 2019. Bertscore: Evaluating text generation with bert. In Bertscore: Evaluating text generation with bert. arXiv:1904.09675\nZhao, X., Wu, W., Xu, C., Tao, C., Zhao, D., and Yan, R. 2020. Knowledgegrounded dialogue generation with pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 3377--3390 10.18653/v1/2020.emnlp-main.272"
    },
    {
        "filename": "paper_1.txt",
        "start": 14,
        "end": 557,
        "label": "Lacks Synthesis",
        "text": "Large-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Sch\u00fctze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge.",
        "full_text": "Related Work\n\nLarge-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Sch\u00fctze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge.\n\nIntroducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.\n\nThere are various types of knowledge resources that can be used to enhance language models, focusing on different aspects. For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 . We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge align with our need.\n\n References: \nBanerjee, S. and Lavie, A. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. pp. 65--72\nBodenreider, O. 2004. The unified medical language system (umls): integrating biomedical terminology. In Nucleic acids research. pp. D267--D270\nBosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celikyilmaz, A., and Choi, Y. 2019. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4762--4779 10.18653/v1/P19-1470\nChakrabarty, T., Ghosh, D., Muresan, S., and Peng, N. 2020. R\u02c63: Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7976--7986 10.18653/v1/2020.acl-main.711\nDo, N. and Pavlick, E. 2021. Are rotten apples edible? challenging commonsense inference ability with exceptions. In FINDINGS.\nHendrickx, I., Kim, S. N., Kozareva, Z., Nakov, P., Diarmuid, '., O S'eaghdha, S., Pad'o, M., Pennacchiotti, L., Romano, S., and Szpakowicz 2010. SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation. pp. 33--38\nElizabeth M Huffman, D. I., Athanasiadis, N. E., Anton, Lindsay, A., Haskett, D. L., Doster, D., Stefanidis, N. K., and Lee 2021. How resilient is your team? exploring healthcare providers' well-being during the covid-19 pandemic. In The American Journal of Surgery. pp. 277--284\nHwang, J. D., Bhagavatula, C., Ronan Le Bras, J., Da, K., Sakaguchi, A., Bosselut, Y., and Choi 2020. Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs. In ArXiv. abs/2010.05953\nJi, H., Ke, P., Huang, S., Wei, F., Zhu, X., and Huang, M. 2020. Language generation with multi-hop reasoning on commonsense knowledge graph. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 725--736 10.18653/v1/2020.emnlp-main.54\nKassner, N. and Sch\u00fctze, H. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In ACL.\nKatz, N. and Mcnulty, K. 1994. Reflective listening. In Reflective listening.\nKitaev, N. and Klein, D. 2018. Constituency parsing with a self-attentive encoder. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 2676--2686 10.18653/v1/P18-1249\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 1910. Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs. In Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs.\nLi, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 110--119 10.18653/v1/N16-1014\nLin, C. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out. pp. 74--81\nLiu, W., Zhou, P., Zhao, Z., Wang, Z., Ju, Q., Deng, H., and Wang, P. 2020. K-bert: Enabling language representation with knowledge graph. In ArXiv. abs/1909.07606\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nHuanru Henry Mao, B. P., Majumder, J., Mcauley, G., and Cottrell 2019. Improving neural story generation by targeted common sense grounding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5988--5993 10.18653/v1/D19-1615\nWilliam, R., Miller, S., and Rollnick 2012. Motivational interviewing: Helping people change. In Motivational interviewing: Helping people change.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135\nMario R Paredes, V., Apaolaza, C., Fernandez-Robin, P., Hartmann, D., and Ya\u00f1ez-Martinez 2021. The impact of the covid-19 pandemic on subjective mental well-being: The interplay of perceived threat, future anxiety and resilience. In Personality and Individual Differences. pp. 110455\nP\u00e9rez-Rosas, V., Mihalcea, R., Resnicow, K., Singh, S., and An, L. 2016. Building a motivational interviewing dataset. In Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology. pp. 42--51 10.18653/v1/W16-0305\nPetroni, F., Rockt\u00e4schel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. 2019. Language models as knowledge bases? arXiv preprint. In Language models as knowledge bases? arXiv preprint. arXiv:1909.01066\nPetroni, F., Rockt\u00e4schel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. 2019. Association for Computational Linguistics. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2463--2473 10.18653/v1/D19-1250\nRadford, A. and Narasimhan, K. 2018. Improving language understanding by generative pre-training. In Improving language understanding by generative pre-training.\nReimers, N. and Gurevych, I. 1908. Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs. In Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs.\nReimers, N. and Gurevych, I. 2019. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3982--3992 10.18653/v1/D19-1410\nRen, P., Chen, Z., Monz, C., Ma, J., and De Rijke, M. 2020. Thinking globally, acting locally: Distantly supervised global-to-local knowledge selection for background based conversation. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8697--8704\nSap, M., Ronan Le Bras, E., Allaway, C., Bhagavatula, N., Lourie, H., Rashkin, B., Roof, Noah, A., Smith, Y., and Choi 2019. Atomic: An atlas of machine commonsense for ifthen reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 3027--3035\nShwartz, V., West, P., Ronan, L., Bras, C., Bhagavatula, Y., and Choi 2020. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 4615--4629 10.18653/v1/2020.emnlp-main.373\nSong, H., Zhang, W., Cui, Y., Wang, D., and Liu, T. 2019. Exploiting persona information for diverse generation of conversational responses. In IJCAI.\nSpeer, R., Chin, J., and Havasi, C. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence.\nWolf, T., Sanh, V., Chaumond, J., and Delangue, C. 2019. Transfertransfo: A transfer learning approach for neural network based conversational agents. In Transfertransfo: A transfer learning approach for neural network based conversational agents. arXiv:1901.08149\nZhang, H., Khashabi, D., Song, Y., and Roth, D. 2020. Transomcs: From linguistic graphs to commonsense knowledge. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI). pp. 2020\nZhang, T., Kishore, V., Wu, F., Kilian, Q., Weinberger, Y., and Artzi 2019. Bertscore: Evaluating text generation with bert. In Bertscore: Evaluating text generation with bert. arXiv:1904.09675\nZhao, X., Wu, W., Xu, C., Tao, C., Zhao, D., and Yan, R. 2020. Knowledgegrounded dialogue generation with pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 3377--3390 10.18653/v1/2020.emnlp-main.272"
    },
    {
        "filename": "paper_1.txt",
        "start": 559,
        "end": 1247,
        "label": "Lacks Synthesis",
        "text": "Introducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.",
        "full_text": "Related Work\n\nLarge-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Sch\u00fctze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge.\n\nIntroducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.\n\nThere are various types of knowledge resources that can be used to enhance language models, focusing on different aspects. For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 . We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge align with our need.\n\n References: \nBanerjee, S. and Lavie, A. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. pp. 65--72\nBodenreider, O. 2004. The unified medical language system (umls): integrating biomedical terminology. In Nucleic acids research. pp. D267--D270\nBosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celikyilmaz, A., and Choi, Y. 2019. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4762--4779 10.18653/v1/P19-1470\nChakrabarty, T., Ghosh, D., Muresan, S., and Peng, N. 2020. R\u02c63: Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7976--7986 10.18653/v1/2020.acl-main.711\nDo, N. and Pavlick, E. 2021. Are rotten apples edible? challenging commonsense inference ability with exceptions. In FINDINGS.\nHendrickx, I., Kim, S. N., Kozareva, Z., Nakov, P., Diarmuid, '., O S'eaghdha, S., Pad'o, M., Pennacchiotti, L., Romano, S., and Szpakowicz 2010. SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation. pp. 33--38\nElizabeth M Huffman, D. I., Athanasiadis, N. E., Anton, Lindsay, A., Haskett, D. L., Doster, D., Stefanidis, N. K., and Lee 2021. How resilient is your team? exploring healthcare providers' well-being during the covid-19 pandemic. In The American Journal of Surgery. pp. 277--284\nHwang, J. D., Bhagavatula, C., Ronan Le Bras, J., Da, K., Sakaguchi, A., Bosselut, Y., and Choi 2020. Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs. In ArXiv. abs/2010.05953\nJi, H., Ke, P., Huang, S., Wei, F., Zhu, X., and Huang, M. 2020. Language generation with multi-hop reasoning on commonsense knowledge graph. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 725--736 10.18653/v1/2020.emnlp-main.54\nKassner, N. and Sch\u00fctze, H. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In ACL.\nKatz, N. and Mcnulty, K. 1994. Reflective listening. In Reflective listening.\nKitaev, N. and Klein, D. 2018. Constituency parsing with a self-attentive encoder. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 2676--2686 10.18653/v1/P18-1249\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 1910. Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs. In Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs.\nLi, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 110--119 10.18653/v1/N16-1014\nLin, C. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out. pp. 74--81\nLiu, W., Zhou, P., Zhao, Z., Wang, Z., Ju, Q., Deng, H., and Wang, P. 2020. K-bert: Enabling language representation with knowledge graph. In ArXiv. abs/1909.07606\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nHuanru Henry Mao, B. P., Majumder, J., Mcauley, G., and Cottrell 2019. Improving neural story generation by targeted common sense grounding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5988--5993 10.18653/v1/D19-1615\nWilliam, R., Miller, S., and Rollnick 2012. Motivational interviewing: Helping people change. In Motivational interviewing: Helping people change.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135\nMario R Paredes, V., Apaolaza, C., Fernandez-Robin, P., Hartmann, D., and Ya\u00f1ez-Martinez 2021. The impact of the covid-19 pandemic on subjective mental well-being: The interplay of perceived threat, future anxiety and resilience. In Personality and Individual Differences. pp. 110455\nP\u00e9rez-Rosas, V., Mihalcea, R., Resnicow, K., Singh, S., and An, L. 2016. Building a motivational interviewing dataset. In Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology. pp. 42--51 10.18653/v1/W16-0305\nPetroni, F., Rockt\u00e4schel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. 2019. Language models as knowledge bases? arXiv preprint. In Language models as knowledge bases? arXiv preprint. arXiv:1909.01066\nPetroni, F., Rockt\u00e4schel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. 2019. Association for Computational Linguistics. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2463--2473 10.18653/v1/D19-1250\nRadford, A. and Narasimhan, K. 2018. Improving language understanding by generative pre-training. In Improving language understanding by generative pre-training.\nReimers, N. and Gurevych, I. 1908. Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs. In Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs.\nReimers, N. and Gurevych, I. 2019. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3982--3992 10.18653/v1/D19-1410\nRen, P., Chen, Z., Monz, C., Ma, J., and De Rijke, M. 2020. Thinking globally, acting locally: Distantly supervised global-to-local knowledge selection for background based conversation. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8697--8704\nSap, M., Ronan Le Bras, E., Allaway, C., Bhagavatula, N., Lourie, H., Rashkin, B., Roof, Noah, A., Smith, Y., and Choi 2019. Atomic: An atlas of machine commonsense for ifthen reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 3027--3035\nShwartz, V., West, P., Ronan, L., Bras, C., Bhagavatula, Y., and Choi 2020. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 4615--4629 10.18653/v1/2020.emnlp-main.373\nSong, H., Zhang, W., Cui, Y., Wang, D., and Liu, T. 2019. Exploiting persona information for diverse generation of conversational responses. In IJCAI.\nSpeer, R., Chin, J., and Havasi, C. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence.\nWolf, T., Sanh, V., Chaumond, J., and Delangue, C. 2019. Transfertransfo: A transfer learning approach for neural network based conversational agents. In Transfertransfo: A transfer learning approach for neural network based conversational agents. arXiv:1901.08149\nZhang, H., Khashabi, D., Song, Y., and Roth, D. 2020. Transomcs: From linguistic graphs to commonsense knowledge. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI). pp. 2020\nZhang, T., Kishore, V., Wu, F., Kilian, Q., Weinberger, Y., and Artzi 2019. Bertscore: Evaluating text generation with bert. In Bertscore: Evaluating text generation with bert. arXiv:1904.09675\nZhao, X., Wu, W., Xu, C., Tao, C., Zhao, D., and Yan, R. 2020. Knowledgegrounded dialogue generation with pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 3377--3390 10.18653/v1/2020.emnlp-main.272"
    },
    {
        "filename": "paper_1.txt",
        "start": 473,
        "end": 486,
        "label": "Unsupported Claim",
        "text": "recent works",
        "full_text": "Related Work\n\nLarge-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Sch\u00fctze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge.\n\nIntroducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets. Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation. Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation. Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet. Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.\n\nThere are various types of knowledge resources that can be used to enhance language models, focusing on different aspects. For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge base such UMLS (Bodenreider, 2004) and OHAMA 1 . We use ConceptNet for commonsense and decide to collect a counseling knowledge base as the medical knowledge bases have a limited amount of knowledge align with our need.\n\n References: \nBanerjee, S. and Lavie, A. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. pp. 65--72\nBodenreider, O. 2004. The unified medical language system (umls): integrating biomedical terminology. In Nucleic acids research. pp. D267--D270\nBosselut, A., Rashkin, H., Sap, M., Malaviya, C., Celikyilmaz, A., and Choi, Y. 2019. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 4762--4779 10.18653/v1/P19-1470\nChakrabarty, T., Ghosh, D., Muresan, S., and Peng, N. 2020. R\u02c63: Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7976--7986 10.18653/v1/2020.acl-main.711\nDo, N. and Pavlick, E. 2021. Are rotten apples edible? challenging commonsense inference ability with exceptions. In FINDINGS.\nHendrickx, I., Kim, S. N., Kozareva, Z., Nakov, P., Diarmuid, '., O S'eaghdha, S., Pad'o, M., Pennacchiotti, L., Romano, S., and Szpakowicz 2010. SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation. pp. 33--38\nElizabeth M Huffman, D. I., Athanasiadis, N. E., Anton, Lindsay, A., Haskett, D. L., Doster, D., Stefanidis, N. K., and Lee 2021. How resilient is your team? exploring healthcare providers' well-being during the covid-19 pandemic. In The American Journal of Surgery. pp. 277--284\nHwang, J. D., Bhagavatula, C., Ronan Le Bras, J., Da, K., Sakaguchi, A., Bosselut, Y., and Choi 2020. Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs. In ArXiv. abs/2010.05953\nJi, H., Ke, P., Huang, S., Wei, F., Zhu, X., and Huang, M. 2020. Language generation with multi-hop reasoning on commonsense knowledge graph. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 725--736 10.18653/v1/2020.emnlp-main.54\nKassner, N. and Sch\u00fctze, H. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In ACL.\nKatz, N. and Mcnulty, K. 1994. Reflective listening. In Reflective listening.\nKitaev, N. and Klein, D. 2018. Constituency parsing with a self-attentive encoder. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. pp. 2676--2686 10.18653/v1/P18-1249\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 1910. Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs. In Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. ArXiv, abs.\nLi, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 110--119 10.18653/v1/N16-1014\nLin, C. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out. pp. 74--81\nLiu, W., Zhou, P., Zhao, Z., Wang, Z., Ju, Q., Deng, H., and Wang, P. 2020. K-bert: Enabling language representation with knowledge graph. In ArXiv. abs/1909.07606\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nHuanru Henry Mao, B. P., Majumder, J., Mcauley, G., and Cottrell 2019. Improving neural story generation by targeted common sense grounding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5988--5993 10.18653/v1/D19-1615\nWilliam, R., Miller, S., and Rollnick 2012. Motivational interviewing: Helping people change. In Motivational interviewing: Helping people change.\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135\nMario R Paredes, V., Apaolaza, C., Fernandez-Robin, P., Hartmann, D., and Ya\u00f1ez-Martinez 2021. The impact of the covid-19 pandemic on subjective mental well-being: The interplay of perceived threat, future anxiety and resilience. In Personality and Individual Differences. pp. 110455\nP\u00e9rez-Rosas, V., Mihalcea, R., Resnicow, K., Singh, S., and An, L. 2016. Building a motivational interviewing dataset. In Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology. pp. 42--51 10.18653/v1/W16-0305\nPetroni, F., Rockt\u00e4schel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. 2019. Language models as knowledge bases? arXiv preprint. In Language models as knowledge bases? arXiv preprint. arXiv:1909.01066\nPetroni, F., Rockt\u00e4schel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. 2019. Association for Computational Linguistics. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2463--2473 10.18653/v1/D19-1250\nRadford, A. and Narasimhan, K. 2018. Improving language understanding by generative pre-training. In Improving language understanding by generative pre-training.\nReimers, N. and Gurevych, I. 1908. Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs. In Sentencebert: Sentence embeddings using siamese bertnetworks. ArXiv, abs.\nReimers, N. and Gurevych, I. 2019. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 3982--3992 10.18653/v1/D19-1410\nRen, P., Chen, Z., Monz, C., Ma, J., and De Rijke, M. 2020. Thinking globally, acting locally: Distantly supervised global-to-local knowledge selection for background based conversation. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 8697--8704\nSap, M., Ronan Le Bras, E., Allaway, C., Bhagavatula, N., Lourie, H., Rashkin, B., Roof, Noah, A., Smith, Y., and Choi 2019. Atomic: An atlas of machine commonsense for ifthen reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 3027--3035\nShwartz, V., West, P., Ronan, L., Bras, C., Bhagavatula, Y., and Choi 2020. Unsupervised commonsense question answering with self-talk. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 4615--4629 10.18653/v1/2020.emnlp-main.373\nSong, H., Zhang, W., Cui, Y., Wang, D., and Liu, T. 2019. Exploiting persona information for diverse generation of conversational responses. In IJCAI.\nSpeer, R., Chin, J., and Havasi, C. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence.\nWolf, T., Sanh, V., Chaumond, J., and Delangue, C. 2019. Transfertransfo: A transfer learning approach for neural network based conversational agents. In Transfertransfo: A transfer learning approach for neural network based conversational agents. arXiv:1901.08149\nZhang, H., Khashabi, D., Song, Y., and Roth, D. 2020. Transomcs: From linguistic graphs to commonsense knowledge. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI). pp. 2020\nZhang, T., Kishore, V., Wu, F., Kilian, Q., Weinberger, Y., and Artzi 2019. Bertscore: Evaluating text generation with bert. In Bertscore: Evaluating text generation with bert. arXiv:1904.09675\nZhao, X., Wu, W., Xu, C., Tao, C., Zhao, D., and Yan, R. 2020. Knowledgegrounded dialogue generation with pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 3377--3390 10.18653/v1/2020.emnlp-main.272"
    }
]