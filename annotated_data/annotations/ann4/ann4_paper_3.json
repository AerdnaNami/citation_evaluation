[
    {
        "filename": "paper_3.txt",
        "start": 789,
        "end": 801,
        "label": "Unsupported Claim",
        "text": "most of them",
        "full_text": "Introduction\n\nStarting with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al., 2019; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020). Following the pre-training techniques in NLP, self-supervised speech representation learning has also been investigated and shown promising results, benefiting from richly learned representations (Chung and Glass, 2018;Chuang et al., 2019;Song et al., 2019;Baevski et al., 2020;Hsu et al., 2021;Chung et al., 2021a), such as wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021).\n\nHowever, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks. How to design a unified encoder-decoder model that can take advantage of both unlabeled speech and text data to improve various spoken language processing tasks is not well explored.\n\nInspired by the T5 method (Raffel et al., 2019), we attempt to formulate each spoken language processing task as a speech/text to speech/text problem via an encoder-decoder framework, which enables us to use the same pre-trained model with bimodal data across diverse tasks, as shown in Figure 1. To achieve this, we propose a unified-modal pre-training framework, SpeechT5, containing an encoder-decoder backbone network and modal-specific pre/post-nets. With the pre-nets, the input speech/text is embedded in a shared space, and the encoder-decoder backbone network models the sequence-to-sequence conversion, from which the model-specific post-nets generate the speech/text output. Particularly, SpeechT5 is mainly pre-trained with a denoising sequence-tosequence method by leveraging large-scale unlabeled text and speech corpus. To align the textual and acoustic information into a unified semantic space, the proposed SpeechT5 model ( 1) maps text and speech representations into a shared vector quantization space, and (2) randomly mixes up the quantized latent representations and the contextual states, which can better guide the quantizer to learn the cross-modal features.\n\nWe fine-tune SpeechT5 on a wide variety of downstream spoken language processing tasks, including automatic speech recognition (ASR), textto-speech (TTS), speech translation (ST), voice conversion (VC), speech enhancement (SE), and speaker identification (SID). Massive experiments show that the proposed SpeechT5 model achieves a significant improvement on these spoken language processing tasks compared with the state-of-theart baselines. Specifically, the proposed SpeechT5 outperforms wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021) with the BASE model on the ASR task and also performs better than the state-of-the-art voice Transformer network  on the VC task. Besides, SpeechT5 is significantly superior to SpeechNet (Chen et al., 2021b) and pre-trained models from SUPERB (Yang et al., 2021) and achieves the stateof-the-art performance (i.e., 96.49%) on the SID task. We further provide an empirical comparison of the pre-training tasks and modules, and the ablation study demonstrates the effectiveness of the proposed joint speech-text pre-training method.\n\nThe contributions of this paper are summarized as follows.\n\n\u2022 To the best of our knowledge, this is the first work to investigate a unified encoder-decoder framework for various spoken language processing tasks.\n\n\u2022 We propose a cross-modal vector quantization approach, which learns the implicit alignment between acoustic and textual represen-tation with large-scale unlabeled speech and text data.\n\n\u2022 Extensive experiments on spoken language processing tasks demonstrate the effectiveness and superiority of the proposed SpeechT5 model.\n\n References: \nBaevski, A., Auli, M., and Mohamed, A. 2019. Effectiveness of self-supervised pretraining for speech recognition. In Effectiveness of self-supervised pretraining for speech recognition. arXiv:1911.03912\nBaevski, A., Zhou, Y., Mohamed, A., and Auli, M. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Proceedings of the 34th Conference on Neural Information Processing Systems. pp. 12449--12460\nChen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., Zhou, L., Ren, S., Qian, Y., Qian, Y., Wu, J., Zeng, M., and Wei, F. 2021. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. In Wavlm: Large-scale self-supervised pre-training for full stack speech processing. arXiv:2110.13900\nChen, Y., Chi, P., Yang, S., Chang, K., Lin, J., Huang, S., Liu, D., and Liu, C. Cheng-Kuang Lee, and Hungyi Lee. 2021b. Speechnet: A universal modularized model for speech processing tasks. In Cheng-Kuang Lee, and Hungyi Lee. 2021b. Speechnet: A universal modularized model for speech processing tasks. arXiv:2105.03070\nChen, Z., Zhang, Y., Rosenberg, A., Ramabhadran, B., Wang, G., and Moreno, P. 2021. Injecting text in self-supervised speech pretraining. In Injecting text in self-supervised speech pretraining. arXiv:2108.12226\nChuang, Y., Liu, C., Lee, H., and Lee, L. 2019. Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering. In Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering. arXiv:1910.11559\nChung, J.S., Huh, J., and Mun, S. 2020. Delving into VoxCeleb: Environment invariant speaker recognition. In Proceedings of Odyssey. pp. 349--356\nChung, Y. and Glass, J. 2018. Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech. In Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech. arXiv:1803.08976\nChung, Y., Zhang, Y., Han, W., Chiu, C., Qin, J., Pang, R., and Wu, Y. 2021. W2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training. In W2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training. arXiv:2108.06209\nChung, Y., Zhu, C., and Zeng, M. 2021. SPLAT: Speech-language joint pre-training for spoken language understanding. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1897--1907 10.18653/v1/2021.naacl-main.152\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423\nMattia, A. D., Gangi, R., Cattoni, L., Bentivogli, M., Negri, M., and Turchi 2019. MuST-C: a Multilingual Speech Translation Corpus. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2012--2017 10.18653/v1/N19-1202\nDong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y., Gao, J., Zhou, M., and Hon, H. 2019. Unified language model pre-training for natural language understanding and generation. In Proceedings of the 33rd Conference on Neural Information Processing Systems. pp. 13063--13075\nGulcehre, C., Firat, O., Xu, K., Cho, K., Barrault, L., Lin, H., Bougares, F., Schwenk, H., and Bengio, Y. 2015. On using monolingual corpora in neural machine translation. In On using monolingual corpora in neural machine translation. arXiv:1503.03535\nHori, T., Watanabe, S., and Hershey, J. 2017. Joint CTC/attention decoding for end-to-end speech recognition. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 518--529 10.18653/v1/P17-1048\nHsu, W., Bolte, B., Tsai, Y.H., and Lakhotia, K. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. In Hubert: Self-supervised speech representation learning by masked prediction of hidden units. arXiv:2106.07447\nHuang, W., Hayashi, T., Wu, Y., Kameoka, H., and Toda, T. 2021. Pretraining techniques for sequence-to-sequence voice conversion. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 745--755 10.1109/TASLP.2021.3049336\nInaguma, H., Kiyono, S., Duh, K., Karita, S., NelsonEnrique, Soplin, Y., Hayashi, T., and Watanabe, S. 2020. Espnet-st: Allin-one speech translation toolkit. In Espnet-st: Allin-one speech translation toolkit. arXiv:2004.10234\nJensen, J. and Taal, C. H. 2016. An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers. In IEEE/ACM Transactions on Audio Speech and Language Processing. pp. 2009--2022 10.1109/TASLP.2016.2585878\nKameoka, H., Huang, W., Tanaka, K., Kaneko, T., Hojo, N., and Toda, T. 2021. Many-to-many voice transformer network. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 656--670 10.1109/TASLP.2020.3047262\nKharitonov, E., Lee, A., Polyak, A., Adi, Y., Copet, J., Lakhotia, K., Nguyen, T., Rivi\u00e8re, M., Mohamed, A., and Dupoux, E. 2021. Text-free prosody-aware generative spoken language modeling. In Text-free prosody-aware generative spoken language modeling. arXiv:2109.03264\nKim, M., Kim, G., Lee, S., and Ha, J. 2021. St-bert: Cross-modal language model pre-training for end-to-end spoken language understanding. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 7478--7482 10.1109/ICASSP39728.2021.9414558\nDiederik, P., Kingma, J., and Ba 2014. Adam: A method for stochastic optimization. In Adam: A method for stochastic optimization. arXiv:1412.6980\nKominek, J. and Black, A. W. 2004. The cmu arctic speech databases. In Proceedings of the Fifth ISCA workshop on speech synthesis.\nKong, J., Kim, J., and Bae, J. 2020. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. In Proceedings of the 34th Conference on Neural Information Processing Systems. pp. 17022--17033\nLakhotia, K., Kharitonov, E., Hsu, W., Adi, Y., Polyak, A., Bolte, B., Nguyen, T., Copet, J., and Baevski, A. Generative spoken language modeling from raw audio. In Generative spoken language modeling from raw audio. arXiv:2102.01192\nLample, G. and Conneau, A. 2019. Crosslingual language model pretraining. In Crosslingual language model pretraining. arXiv:1901.07291\nLe, H., Pino, J., Wang, C., Gu, J., Schwab, D., and Besacier, L. 2021. Lightweight adapter tuning for multilingual speech translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 817--824 10.18653/v1/2021.acl-short.103\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703\nLi, N., Liu, S., Liu, Y., Zhao, S., and Liu, M. 2019. Neural speech synthesis with transformer network. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 6706--6713 10.1609/aaai.v33i01.33016706\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nMittag, G. and M\u00f6ller, S. 2020. Deep learning based assessment of synthetic speech naturalness. In Proceedings of the 2020 Interspeech. pp. 1748--1752\nNagrani, A. 2017. Joon Son Chung, and Andrew Zisserman. In Voxceleb: A large-scale speaker identification dataset. arXiv:1706.08612\nVan Den Oord, A., Vinyals, O., and Kavukcuoglu, K. 2017. Neural discrete representation learning. In Neural discrete representation learning. arXiv:1711.00937\nOtt, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In fairseq: A fast, extensible toolkit for sequence modeling. arXiv:1904.01038\nPanayotov, V., Chen, G., Povey, D., and Khudanpur, S. 2015. Librispeech: an asr corpus based on public domain audio books. In Proceedings of the 2015 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 5206--5210 10.1109/ICASSP.2015.7178964\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2227--2237 10.18653/v1/N18-1202\nQian, Y., Bianv, X., Shi, Y., Kanda, N., Shen, L., Xiao, Z., and Zeng, M. 2021. Speech-language pre-training for end-to-end spoken language understanding. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 7458--7462 10.1109/ICASSP39728.2021.9414900\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. 2019. Language models are unsupervised multitask learners. In OpenAI blog. pp. 9\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.J. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. In Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv:1910.10683\nChandan, K. A., Reddy, H., Dubey, V., Gopal, R., Cutler, S., and Braun Hannes Gamper, Robert Aichner, and Sriram Srinivasan. 2021. ICASSP 2021 deep noise suppression challenge. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 6623--6627 10.1109/ICASSP39728.2021.9415105\nShuo Ren, L., Zhou, S., Liu, F., Wei, M., Zhou, S., and Ma 2021. Semface: Pre-training encoder and decoder with a semantic interface for neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 4518--4527 10.18653/v1/2021.acl-long.348\nRix, A. W., Beerends, J. G., Hollier, M. P., and AP 2001. Perceptual evaluation of speech quality (PESQ) -A new method for speech quality assessment of telephone networks and codecs. In Proceedings of the 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. pp. 749--752 10.1109/ICASSP.2001.941023\nSebastian, B. and Ivan, T. 2020. Data augmentation and loss normalization for deep noise suppression. In Proceedings of Speech and Computer. pp. 79--86\nShaw, P., Uszkoreit, J., and Vaswani, A. 2018. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 464--468 10.18653/v1/N18-2074\nShen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y., Wang, Y., Skerrv-Ryan, R., Saurous, R. A., Agiomvrgiannakis, Y., and Wu, Y. 2018. Natural tts synthesis by conditioning wavenet on mel spectrogram predictions. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 4779--4783 10.1109/ICASSP.2018.8461368\nSnyder, D., Garcia-Romero, D., Sell, G., Povey, D., and Khudanpur, S. 2018. Xvectors: Robust DNN embeddings for speaker recognition. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 5329--5333 10.1109/ICASSP.2018.8461375\nSong, X., Wang, G., Wu, Z., Huang, Y., Su, D., Yu, D., and Meng, H. 2019. Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks. In Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks. arXiv:1910.10387\nSynnaeve, G., Xu, Q., Kahn, J., Likhomanenko, T., Grave, E., Pratap, V., Sriram, A., Liptchinsky, V., and Collobert, R. 2020. End-to-end asr: from supervised to semi-supervised learning with modern architectures. In End-to-end asr: from supervised to semi-supervised learning with modern architectures. arXiv:1911.08460\nTachibana, H., Uenoyama, K., and Aihara, S. 2018. Efficiently trainable text-tospeech system based on deep convolutional networks with guided attention. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 4784--4788 10.1109/ICASSP.2018.8461829\nTjandra, A., Sakti, S., and Nakamura, S. 2020. Machine speech chain. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 976--989 10.1109/TASLP.2020.2977776\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. 2017. Attention is all you need. In Proceedings of the 31st Conference on Neural Information Processing Systems. pp. 6000--6010 10.5555/3295222.3295349\nWang, C., Tang, Y., Ma, X., Wu, A., Okhonko, D., and Pino, J. 2020. Fairseq s2t: Fast speech-to-text modeling with fairseq. In Fairseq s2t: Fast speech-to-text modeling with fairseq. arXiv:2010.05171\nWang, C., Wu, Y., Qian, Y., Kumatani, K., Liu, S., Wei, F., Zeng, M., and Huang, X. 2021. Unispeech: Unified speech representation learning with labeled and unlabeled data. In Unispeech: Unified speech representation learning with labeled and unlabeled data. arXiv:2101.07597\nWatanabe, S., Hori, T., Karita, S., Hayashi, T., Nishitoba, J., Unno, Y., NelsonEnrique, Soplin, Y., Heymann, J., Wiesner, M., and Chen, N. 2018. Espnet: Endto-end speech processing toolkit. In Espnet: Endto-end speech processing toolkit. arXiv:1804.00015\nWatanabe, S., Hori, T., Kim, S., Hershey, J. R., and Hayashi, T. 2017. Hybrid ctc/attention architecture for end-to-end speech recognition. In IEEE Journal of Selected Topics in Signal Processing. pp. 1240--1253 10.1109/JSTSP.2017.2763455\nWichern, G., Antognini, J., Flynn, M., Zhu, L. R., Mcquinn, E., Crow, D., Manilow, E., and Roux, J. L. 2019. WHAM!: Extending speech separation to noisy environments. In WHAM!: Extending speech separation to noisy environments. arXiv:1907.01160\nYamamoto, R., Song, E., and Kim, J. 2020. Parallel Wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram. In Proceedings of the 2020 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 6199--6203 10.1109/ICASSP40776.2020.9053795\nShu-Wen Yang, P., Chi, Y., Chuang, Cheng, -.J., Lai, K., Lakhotia, Yist, Y., Lin, A. T., Liu, J., Shi, X., Chang, G., and Lin 2021. Superb: Speech processing universal performance benchmark. In Superb: Speech processing universal performance benchmark. arXiv:2105.01051"
    },
    {
        "filename": "paper_3.txt",
        "start": 1017,
        "end": 1037,
        "label": "Unsupported Claim",
        "text": "most of these models",
        "full_text": "Introduction\n\nStarting with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al., 2019; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020). Following the pre-training techniques in NLP, self-supervised speech representation learning has also been investigated and shown promising results, benefiting from richly learned representations (Chung and Glass, 2018;Chuang et al., 2019;Song et al., 2019;Baevski et al., 2020;Hsu et al., 2021;Chung et al., 2021a), such as wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021).\n\nHowever, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks. How to design a unified encoder-decoder model that can take advantage of both unlabeled speech and text data to improve various spoken language processing tasks is not well explored.\n\nInspired by the T5 method (Raffel et al., 2019), we attempt to formulate each spoken language processing task as a speech/text to speech/text problem via an encoder-decoder framework, which enables us to use the same pre-trained model with bimodal data across diverse tasks, as shown in Figure 1. To achieve this, we propose a unified-modal pre-training framework, SpeechT5, containing an encoder-decoder backbone network and modal-specific pre/post-nets. With the pre-nets, the input speech/text is embedded in a shared space, and the encoder-decoder backbone network models the sequence-to-sequence conversion, from which the model-specific post-nets generate the speech/text output. Particularly, SpeechT5 is mainly pre-trained with a denoising sequence-tosequence method by leveraging large-scale unlabeled text and speech corpus. To align the textual and acoustic information into a unified semantic space, the proposed SpeechT5 model ( 1) maps text and speech representations into a shared vector quantization space, and (2) randomly mixes up the quantized latent representations and the contextual states, which can better guide the quantizer to learn the cross-modal features.\n\nWe fine-tune SpeechT5 on a wide variety of downstream spoken language processing tasks, including automatic speech recognition (ASR), textto-speech (TTS), speech translation (ST), voice conversion (VC), speech enhancement (SE), and speaker identification (SID). Massive experiments show that the proposed SpeechT5 model achieves a significant improvement on these spoken language processing tasks compared with the state-of-theart baselines. Specifically, the proposed SpeechT5 outperforms wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021) with the BASE model on the ASR task and also performs better than the state-of-the-art voice Transformer network  on the VC task. Besides, SpeechT5 is significantly superior to SpeechNet (Chen et al., 2021b) and pre-trained models from SUPERB (Yang et al., 2021) and achieves the stateof-the-art performance (i.e., 96.49%) on the SID task. We further provide an empirical comparison of the pre-training tasks and modules, and the ablation study demonstrates the effectiveness of the proposed joint speech-text pre-training method.\n\nThe contributions of this paper are summarized as follows.\n\n\u2022 To the best of our knowledge, this is the first work to investigate a unified encoder-decoder framework for various spoken language processing tasks.\n\n\u2022 We propose a cross-modal vector quantization approach, which learns the implicit alignment between acoustic and textual represen-tation with large-scale unlabeled speech and text data.\n\n\u2022 Extensive experiments on spoken language processing tasks demonstrate the effectiveness and superiority of the proposed SpeechT5 model.\n\n References: \nBaevski, A., Auli, M., and Mohamed, A. 2019. Effectiveness of self-supervised pretraining for speech recognition. In Effectiveness of self-supervised pretraining for speech recognition. arXiv:1911.03912\nBaevski, A., Zhou, Y., Mohamed, A., and Auli, M. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Proceedings of the 34th Conference on Neural Information Processing Systems. pp. 12449--12460\nChen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., Zhou, L., Ren, S., Qian, Y., Qian, Y., Wu, J., Zeng, M., and Wei, F. 2021. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. In Wavlm: Large-scale self-supervised pre-training for full stack speech processing. arXiv:2110.13900\nChen, Y., Chi, P., Yang, S., Chang, K., Lin, J., Huang, S., Liu, D., and Liu, C. Cheng-Kuang Lee, and Hungyi Lee. 2021b. Speechnet: A universal modularized model for speech processing tasks. In Cheng-Kuang Lee, and Hungyi Lee. 2021b. Speechnet: A universal modularized model for speech processing tasks. arXiv:2105.03070\nChen, Z., Zhang, Y., Rosenberg, A., Ramabhadran, B., Wang, G., and Moreno, P. 2021. Injecting text in self-supervised speech pretraining. In Injecting text in self-supervised speech pretraining. arXiv:2108.12226\nChuang, Y., Liu, C., Lee, H., and Lee, L. 2019. Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering. In Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering. arXiv:1910.11559\nChung, J.S., Huh, J., and Mun, S. 2020. Delving into VoxCeleb: Environment invariant speaker recognition. In Proceedings of Odyssey. pp. 349--356\nChung, Y. and Glass, J. 2018. Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech. In Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech. arXiv:1803.08976\nChung, Y., Zhang, Y., Han, W., Chiu, C., Qin, J., Pang, R., and Wu, Y. 2021. W2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training. In W2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training. arXiv:2108.06209\nChung, Y., Zhu, C., and Zeng, M. 2021. SPLAT: Speech-language joint pre-training for spoken language understanding. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1897--1907 10.18653/v1/2021.naacl-main.152\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423\nMattia, A. D., Gangi, R., Cattoni, L., Bentivogli, M., Negri, M., and Turchi 2019. MuST-C: a Multilingual Speech Translation Corpus. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2012--2017 10.18653/v1/N19-1202\nDong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y., Gao, J., Zhou, M., and Hon, H. 2019. Unified language model pre-training for natural language understanding and generation. In Proceedings of the 33rd Conference on Neural Information Processing Systems. pp. 13063--13075\nGulcehre, C., Firat, O., Xu, K., Cho, K., Barrault, L., Lin, H., Bougares, F., Schwenk, H., and Bengio, Y. 2015. On using monolingual corpora in neural machine translation. In On using monolingual corpora in neural machine translation. arXiv:1503.03535\nHori, T., Watanabe, S., and Hershey, J. 2017. Joint CTC/attention decoding for end-to-end speech recognition. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 518--529 10.18653/v1/P17-1048\nHsu, W., Bolte, B., Tsai, Y.H., and Lakhotia, K. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. In Hubert: Self-supervised speech representation learning by masked prediction of hidden units. arXiv:2106.07447\nHuang, W., Hayashi, T., Wu, Y., Kameoka, H., and Toda, T. 2021. Pretraining techniques for sequence-to-sequence voice conversion. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 745--755 10.1109/TASLP.2021.3049336\nInaguma, H., Kiyono, S., Duh, K., Karita, S., NelsonEnrique, Soplin, Y., Hayashi, T., and Watanabe, S. 2020. Espnet-st: Allin-one speech translation toolkit. In Espnet-st: Allin-one speech translation toolkit. arXiv:2004.10234\nJensen, J. and Taal, C. H. 2016. An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers. In IEEE/ACM Transactions on Audio Speech and Language Processing. pp. 2009--2022 10.1109/TASLP.2016.2585878\nKameoka, H., Huang, W., Tanaka, K., Kaneko, T., Hojo, N., and Toda, T. 2021. Many-to-many voice transformer network. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 656--670 10.1109/TASLP.2020.3047262\nKharitonov, E., Lee, A., Polyak, A., Adi, Y., Copet, J., Lakhotia, K., Nguyen, T., Rivi\u00e8re, M., Mohamed, A., and Dupoux, E. 2021. Text-free prosody-aware generative spoken language modeling. In Text-free prosody-aware generative spoken language modeling. arXiv:2109.03264\nKim, M., Kim, G., Lee, S., and Ha, J. 2021. St-bert: Cross-modal language model pre-training for end-to-end spoken language understanding. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 7478--7482 10.1109/ICASSP39728.2021.9414558\nDiederik, P., Kingma, J., and Ba 2014. Adam: A method for stochastic optimization. In Adam: A method for stochastic optimization. arXiv:1412.6980\nKominek, J. and Black, A. W. 2004. The cmu arctic speech databases. In Proceedings of the Fifth ISCA workshop on speech synthesis.\nKong, J., Kim, J., and Bae, J. 2020. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. In Proceedings of the 34th Conference on Neural Information Processing Systems. pp. 17022--17033\nLakhotia, K., Kharitonov, E., Hsu, W., Adi, Y., Polyak, A., Bolte, B., Nguyen, T., Copet, J., and Baevski, A. Generative spoken language modeling from raw audio. In Generative spoken language modeling from raw audio. arXiv:2102.01192\nLample, G. and Conneau, A. 2019. Crosslingual language model pretraining. In Crosslingual language model pretraining. arXiv:1901.07291\nLe, H., Pino, J., Wang, C., Gu, J., Schwab, D., and Besacier, L. 2021. Lightweight adapter tuning for multilingual speech translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 817--824 10.18653/v1/2021.acl-short.103\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703\nLi, N., Liu, S., Liu, Y., Zhao, S., and Liu, M. 2019. Neural speech synthesis with transformer network. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 6706--6713 10.1609/aaai.v33i01.33016706\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nMittag, G. and M\u00f6ller, S. 2020. Deep learning based assessment of synthetic speech naturalness. In Proceedings of the 2020 Interspeech. pp. 1748--1752\nNagrani, A. 2017. Joon Son Chung, and Andrew Zisserman. In Voxceleb: A large-scale speaker identification dataset. arXiv:1706.08612\nVan Den Oord, A., Vinyals, O., and Kavukcuoglu, K. 2017. Neural discrete representation learning. In Neural discrete representation learning. arXiv:1711.00937\nOtt, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In fairseq: A fast, extensible toolkit for sequence modeling. arXiv:1904.01038\nPanayotov, V., Chen, G., Povey, D., and Khudanpur, S. 2015. Librispeech: an asr corpus based on public domain audio books. In Proceedings of the 2015 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 5206--5210 10.1109/ICASSP.2015.7178964\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2227--2237 10.18653/v1/N18-1202\nQian, Y., Bianv, X., Shi, Y., Kanda, N., Shen, L., Xiao, Z., and Zeng, M. 2021. Speech-language pre-training for end-to-end spoken language understanding. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 7458--7462 10.1109/ICASSP39728.2021.9414900\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. 2019. Language models are unsupervised multitask learners. In OpenAI blog. pp. 9\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.J. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. In Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv:1910.10683\nChandan, K. A., Reddy, H., Dubey, V., Gopal, R., Cutler, S., and Braun Hannes Gamper, Robert Aichner, and Sriram Srinivasan. 2021. ICASSP 2021 deep noise suppression challenge. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 6623--6627 10.1109/ICASSP39728.2021.9415105\nShuo Ren, L., Zhou, S., Liu, F., Wei, M., Zhou, S., and Ma 2021. Semface: Pre-training encoder and decoder with a semantic interface for neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 4518--4527 10.18653/v1/2021.acl-long.348\nRix, A. W., Beerends, J. G., Hollier, M. P., and AP 2001. Perceptual evaluation of speech quality (PESQ) -A new method for speech quality assessment of telephone networks and codecs. In Proceedings of the 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. pp. 749--752 10.1109/ICASSP.2001.941023\nSebastian, B. and Ivan, T. 2020. Data augmentation and loss normalization for deep noise suppression. In Proceedings of Speech and Computer. pp. 79--86\nShaw, P., Uszkoreit, J., and Vaswani, A. 2018. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 464--468 10.18653/v1/N18-2074\nShen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y., Wang, Y., Skerrv-Ryan, R., Saurous, R. A., Agiomvrgiannakis, Y., and Wu, Y. 2018. Natural tts synthesis by conditioning wavenet on mel spectrogram predictions. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 4779--4783 10.1109/ICASSP.2018.8461368\nSnyder, D., Garcia-Romero, D., Sell, G., Povey, D., and Khudanpur, S. 2018. Xvectors: Robust DNN embeddings for speaker recognition. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 5329--5333 10.1109/ICASSP.2018.8461375\nSong, X., Wang, G., Wu, Z., Huang, Y., Su, D., Yu, D., and Meng, H. 2019. Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks. In Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks. arXiv:1910.10387\nSynnaeve, G., Xu, Q., Kahn, J., Likhomanenko, T., Grave, E., Pratap, V., Sriram, A., Liptchinsky, V., and Collobert, R. 2020. End-to-end asr: from supervised to semi-supervised learning with modern architectures. In End-to-end asr: from supervised to semi-supervised learning with modern architectures. arXiv:1911.08460\nTachibana, H., Uenoyama, K., and Aihara, S. 2018. Efficiently trainable text-tospeech system based on deep convolutional networks with guided attention. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 4784--4788 10.1109/ICASSP.2018.8461829\nTjandra, A., Sakti, S., and Nakamura, S. 2020. Machine speech chain. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 976--989 10.1109/TASLP.2020.2977776\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. 2017. Attention is all you need. In Proceedings of the 31st Conference on Neural Information Processing Systems. pp. 6000--6010 10.5555/3295222.3295349\nWang, C., Tang, Y., Ma, X., Wu, A., Okhonko, D., and Pino, J. 2020. Fairseq s2t: Fast speech-to-text modeling with fairseq. In Fairseq s2t: Fast speech-to-text modeling with fairseq. arXiv:2010.05171\nWang, C., Wu, Y., Qian, Y., Kumatani, K., Liu, S., Wei, F., Zeng, M., and Huang, X. 2021. Unispeech: Unified speech representation learning with labeled and unlabeled data. In Unispeech: Unified speech representation learning with labeled and unlabeled data. arXiv:2101.07597\nWatanabe, S., Hori, T., Karita, S., Hayashi, T., Nishitoba, J., Unno, Y., NelsonEnrique, Soplin, Y., Heymann, J., Wiesner, M., and Chen, N. 2018. Espnet: Endto-end speech processing toolkit. In Espnet: Endto-end speech processing toolkit. arXiv:1804.00015\nWatanabe, S., Hori, T., Kim, S., Hershey, J. R., and Hayashi, T. 2017. Hybrid ctc/attention architecture for end-to-end speech recognition. In IEEE Journal of Selected Topics in Signal Processing. pp. 1240--1253 10.1109/JSTSP.2017.2763455\nWichern, G., Antognini, J., Flynn, M., Zhu, L. R., Mcquinn, E., Crow, D., Manilow, E., and Roux, J. L. 2019. WHAM!: Extending speech separation to noisy environments. In WHAM!: Extending speech separation to noisy environments. arXiv:1907.01160\nYamamoto, R., Song, E., and Kim, J. 2020. Parallel Wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram. In Proceedings of the 2020 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 6199--6203 10.1109/ICASSP40776.2020.9053795\nShu-Wen Yang, P., Chi, Y., Chuang, Cheng, -.J., Lai, K., Lakhotia, Yist, Y., Lin, A. T., Liu, J., Shi, X., Chang, G., and Lin 2021. Superb: Speech processing universal performance benchmark. In Superb: Speech processing universal performance benchmark. arXiv:2105.01051"
    },
    {
        "filename": "paper_3.txt",
        "start": 1359,
        "end": 1377,
        "label": "Unsupported Claim",
        "text": "not well explored.",
        "full_text": "Introduction\n\nStarting with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), substantial work has shown that pre-trained models can significantly improve in various natural language processing (NLP) tasks (Radford et al., 2019; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Lewis et al., 2020). Following the pre-training techniques in NLP, self-supervised speech representation learning has also been investigated and shown promising results, benefiting from richly learned representations (Chung and Glass, 2018;Chuang et al., 2019;Song et al., 2019;Baevski et al., 2020;Hsu et al., 2021;Chung et al., 2021a), such as wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021).\n\nHowever, previous speech pre-training work suffers from two problems: (1) most of them learn the speech representation with only unlabeled speech data but ignore the importance of textual data to spoken language tasks (e.g., automatic speech recognition) which require the modality transformation; (2) most of these models solely rely on a pre-trained speech encoder for various downstream tasks, leaving the decoder not pre-trained for the sequence-to-sequence generation tasks. How to design a unified encoder-decoder model that can take advantage of both unlabeled speech and text data to improve various spoken language processing tasks is not well explored.\n\nInspired by the T5 method (Raffel et al., 2019), we attempt to formulate each spoken language processing task as a speech/text to speech/text problem via an encoder-decoder framework, which enables us to use the same pre-trained model with bimodal data across diverse tasks, as shown in Figure 1. To achieve this, we propose a unified-modal pre-training framework, SpeechT5, containing an encoder-decoder backbone network and modal-specific pre/post-nets. With the pre-nets, the input speech/text is embedded in a shared space, and the encoder-decoder backbone network models the sequence-to-sequence conversion, from which the model-specific post-nets generate the speech/text output. Particularly, SpeechT5 is mainly pre-trained with a denoising sequence-tosequence method by leveraging large-scale unlabeled text and speech corpus. To align the textual and acoustic information into a unified semantic space, the proposed SpeechT5 model ( 1) maps text and speech representations into a shared vector quantization space, and (2) randomly mixes up the quantized latent representations and the contextual states, which can better guide the quantizer to learn the cross-modal features.\n\nWe fine-tune SpeechT5 on a wide variety of downstream spoken language processing tasks, including automatic speech recognition (ASR), textto-speech (TTS), speech translation (ST), voice conversion (VC), speech enhancement (SE), and speaker identification (SID). Massive experiments show that the proposed SpeechT5 model achieves a significant improvement on these spoken language processing tasks compared with the state-of-theart baselines. Specifically, the proposed SpeechT5 outperforms wav2vec 2.0 (Baevski et al., 2020) and HuBERT (Hsu et al., 2021) with the BASE model on the ASR task and also performs better than the state-of-the-art voice Transformer network  on the VC task. Besides, SpeechT5 is significantly superior to SpeechNet (Chen et al., 2021b) and pre-trained models from SUPERB (Yang et al., 2021) and achieves the stateof-the-art performance (i.e., 96.49%) on the SID task. We further provide an empirical comparison of the pre-training tasks and modules, and the ablation study demonstrates the effectiveness of the proposed joint speech-text pre-training method.\n\nThe contributions of this paper are summarized as follows.\n\n\u2022 To the best of our knowledge, this is the first work to investigate a unified encoder-decoder framework for various spoken language processing tasks.\n\n\u2022 We propose a cross-modal vector quantization approach, which learns the implicit alignment between acoustic and textual represen-tation with large-scale unlabeled speech and text data.\n\n\u2022 Extensive experiments on spoken language processing tasks demonstrate the effectiveness and superiority of the proposed SpeechT5 model.\n\n References: \nBaevski, A., Auli, M., and Mohamed, A. 2019. Effectiveness of self-supervised pretraining for speech recognition. In Effectiveness of self-supervised pretraining for speech recognition. arXiv:1911.03912\nBaevski, A., Zhou, Y., Mohamed, A., and Auli, M. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Proceedings of the 34th Conference on Neural Information Processing Systems. pp. 12449--12460\nChen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., Zhou, L., Ren, S., Qian, Y., Qian, Y., Wu, J., Zeng, M., and Wei, F. 2021. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. In Wavlm: Large-scale self-supervised pre-training for full stack speech processing. arXiv:2110.13900\nChen, Y., Chi, P., Yang, S., Chang, K., Lin, J., Huang, S., Liu, D., and Liu, C. Cheng-Kuang Lee, and Hungyi Lee. 2021b. Speechnet: A universal modularized model for speech processing tasks. In Cheng-Kuang Lee, and Hungyi Lee. 2021b. Speechnet: A universal modularized model for speech processing tasks. arXiv:2105.03070\nChen, Z., Zhang, Y., Rosenberg, A., Ramabhadran, B., Wang, G., and Moreno, P. 2021. Injecting text in self-supervised speech pretraining. In Injecting text in self-supervised speech pretraining. arXiv:2108.12226\nChuang, Y., Liu, C., Lee, H., and Lee, L. 2019. Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering. In Speechbert: An audioand-text jointly learned language model for endto-end spoken question answering. arXiv:1910.11559\nChung, J.S., Huh, J., and Mun, S. 2020. Delving into VoxCeleb: Environment invariant speaker recognition. In Proceedings of Odyssey. pp. 349--356\nChung, Y. and Glass, J. 2018. Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech. In Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech. arXiv:1803.08976\nChung, Y., Zhang, Y., Han, W., Chiu, C., Qin, J., Pang, R., and Wu, Y. 2021. W2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training. In W2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training. arXiv:2108.06209\nChung, Y., Zhu, C., and Zeng, M. 2021. SPLAT: Speech-language joint pre-training for spoken language understanding. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1897--1907 10.18653/v1/2021.naacl-main.152\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423\nMattia, A. D., Gangi, R., Cattoni, L., Bentivogli, M., Negri, M., and Turchi 2019. MuST-C: a Multilingual Speech Translation Corpus. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2012--2017 10.18653/v1/N19-1202\nDong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y., Gao, J., Zhou, M., and Hon, H. 2019. Unified language model pre-training for natural language understanding and generation. In Proceedings of the 33rd Conference on Neural Information Processing Systems. pp. 13063--13075\nGulcehre, C., Firat, O., Xu, K., Cho, K., Barrault, L., Lin, H., Bougares, F., Schwenk, H., and Bengio, Y. 2015. On using monolingual corpora in neural machine translation. In On using monolingual corpora in neural machine translation. arXiv:1503.03535\nHori, T., Watanabe, S., and Hershey, J. 2017. Joint CTC/attention decoding for end-to-end speech recognition. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. pp. 518--529 10.18653/v1/P17-1048\nHsu, W., Bolte, B., Tsai, Y.H., and Lakhotia, K. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. In Hubert: Self-supervised speech representation learning by masked prediction of hidden units. arXiv:2106.07447\nHuang, W., Hayashi, T., Wu, Y., Kameoka, H., and Toda, T. 2021. Pretraining techniques for sequence-to-sequence voice conversion. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 745--755 10.1109/TASLP.2021.3049336\nInaguma, H., Kiyono, S., Duh, K., Karita, S., NelsonEnrique, Soplin, Y., Hayashi, T., and Watanabe, S. 2020. Espnet-st: Allin-one speech translation toolkit. In Espnet-st: Allin-one speech translation toolkit. arXiv:2004.10234\nJensen, J. and Taal, C. H. 2016. An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers. In IEEE/ACM Transactions on Audio Speech and Language Processing. pp. 2009--2022 10.1109/TASLP.2016.2585878\nKameoka, H., Huang, W., Tanaka, K., Kaneko, T., Hojo, N., and Toda, T. 2021. Many-to-many voice transformer network. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 656--670 10.1109/TASLP.2020.3047262\nKharitonov, E., Lee, A., Polyak, A., Adi, Y., Copet, J., Lakhotia, K., Nguyen, T., Rivi\u00e8re, M., Mohamed, A., and Dupoux, E. 2021. Text-free prosody-aware generative spoken language modeling. In Text-free prosody-aware generative spoken language modeling. arXiv:2109.03264\nKim, M., Kim, G., Lee, S., and Ha, J. 2021. St-bert: Cross-modal language model pre-training for end-to-end spoken language understanding. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 7478--7482 10.1109/ICASSP39728.2021.9414558\nDiederik, P., Kingma, J., and Ba 2014. Adam: A method for stochastic optimization. In Adam: A method for stochastic optimization. arXiv:1412.6980\nKominek, J. and Black, A. W. 2004. The cmu arctic speech databases. In Proceedings of the Fifth ISCA workshop on speech synthesis.\nKong, J., Kim, J., and Bae, J. 2020. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. In Proceedings of the 34th Conference on Neural Information Processing Systems. pp. 17022--17033\nLakhotia, K., Kharitonov, E., Hsu, W., Adi, Y., Polyak, A., Bolte, B., Nguyen, T., Copet, J., and Baevski, A. Generative spoken language modeling from raw audio. In Generative spoken language modeling from raw audio. arXiv:2102.01192\nLample, G. and Conneau, A. 2019. Crosslingual language model pretraining. In Crosslingual language model pretraining. arXiv:1901.07291\nLe, H., Pino, J., Wang, C., Gu, J., Schwab, D., and Besacier, L. 2021. Lightweight adapter tuning for multilingual speech translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 817--824 10.18653/v1/2021.acl-short.103\nLewis, M., Liu, Y., Goyal ; Abdelrahman Mohamed, N., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703\nLi, N., Liu, S., Liu, Y., Zhao, S., and Liu, M. 2019. Neural speech synthesis with transformer network. In Proceedings of the AAAI Conference on Artificial Intelligence. pp. 6706--6713 10.1609/aaai.v33i01.33016706\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nMittag, G. and M\u00f6ller, S. 2020. Deep learning based assessment of synthetic speech naturalness. In Proceedings of the 2020 Interspeech. pp. 1748--1752\nNagrani, A. 2017. Joon Son Chung, and Andrew Zisserman. In Voxceleb: A large-scale speaker identification dataset. arXiv:1706.08612\nVan Den Oord, A., Vinyals, O., and Kavukcuoglu, K. 2017. Neural discrete representation learning. In Neural discrete representation learning. arXiv:1711.00937\nOtt, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In fairseq: A fast, extensible toolkit for sequence modeling. arXiv:1904.01038\nPanayotov, V., Chen, G., Povey, D., and Khudanpur, S. 2015. Librispeech: an asr corpus based on public domain audio books. In Proceedings of the 2015 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 5206--5210 10.1109/ICASSP.2015.7178964\nPapineni, K., Roukos, S., Ward, T., and Zhu, W. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. pp. 311--318 10.3115/1073083.1073135\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 2227--2237 10.18653/v1/N18-1202\nQian, Y., Bianv, X., Shi, Y., Kanda, N., Shen, L., Xiao, Z., and Zeng, M. 2021. Speech-language pre-training for end-to-end spoken language understanding. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 7458--7462 10.1109/ICASSP39728.2021.9414900\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. 2019. Language models are unsupervised multitask learners. In OpenAI blog. pp. 9\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.J. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. In Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv:1910.10683\nChandan, K. A., Reddy, H., Dubey, V., Gopal, R., Cutler, S., and Braun Hannes Gamper, Robert Aichner, and Sriram Srinivasan. 2021. ICASSP 2021 deep noise suppression challenge. In Proceedings of the 2021 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 6623--6627 10.1109/ICASSP39728.2021.9415105\nShuo Ren, L., Zhou, S., Liu, F., Wei, M., Zhou, S., and Ma 2021. Semface: Pre-training encoder and decoder with a semantic interface for neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 4518--4527 10.18653/v1/2021.acl-long.348\nRix, A. W., Beerends, J. G., Hollier, M. P., and AP 2001. Perceptual evaluation of speech quality (PESQ) -A new method for speech quality assessment of telephone networks and codecs. In Proceedings of the 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. pp. 749--752 10.1109/ICASSP.2001.941023\nSebastian, B. and Ivan, T. 2020. Data augmentation and loss normalization for deep noise suppression. In Proceedings of Speech and Computer. pp. 79--86\nShaw, P., Uszkoreit, J., and Vaswani, A. 2018. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 464--468 10.18653/v1/N18-2074\nShen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y., Wang, Y., Skerrv-Ryan, R., Saurous, R. A., Agiomvrgiannakis, Y., and Wu, Y. 2018. Natural tts synthesis by conditioning wavenet on mel spectrogram predictions. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 4779--4783 10.1109/ICASSP.2018.8461368\nSnyder, D., Garcia-Romero, D., Sell, G., Povey, D., and Khudanpur, S. 2018. Xvectors: Robust DNN embeddings for speaker recognition. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 5329--5333 10.1109/ICASSP.2018.8461375\nSong, X., Wang, G., Wu, Z., Huang, Y., Su, D., Yu, D., and Meng, H. 2019. Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks. In Speech-xlnet: Unsupervised acoustic model pretraining for self-attention networks. arXiv:1910.10387\nSynnaeve, G., Xu, Q., Kahn, J., Likhomanenko, T., Grave, E., Pratap, V., Sriram, A., Liptchinsky, V., and Collobert, R. 2020. End-to-end asr: from supervised to semi-supervised learning with modern architectures. In End-to-end asr: from supervised to semi-supervised learning with modern architectures. arXiv:1911.08460\nTachibana, H., Uenoyama, K., and Aihara, S. 2018. Efficiently trainable text-tospeech system based on deep convolutional networks with guided attention. In Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 4784--4788 10.1109/ICASSP.2018.8461829\nTjandra, A., Sakti, S., and Nakamura, S. 2020. Machine speech chain. In IEEE/ACM Transactions on Audio, Speech, and Language Processing. pp. 976--989 10.1109/TASLP.2020.2977776\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. 2017. Attention is all you need. In Proceedings of the 31st Conference on Neural Information Processing Systems. pp. 6000--6010 10.5555/3295222.3295349\nWang, C., Tang, Y., Ma, X., Wu, A., Okhonko, D., and Pino, J. 2020. Fairseq s2t: Fast speech-to-text modeling with fairseq. In Fairseq s2t: Fast speech-to-text modeling with fairseq. arXiv:2010.05171\nWang, C., Wu, Y., Qian, Y., Kumatani, K., Liu, S., Wei, F., Zeng, M., and Huang, X. 2021. Unispeech: Unified speech representation learning with labeled and unlabeled data. In Unispeech: Unified speech representation learning with labeled and unlabeled data. arXiv:2101.07597\nWatanabe, S., Hori, T., Karita, S., Hayashi, T., Nishitoba, J., Unno, Y., NelsonEnrique, Soplin, Y., Heymann, J., Wiesner, M., and Chen, N. 2018. Espnet: Endto-end speech processing toolkit. In Espnet: Endto-end speech processing toolkit. arXiv:1804.00015\nWatanabe, S., Hori, T., Kim, S., Hershey, J. R., and Hayashi, T. 2017. Hybrid ctc/attention architecture for end-to-end speech recognition. In IEEE Journal of Selected Topics in Signal Processing. pp. 1240--1253 10.1109/JSTSP.2017.2763455\nWichern, G., Antognini, J., Flynn, M., Zhu, L. R., Mcquinn, E., Crow, D., Manilow, E., and Roux, J. L. 2019. WHAM!: Extending speech separation to noisy environments. In WHAM!: Extending speech separation to noisy environments. arXiv:1907.01160\nYamamoto, R., Song, E., and Kim, J. 2020. Parallel Wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram. In Proceedings of the 2020 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 6199--6203 10.1109/ICASSP40776.2020.9053795\nShu-Wen Yang, P., Chi, Y., Chuang, Cheng, -.J., Lai, K., Lakhotia, Yist, Y., Lin, A. T., Liu, J., Shi, X., Chang, G., and Lin 2021. Superb: Speech processing universal performance benchmark. In Superb: Speech processing universal performance benchmark. arXiv:2105.01051"
    }
]