[
    {
        "filename": "paper_8.txt",
        "start": 814,
        "end": 1063,
        "label": "Coherence",
        "text": "On the one hand, these metrics are not accessible on some tasks such as regression. On the other hand, In order for these methods to perform well, one usually needs to fine-tune the threshold, which varies widely across different tasks and datasets.",
        "full_text": "Introduction\n\nEarly exiting is a widely used technique to accelerate inference of deep neural networks. With the rising of pre-trained language models (PLMs) (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;Qiu et al., 2020), early exiting is drawing increasing attention in the NLP community. At its core, early exiting allows simple instances to exit early while allowing hard instances to exit late. Thus, how to measure instance difficulty is a crucial problem.\n\nMost existing early exiting methods attach multiple internal classifiers to the PLM and adopt some heuristic metrics, such as entropy  (Xin et al., 2020;Liu et al., 2020a) or maximum softmax score (Schwartz et al., 2020) of internal outputs, to measure instance difficulty. However, these methods can not easily generalize to new tasks. On the one hand, these metrics are not accessible on some tasks such as regression. On the other hand, In order for these methods to perform well, one usually needs to fine-tune the threshold, which varies widely across different tasks and datasets.\n\nAnother way to measure instance difficulty is to directly learn it. Recent studies (Elbayad et al., 2020;Xin et al., 2021) that use the idea of \"learn-toexit\" have achieved promising results. They jointly train a neural model to predict for each instance the exiting layer. At their core, the learn-to-exit module is to estimate the difficulty for each instance. Compared with previous heuristically designed metrics for difficulty, learn-to-exit is task-agnostic and does not require threshold-tuning, therefore is a more promising way.\n\nDespite their success, it is still unknown whether or how well the instance difficulty can be learned. As a response, in this work, we construct datasets for two kinds of instance difficulty: (a) Humandefined difficulty, and (b) Model-defined difficulty. The dataset for human-defined difficulty has two labels, 0 for instances that can be annotated by human and 1 for instances that cannot. For modeldefined difficulty, we train a multi-exit BERT (Devlin et al., 2019), which is attached with an internal classifier at each layer, on a sentence-level classification task, SNLI (Bowman et al., 2015), and a token-level classification task, OntoNotes NER (Hovy et al., 2006). The trained multi-exit BERTs are then used to annotate for each development instance whether it can be correctly predicted by each internal classifier. Thus, our constructed sentence-level and token-level model-defined difficulty datasets are multi-label classification datasets. Experimental results demonstrate that, modern neu-ral networks perform poorly on predicting instance difficulty. This observation is consistent with previous work (Laverghetta et al., 2020) on estimating instance difficulty for curriculum learning.\n\nGiven that instance difficulty is hard to be predicted, then what works in the learn-to-exit modules? We hypothesis that the consistency between training and inference may play an important role. That is, for a training instance x i that is predicted to exit at layer l, an inference instance x j that is similar with x i should be predicted to exit at layer l, too. Since neural networks are usually smooth functions (Ziegel, 2003), this consistency can be easily satisfied by neural learn-to-exit modules. If this hypothesis holds, we can replace the learn-to-exit module with a simple hash function. In particular, we use hash functions to assign each token to a fixed exiting layer. This hash-based early exiting method is named HASHEE.\n\nCompared with previous methods that use heuristic metrics for difficulty or jointly learn to exit, HASHEE offers several advantages: (a) HASHEE requires no internal classifiers nor extra parameters, which are necessary in previous work. (b) HASHEE can perform token-level early exiting without supervision, therefore can be widely used on various tasks including language understanding and generation. (c) The speed-up ratio can be easily tuned by modifying the hash function. (d) HASHEE can significantly accelerate model inference on a per-batch basis instead of per-instance basis as in previous work Liu et al., 2020a;.\n\nWe conduct experiments on classification, regression, and generation tasks. Experimental results on ELUE (Liu et al., 2021a) demonstrate that HASHEE, despite its simplicity, can achieve higher performance with fewer FLOPs and inference time than previous state-of-the-art methods on various tasks. Besides, our experiments on several text summarization datasets show that HASHEE can reduce \u223c50% FLOPs of BART (Lewis et al., 2020) and CPT  while maintaining 97% ROUGE-1 score.\n\n References: \nSamuel, R., Bowman, G., Angeli, C., Potts, C. D., and Manning 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 632--642 10.18653/v1/d15-1075\nDaniel, M., Cer, M. T., Diab, E., Agirre, I., Lopez-Gazpio, L., and Specia 2017. Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. In Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. abs/1708.00055\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L. 2019. Universal transformers. In 7th International Conference on Learning Representations. ICLR 2019\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 4171--4186 10.18653/v1/n19-1423\nWilliam, B., Dolan, C., and Brockett 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005.\nElbayad, M., Gu, J., Grave, E., and Auli, M. 2020. Depth-adaptive transformer. In 8th International Conference on Learning Representations.\nGordon, M. A., Duh, K., and Andrews, N. 2020. Compressing BERT: studying the effects of weight pruning on transfer learning. In Proceedings of the 5th Workshop on Representation Learning for NLP, RepL4NLP@ACL 2020, Online. pp. 143--155 10.18653/v1/2020.repl4nlp-1.18\nMoritz Hermann, K., Kocisk\u00fd, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., and Blunsom, P. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. pp. 1693--1701\nHochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. In Neural Comput. pp. 1735--1780 10.1162/neco.1997.9.8.1735\nHovy, E. H., Marcus, M. P., Palmer, M., Ramshaw, L. A., and Weischedel, R. M. 2006. Ontonotes: The 90% solution. In Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings.\nHua, L., Wan, X., and Li, L. 2017. Overview of the nlpcc 2017 shared task: Single document summarization. In National CCF Conference on Natural Language Processing and Chinese Computing. pp. 942--947\nJiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., and Liu, Q. 2020. Tinybert: Distilling BERT for natural language understanding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020. pp. 4163--4174 10.18653/v1/2020.findings-emnlp.372\nKhot, T., Sabharwal, A., and Clark, P. 2018. Scitail: A textual entailment dataset from science question answering. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). pp. 5189--5197\nKim, B., Kim, H., and Kim, G. 2019. Abstractive summarization of reddit posts with multi-level memory networks. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 2519--2531 10.18653/v1/n19-1260\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In 8th International Conference on Learning Representations.\nLaverghetta, A., Mirzakhalov, J., and Licato, J. 2020. Towards a task-agnostic model of difficulty estimation for supervised learning tasks. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, AACL/IJCNLP 2021. pp. 16--23\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020.\n2020. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703\nLi, L., Lin, Y., Chen, D., Ren, S., Li, P., Zhou, J., and Sun, X. 2021. Cascadebert: Accelerating inference of pre-trained language models via calibrated complete models cascade. In Findings of EMNLP.\nLi, X., Shao, Y., Sun, T., Yan, H., Qiu, X., and Huang, X. 2021. Accelerating BERT inference for sequence labeling via earlyexit. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 189--199 10.18653/v1/2021.acl-long.16\nLiao, K., Zhang, Y., Ren, X., and Su, Q. 2021. A global past-future early exit method for accelerating inference of pretrained language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. pp. 2013--2023 10.18653/v1/2021.naacl-main.162\nLiu, W., Zhou, P., Wang, Z., Zhao, Z., Deng, H., and Ju, Q. 2020. Fastbert: a selfdistilling BERT with adaptive inference time. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. pp. 6035--6044\nLiu, X., Sun, T., He, J., Wu, L., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. In Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. abs/2110.07038\nLiu, X., Lai, H., Wong, D. F., and Chao, L. S. 2020. Norm-based curriculum learning for neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 427--436 10.18653/v1/2020.acl-main.41\nLiu, Y., Meng, F., Zhou, J., Chen, Y., and Xu, J. 2021. Faster depth-adaptive transformers. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. pp. 13424--13432\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. 2011. Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference. pp. 142--150\nMichel, P., Levy, O., and Neubig, G. 2019. Are sixteen heads really better than one?. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 14014--14024\nQiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., and Huang, X. 2020. Pre-trained models for natural language processing: A survey. In SCIENCE CHINA Technological Sciences. 10.1007/s11431-020-1647-3\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In J. Mach. Learn. Res. pp. 67\nReimers, N. and Gurevych, I. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 3980--3990 10.18653/v1/D19-1410\nRoller, S., Sukhbaatar, S., Szlam, A., and Weston, J. 2021. Hash layers for large sparse models. In Hash layers for large sparse models.\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. abs/1910.01108\nAdriaan, M. J., Schakel, B. J., and Wilson 2015. Measuring word significance using distributed representations of words. In Measuring word significance using distributed representations of words. abs/1508.02297\nSchwartz, R., Stanovsky, G., Swayamdipta, S., Dodge, J., and Smith, N. A. 2020. The right tool for the job: Matching model and instance complexities. In Proceedings of the 58th.\n2020. Annual Meeting of the Association for Computational Linguistics. In Annual Meeting of the Association for Computational Linguistics. pp. 6640--6651\nShao, Y., Geng, Z., Liu, Y., Dai, J., Yang, F., Zhe, L., Bao, H., and Qiu, X. 2021. CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. In CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. abs/2109.05729\nShen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer, K. 2020. Q-BERT: hessian based ultra low precision quantization of BERT. In The Thirty-Fourth AAAI Conference on Artificial Intelligence. pp. 8815--8821\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1631--1642\nSun, S., Cheng, Y., Gan, Z., and Liu, J. 2019. Patient knowledge distillation for BERT model compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 4322--4331 10.18653/v1/D19-1441\nSun, T., Zhou, Y., Liu, X., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. In Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. abs/2105.13792\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. pp. 5998--6008\nWang, R., Cheng, M., Chen, X., Tang, X., and Hsieh, C. 2021. Rethinking architecture selection in differentiable NAS. In 9th International Conference on Learning Representations.\nXin, J., Tang, R., Lee, J., Yu, Y., and Lin, J. 2020. Deebert: Dynamic early exiting for accelerating BERT inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 2246--2251\nXin, J., Tang, R., Yu, Y., and Lin, J. 2021. Berxit: Early exiting for BERT with better finetuning and extension to regression. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main. pp. 91--104\nXu, C., Zhou, W., Ge, T., Wei, F., and Zhou, M. 2020. Bert-of-theseus: Compressing BERT by progressive module replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. pp. 7859--7869 10.18653/v1/2020.emnlp-main.633\nXu, L., Hu, H., Zhang, X., Li, L., Cao, C., Li, Y., Xu, Y., Sun, K., Yu, D., Yu, C., Tian, Y., Dong, Q., Liu, W., Shi, B., Cui, Y., Li, J., Zeng, J., Wang, R., Xie, W., Li, Y., Patterson, Y., Tian, Z., Zhang, Y., Zhou, H., Liu, S., Zhao, Z., Zhao, Q., Yue, C., Zhang, X., Yang, Z., Richardson, K., and Lan, Z. 2020. CLUE: A chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics. pp. 4762--4772 10.18653/v1/2020.coling-main.419\nYang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., Quoc, V., and Le 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 5754--5764\nZhang, C., Yu, M., Wang, W., and Yan, F. 2019. Mark: Exploiting cloud services for costeffective, slo-aware machine learning inference serving. In 2019 USENIX Annual Technical Conference, USENIX ATC 2019. pp. 1049--1062\nZhou, W., Xu, C., Ge, T., Mcauley, J. J., Xu, K., and Wei, F. 2020. BERT loses patience: Fast and robust inference with early exit. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020.\nZhu, W. 2021. Leebert: Learned early exit for BERT with cross-level optimization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 2968--2980 10.18653/v1/2021.acl-long.231\nZiegel, E. R. 2003. The elements of statistical learning. In Technometrics. pp. 267--268 10.1198/tech.2003.s770"
    },
    {
        "filename": "paper_8.txt",
        "start": 33,
        "end": 54,
        "label": "Unsupported Claim",
        "text": "widely used technique",
        "full_text": "Introduction\n\nEarly exiting is a widely used technique to accelerate inference of deep neural networks. With the rising of pre-trained language models (PLMs) (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;Qiu et al., 2020), early exiting is drawing increasing attention in the NLP community. At its core, early exiting allows simple instances to exit early while allowing hard instances to exit late. Thus, how to measure instance difficulty is a crucial problem.\n\nMost existing early exiting methods attach multiple internal classifiers to the PLM and adopt some heuristic metrics, such as entropy  (Xin et al., 2020;Liu et al., 2020a) or maximum softmax score (Schwartz et al., 2020) of internal outputs, to measure instance difficulty. However, these methods can not easily generalize to new tasks. On the one hand, these metrics are not accessible on some tasks such as regression. On the other hand, In order for these methods to perform well, one usually needs to fine-tune the threshold, which varies widely across different tasks and datasets.\n\nAnother way to measure instance difficulty is to directly learn it. Recent studies (Elbayad et al., 2020;Xin et al., 2021) that use the idea of \"learn-toexit\" have achieved promising results. They jointly train a neural model to predict for each instance the exiting layer. At their core, the learn-to-exit module is to estimate the difficulty for each instance. Compared with previous heuristically designed metrics for difficulty, learn-to-exit is task-agnostic and does not require threshold-tuning, therefore is a more promising way.\n\nDespite their success, it is still unknown whether or how well the instance difficulty can be learned. As a response, in this work, we construct datasets for two kinds of instance difficulty: (a) Humandefined difficulty, and (b) Model-defined difficulty. The dataset for human-defined difficulty has two labels, 0 for instances that can be annotated by human and 1 for instances that cannot. For modeldefined difficulty, we train a multi-exit BERT (Devlin et al., 2019), which is attached with an internal classifier at each layer, on a sentence-level classification task, SNLI (Bowman et al., 2015), and a token-level classification task, OntoNotes NER (Hovy et al., 2006). The trained multi-exit BERTs are then used to annotate for each development instance whether it can be correctly predicted by each internal classifier. Thus, our constructed sentence-level and token-level model-defined difficulty datasets are multi-label classification datasets. Experimental results demonstrate that, modern neu-ral networks perform poorly on predicting instance difficulty. This observation is consistent with previous work (Laverghetta et al., 2020) on estimating instance difficulty for curriculum learning.\n\nGiven that instance difficulty is hard to be predicted, then what works in the learn-to-exit modules? We hypothesis that the consistency between training and inference may play an important role. That is, for a training instance x i that is predicted to exit at layer l, an inference instance x j that is similar with x i should be predicted to exit at layer l, too. Since neural networks are usually smooth functions (Ziegel, 2003), this consistency can be easily satisfied by neural learn-to-exit modules. If this hypothesis holds, we can replace the learn-to-exit module with a simple hash function. In particular, we use hash functions to assign each token to a fixed exiting layer. This hash-based early exiting method is named HASHEE.\n\nCompared with previous methods that use heuristic metrics for difficulty or jointly learn to exit, HASHEE offers several advantages: (a) HASHEE requires no internal classifiers nor extra parameters, which are necessary in previous work. (b) HASHEE can perform token-level early exiting without supervision, therefore can be widely used on various tasks including language understanding and generation. (c) The speed-up ratio can be easily tuned by modifying the hash function. (d) HASHEE can significantly accelerate model inference on a per-batch basis instead of per-instance basis as in previous work Liu et al., 2020a;.\n\nWe conduct experiments on classification, regression, and generation tasks. Experimental results on ELUE (Liu et al., 2021a) demonstrate that HASHEE, despite its simplicity, can achieve higher performance with fewer FLOPs and inference time than previous state-of-the-art methods on various tasks. Besides, our experiments on several text summarization datasets show that HASHEE can reduce \u223c50% FLOPs of BART (Lewis et al., 2020) and CPT  while maintaining 97% ROUGE-1 score.\n\n References: \nSamuel, R., Bowman, G., Angeli, C., Potts, C. D., and Manning 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 632--642 10.18653/v1/d15-1075\nDaniel, M., Cer, M. T., Diab, E., Agirre, I., Lopez-Gazpio, L., and Specia 2017. Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. In Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. abs/1708.00055\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L. 2019. Universal transformers. In 7th International Conference on Learning Representations. ICLR 2019\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 4171--4186 10.18653/v1/n19-1423\nWilliam, B., Dolan, C., and Brockett 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005.\nElbayad, M., Gu, J., Grave, E., and Auli, M. 2020. Depth-adaptive transformer. In 8th International Conference on Learning Representations.\nGordon, M. A., Duh, K., and Andrews, N. 2020. Compressing BERT: studying the effects of weight pruning on transfer learning. In Proceedings of the 5th Workshop on Representation Learning for NLP, RepL4NLP@ACL 2020, Online. pp. 143--155 10.18653/v1/2020.repl4nlp-1.18\nMoritz Hermann, K., Kocisk\u00fd, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., and Blunsom, P. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. pp. 1693--1701\nHochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. In Neural Comput. pp. 1735--1780 10.1162/neco.1997.9.8.1735\nHovy, E. H., Marcus, M. P., Palmer, M., Ramshaw, L. A., and Weischedel, R. M. 2006. Ontonotes: The 90% solution. In Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings.\nHua, L., Wan, X., and Li, L. 2017. Overview of the nlpcc 2017 shared task: Single document summarization. In National CCF Conference on Natural Language Processing and Chinese Computing. pp. 942--947\nJiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., and Liu, Q. 2020. Tinybert: Distilling BERT for natural language understanding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020. pp. 4163--4174 10.18653/v1/2020.findings-emnlp.372\nKhot, T., Sabharwal, A., and Clark, P. 2018. Scitail: A textual entailment dataset from science question answering. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). pp. 5189--5197\nKim, B., Kim, H., and Kim, G. 2019. Abstractive summarization of reddit posts with multi-level memory networks. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 2519--2531 10.18653/v1/n19-1260\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In 8th International Conference on Learning Representations.\nLaverghetta, A., Mirzakhalov, J., and Licato, J. 2020. Towards a task-agnostic model of difficulty estimation for supervised learning tasks. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, AACL/IJCNLP 2021. pp. 16--23\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020.\n2020. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703\nLi, L., Lin, Y., Chen, D., Ren, S., Li, P., Zhou, J., and Sun, X. 2021. Cascadebert: Accelerating inference of pre-trained language models via calibrated complete models cascade. In Findings of EMNLP.\nLi, X., Shao, Y., Sun, T., Yan, H., Qiu, X., and Huang, X. 2021. Accelerating BERT inference for sequence labeling via earlyexit. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 189--199 10.18653/v1/2021.acl-long.16\nLiao, K., Zhang, Y., Ren, X., and Su, Q. 2021. A global past-future early exit method for accelerating inference of pretrained language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. pp. 2013--2023 10.18653/v1/2021.naacl-main.162\nLiu, W., Zhou, P., Wang, Z., Zhao, Z., Deng, H., and Ju, Q. 2020. Fastbert: a selfdistilling BERT with adaptive inference time. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. pp. 6035--6044\nLiu, X., Sun, T., He, J., Wu, L., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. In Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. abs/2110.07038\nLiu, X., Lai, H., Wong, D. F., and Chao, L. S. 2020. Norm-based curriculum learning for neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 427--436 10.18653/v1/2020.acl-main.41\nLiu, Y., Meng, F., Zhou, J., Chen, Y., and Xu, J. 2021. Faster depth-adaptive transformers. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. pp. 13424--13432\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. 2011. Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference. pp. 142--150\nMichel, P., Levy, O., and Neubig, G. 2019. Are sixteen heads really better than one?. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 14014--14024\nQiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., and Huang, X. 2020. Pre-trained models for natural language processing: A survey. In SCIENCE CHINA Technological Sciences. 10.1007/s11431-020-1647-3\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In J. Mach. Learn. Res. pp. 67\nReimers, N. and Gurevych, I. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 3980--3990 10.18653/v1/D19-1410\nRoller, S., Sukhbaatar, S., Szlam, A., and Weston, J. 2021. Hash layers for large sparse models. In Hash layers for large sparse models.\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. abs/1910.01108\nAdriaan, M. J., Schakel, B. J., and Wilson 2015. Measuring word significance using distributed representations of words. In Measuring word significance using distributed representations of words. abs/1508.02297\nSchwartz, R., Stanovsky, G., Swayamdipta, S., Dodge, J., and Smith, N. A. 2020. The right tool for the job: Matching model and instance complexities. In Proceedings of the 58th.\n2020. Annual Meeting of the Association for Computational Linguistics. In Annual Meeting of the Association for Computational Linguistics. pp. 6640--6651\nShao, Y., Geng, Z., Liu, Y., Dai, J., Yang, F., Zhe, L., Bao, H., and Qiu, X. 2021. CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. In CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. abs/2109.05729\nShen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer, K. 2020. Q-BERT: hessian based ultra low precision quantization of BERT. In The Thirty-Fourth AAAI Conference on Artificial Intelligence. pp. 8815--8821\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1631--1642\nSun, S., Cheng, Y., Gan, Z., and Liu, J. 2019. Patient knowledge distillation for BERT model compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 4322--4331 10.18653/v1/D19-1441\nSun, T., Zhou, Y., Liu, X., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. In Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. abs/2105.13792\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. pp. 5998--6008\nWang, R., Cheng, M., Chen, X., Tang, X., and Hsieh, C. 2021. Rethinking architecture selection in differentiable NAS. In 9th International Conference on Learning Representations.\nXin, J., Tang, R., Lee, J., Yu, Y., and Lin, J. 2020. Deebert: Dynamic early exiting for accelerating BERT inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 2246--2251\nXin, J., Tang, R., Yu, Y., and Lin, J. 2021. Berxit: Early exiting for BERT with better finetuning and extension to regression. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main. pp. 91--104\nXu, C., Zhou, W., Ge, T., Wei, F., and Zhou, M. 2020. Bert-of-theseus: Compressing BERT by progressive module replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. pp. 7859--7869 10.18653/v1/2020.emnlp-main.633\nXu, L., Hu, H., Zhang, X., Li, L., Cao, C., Li, Y., Xu, Y., Sun, K., Yu, D., Yu, C., Tian, Y., Dong, Q., Liu, W., Shi, B., Cui, Y., Li, J., Zeng, J., Wang, R., Xie, W., Li, Y., Patterson, Y., Tian, Z., Zhang, Y., Zhou, H., Liu, S., Zhao, Z., Zhao, Q., Yue, C., Zhang, X., Yang, Z., Richardson, K., and Lan, Z. 2020. CLUE: A chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics. pp. 4762--4772 10.18653/v1/2020.coling-main.419\nYang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., Quoc, V., and Le 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 5754--5764\nZhang, C., Yu, M., Wang, W., and Yan, F. 2019. Mark: Exploiting cloud services for costeffective, slo-aware machine learning inference serving. In 2019 USENIX Annual Technical Conference, USENIX ATC 2019. pp. 1049--1062\nZhou, W., Xu, C., Ge, T., Mcauley, J. J., Xu, K., and Wei, F. 2020. BERT loses patience: Fast and robust inference with early exit. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020.\nZhu, W. 2021. Leebert: Learned early exit for BERT with cross-level optimization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 2968--2980 10.18653/v1/2021.acl-long.231\nZiegel, E. R. 2003. The elements of statistical learning. In Technometrics. pp. 267--268 10.1198/tech.2003.s770"
    },
    {
        "filename": "paper_8.txt",
        "start": 401,
        "end": 411,
        "label": "Unsupported Claim",
        "text": "exit late",
        "full_text": "Introduction\n\nEarly exiting is a widely used technique to accelerate inference of deep neural networks. With the rising of pre-trained language models (PLMs) (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;Qiu et al., 2020), early exiting is drawing increasing attention in the NLP community. At its core, early exiting allows simple instances to exit early while allowing hard instances to exit late. Thus, how to measure instance difficulty is a crucial problem.\n\nMost existing early exiting methods attach multiple internal classifiers to the PLM and adopt some heuristic metrics, such as entropy  (Xin et al., 2020;Liu et al., 2020a) or maximum softmax score (Schwartz et al., 2020) of internal outputs, to measure instance difficulty. However, these methods can not easily generalize to new tasks. On the one hand, these metrics are not accessible on some tasks such as regression. On the other hand, In order for these methods to perform well, one usually needs to fine-tune the threshold, which varies widely across different tasks and datasets.\n\nAnother way to measure instance difficulty is to directly learn it. Recent studies (Elbayad et al., 2020;Xin et al., 2021) that use the idea of \"learn-toexit\" have achieved promising results. They jointly train a neural model to predict for each instance the exiting layer. At their core, the learn-to-exit module is to estimate the difficulty for each instance. Compared with previous heuristically designed metrics for difficulty, learn-to-exit is task-agnostic and does not require threshold-tuning, therefore is a more promising way.\n\nDespite their success, it is still unknown whether or how well the instance difficulty can be learned. As a response, in this work, we construct datasets for two kinds of instance difficulty: (a) Humandefined difficulty, and (b) Model-defined difficulty. The dataset for human-defined difficulty has two labels, 0 for instances that can be annotated by human and 1 for instances that cannot. For modeldefined difficulty, we train a multi-exit BERT (Devlin et al., 2019), which is attached with an internal classifier at each layer, on a sentence-level classification task, SNLI (Bowman et al., 2015), and a token-level classification task, OntoNotes NER (Hovy et al., 2006). The trained multi-exit BERTs are then used to annotate for each development instance whether it can be correctly predicted by each internal classifier. Thus, our constructed sentence-level and token-level model-defined difficulty datasets are multi-label classification datasets. Experimental results demonstrate that, modern neu-ral networks perform poorly on predicting instance difficulty. This observation is consistent with previous work (Laverghetta et al., 2020) on estimating instance difficulty for curriculum learning.\n\nGiven that instance difficulty is hard to be predicted, then what works in the learn-to-exit modules? We hypothesis that the consistency between training and inference may play an important role. That is, for a training instance x i that is predicted to exit at layer l, an inference instance x j that is similar with x i should be predicted to exit at layer l, too. Since neural networks are usually smooth functions (Ziegel, 2003), this consistency can be easily satisfied by neural learn-to-exit modules. If this hypothesis holds, we can replace the learn-to-exit module with a simple hash function. In particular, we use hash functions to assign each token to a fixed exiting layer. This hash-based early exiting method is named HASHEE.\n\nCompared with previous methods that use heuristic metrics for difficulty or jointly learn to exit, HASHEE offers several advantages: (a) HASHEE requires no internal classifiers nor extra parameters, which are necessary in previous work. (b) HASHEE can perform token-level early exiting without supervision, therefore can be widely used on various tasks including language understanding and generation. (c) The speed-up ratio can be easily tuned by modifying the hash function. (d) HASHEE can significantly accelerate model inference on a per-batch basis instead of per-instance basis as in previous work Liu et al., 2020a;.\n\nWe conduct experiments on classification, regression, and generation tasks. Experimental results on ELUE (Liu et al., 2021a) demonstrate that HASHEE, despite its simplicity, can achieve higher performance with fewer FLOPs and inference time than previous state-of-the-art methods on various tasks. Besides, our experiments on several text summarization datasets show that HASHEE can reduce \u223c50% FLOPs of BART (Lewis et al., 2020) and CPT  while maintaining 97% ROUGE-1 score.\n\n References: \nSamuel, R., Bowman, G., Angeli, C., Potts, C. D., and Manning 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 632--642 10.18653/v1/d15-1075\nDaniel, M., Cer, M. T., Diab, E., Agirre, I., Lopez-Gazpio, L., and Specia 2017. Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. In Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. abs/1708.00055\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L. 2019. Universal transformers. In 7th International Conference on Learning Representations. ICLR 2019\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 4171--4186 10.18653/v1/n19-1423\nWilliam, B., Dolan, C., and Brockett 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005.\nElbayad, M., Gu, J., Grave, E., and Auli, M. 2020. Depth-adaptive transformer. In 8th International Conference on Learning Representations.\nGordon, M. A., Duh, K., and Andrews, N. 2020. Compressing BERT: studying the effects of weight pruning on transfer learning. In Proceedings of the 5th Workshop on Representation Learning for NLP, RepL4NLP@ACL 2020, Online. pp. 143--155 10.18653/v1/2020.repl4nlp-1.18\nMoritz Hermann, K., Kocisk\u00fd, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., and Blunsom, P. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. pp. 1693--1701\nHochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. In Neural Comput. pp. 1735--1780 10.1162/neco.1997.9.8.1735\nHovy, E. H., Marcus, M. P., Palmer, M., Ramshaw, L. A., and Weischedel, R. M. 2006. Ontonotes: The 90% solution. In Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings.\nHua, L., Wan, X., and Li, L. 2017. Overview of the nlpcc 2017 shared task: Single document summarization. In National CCF Conference on Natural Language Processing and Chinese Computing. pp. 942--947\nJiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., and Liu, Q. 2020. Tinybert: Distilling BERT for natural language understanding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020. pp. 4163--4174 10.18653/v1/2020.findings-emnlp.372\nKhot, T., Sabharwal, A., and Clark, P. 2018. Scitail: A textual entailment dataset from science question answering. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). pp. 5189--5197\nKim, B., Kim, H., and Kim, G. 2019. Abstractive summarization of reddit posts with multi-level memory networks. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 2519--2531 10.18653/v1/n19-1260\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In 8th International Conference on Learning Representations.\nLaverghetta, A., Mirzakhalov, J., and Licato, J. 2020. Towards a task-agnostic model of difficulty estimation for supervised learning tasks. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, AACL/IJCNLP 2021. pp. 16--23\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020.\n2020. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703\nLi, L., Lin, Y., Chen, D., Ren, S., Li, P., Zhou, J., and Sun, X. 2021. Cascadebert: Accelerating inference of pre-trained language models via calibrated complete models cascade. In Findings of EMNLP.\nLi, X., Shao, Y., Sun, T., Yan, H., Qiu, X., and Huang, X. 2021. Accelerating BERT inference for sequence labeling via earlyexit. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 189--199 10.18653/v1/2021.acl-long.16\nLiao, K., Zhang, Y., Ren, X., and Su, Q. 2021. A global past-future early exit method for accelerating inference of pretrained language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. pp. 2013--2023 10.18653/v1/2021.naacl-main.162\nLiu, W., Zhou, P., Wang, Z., Zhao, Z., Deng, H., and Ju, Q. 2020. Fastbert: a selfdistilling BERT with adaptive inference time. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. pp. 6035--6044\nLiu, X., Sun, T., He, J., Wu, L., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. In Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. abs/2110.07038\nLiu, X., Lai, H., Wong, D. F., and Chao, L. S. 2020. Norm-based curriculum learning for neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 427--436 10.18653/v1/2020.acl-main.41\nLiu, Y., Meng, F., Zhou, J., Chen, Y., and Xu, J. 2021. Faster depth-adaptive transformers. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. pp. 13424--13432\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. 2011. Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference. pp. 142--150\nMichel, P., Levy, O., and Neubig, G. 2019. Are sixteen heads really better than one?. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 14014--14024\nQiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., and Huang, X. 2020. Pre-trained models for natural language processing: A survey. In SCIENCE CHINA Technological Sciences. 10.1007/s11431-020-1647-3\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In J. Mach. Learn. Res. pp. 67\nReimers, N. and Gurevych, I. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 3980--3990 10.18653/v1/D19-1410\nRoller, S., Sukhbaatar, S., Szlam, A., and Weston, J. 2021. Hash layers for large sparse models. In Hash layers for large sparse models.\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. abs/1910.01108\nAdriaan, M. J., Schakel, B. J., and Wilson 2015. Measuring word significance using distributed representations of words. In Measuring word significance using distributed representations of words. abs/1508.02297\nSchwartz, R., Stanovsky, G., Swayamdipta, S., Dodge, J., and Smith, N. A. 2020. The right tool for the job: Matching model and instance complexities. In Proceedings of the 58th.\n2020. Annual Meeting of the Association for Computational Linguistics. In Annual Meeting of the Association for Computational Linguistics. pp. 6640--6651\nShao, Y., Geng, Z., Liu, Y., Dai, J., Yang, F., Zhe, L., Bao, H., and Qiu, X. 2021. CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. In CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. abs/2109.05729\nShen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer, K. 2020. Q-BERT: hessian based ultra low precision quantization of BERT. In The Thirty-Fourth AAAI Conference on Artificial Intelligence. pp. 8815--8821\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1631--1642\nSun, S., Cheng, Y., Gan, Z., and Liu, J. 2019. Patient knowledge distillation for BERT model compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 4322--4331 10.18653/v1/D19-1441\nSun, T., Zhou, Y., Liu, X., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. In Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. abs/2105.13792\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. pp. 5998--6008\nWang, R., Cheng, M., Chen, X., Tang, X., and Hsieh, C. 2021. Rethinking architecture selection in differentiable NAS. In 9th International Conference on Learning Representations.\nXin, J., Tang, R., Lee, J., Yu, Y., and Lin, J. 2020. Deebert: Dynamic early exiting for accelerating BERT inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 2246--2251\nXin, J., Tang, R., Yu, Y., and Lin, J. 2021. Berxit: Early exiting for BERT with better finetuning and extension to regression. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main. pp. 91--104\nXu, C., Zhou, W., Ge, T., Wei, F., and Zhou, M. 2020. Bert-of-theseus: Compressing BERT by progressive module replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. pp. 7859--7869 10.18653/v1/2020.emnlp-main.633\nXu, L., Hu, H., Zhang, X., Li, L., Cao, C., Li, Y., Xu, Y., Sun, K., Yu, D., Yu, C., Tian, Y., Dong, Q., Liu, W., Shi, B., Cui, Y., Li, J., Zeng, J., Wang, R., Xie, W., Li, Y., Patterson, Y., Tian, Z., Zhang, Y., Zhou, H., Liu, S., Zhao, Z., Zhao, Q., Yue, C., Zhang, X., Yang, Z., Richardson, K., and Lan, Z. 2020. CLUE: A chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics. pp. 4762--4772 10.18653/v1/2020.coling-main.419\nYang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., Quoc, V., and Le 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 5754--5764\nZhang, C., Yu, M., Wang, W., and Yan, F. 2019. Mark: Exploiting cloud services for costeffective, slo-aware machine learning inference serving. In 2019 USENIX Annual Technical Conference, USENIX ATC 2019. pp. 1049--1062\nZhou, W., Xu, C., Ge, T., Mcauley, J. J., Xu, K., and Wei, F. 2020. BERT loses patience: Fast and robust inference with early exit. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020.\nZhu, W. 2021. Leebert: Learned early exit for BERT with cross-level optimization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 2968--2980 10.18653/v1/2021.acl-long.231\nZiegel, E. R. 2003. The elements of statistical learning. In Technometrics. pp. 267--268 10.1198/tech.2003.s770"
    },
    {
        "filename": "paper_8.txt",
        "start": 751,
        "end": 812,
        "label": "Unsupported Claim",
        "text": "However, these methods can not easily generalize to new tasks",
        "full_text": "Introduction\n\nEarly exiting is a widely used technique to accelerate inference of deep neural networks. With the rising of pre-trained language models (PLMs) (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;Qiu et al., 2020), early exiting is drawing increasing attention in the NLP community. At its core, early exiting allows simple instances to exit early while allowing hard instances to exit late. Thus, how to measure instance difficulty is a crucial problem.\n\nMost existing early exiting methods attach multiple internal classifiers to the PLM and adopt some heuristic metrics, such as entropy  (Xin et al., 2020;Liu et al., 2020a) or maximum softmax score (Schwartz et al., 2020) of internal outputs, to measure instance difficulty. However, these methods can not easily generalize to new tasks. On the one hand, these metrics are not accessible on some tasks such as regression. On the other hand, In order for these methods to perform well, one usually needs to fine-tune the threshold, which varies widely across different tasks and datasets.\n\nAnother way to measure instance difficulty is to directly learn it. Recent studies (Elbayad et al., 2020;Xin et al., 2021) that use the idea of \"learn-toexit\" have achieved promising results. They jointly train a neural model to predict for each instance the exiting layer. At their core, the learn-to-exit module is to estimate the difficulty for each instance. Compared with previous heuristically designed metrics for difficulty, learn-to-exit is task-agnostic and does not require threshold-tuning, therefore is a more promising way.\n\nDespite their success, it is still unknown whether or how well the instance difficulty can be learned. As a response, in this work, we construct datasets for two kinds of instance difficulty: (a) Humandefined difficulty, and (b) Model-defined difficulty. The dataset for human-defined difficulty has two labels, 0 for instances that can be annotated by human and 1 for instances that cannot. For modeldefined difficulty, we train a multi-exit BERT (Devlin et al., 2019), which is attached with an internal classifier at each layer, on a sentence-level classification task, SNLI (Bowman et al., 2015), and a token-level classification task, OntoNotes NER (Hovy et al., 2006). The trained multi-exit BERTs are then used to annotate for each development instance whether it can be correctly predicted by each internal classifier. Thus, our constructed sentence-level and token-level model-defined difficulty datasets are multi-label classification datasets. Experimental results demonstrate that, modern neu-ral networks perform poorly on predicting instance difficulty. This observation is consistent with previous work (Laverghetta et al., 2020) on estimating instance difficulty for curriculum learning.\n\nGiven that instance difficulty is hard to be predicted, then what works in the learn-to-exit modules? We hypothesis that the consistency between training and inference may play an important role. That is, for a training instance x i that is predicted to exit at layer l, an inference instance x j that is similar with x i should be predicted to exit at layer l, too. Since neural networks are usually smooth functions (Ziegel, 2003), this consistency can be easily satisfied by neural learn-to-exit modules. If this hypothesis holds, we can replace the learn-to-exit module with a simple hash function. In particular, we use hash functions to assign each token to a fixed exiting layer. This hash-based early exiting method is named HASHEE.\n\nCompared with previous methods that use heuristic metrics for difficulty or jointly learn to exit, HASHEE offers several advantages: (a) HASHEE requires no internal classifiers nor extra parameters, which are necessary in previous work. (b) HASHEE can perform token-level early exiting without supervision, therefore can be widely used on various tasks including language understanding and generation. (c) The speed-up ratio can be easily tuned by modifying the hash function. (d) HASHEE can significantly accelerate model inference on a per-batch basis instead of per-instance basis as in previous work Liu et al., 2020a;.\n\nWe conduct experiments on classification, regression, and generation tasks. Experimental results on ELUE (Liu et al., 2021a) demonstrate that HASHEE, despite its simplicity, can achieve higher performance with fewer FLOPs and inference time than previous state-of-the-art methods on various tasks. Besides, our experiments on several text summarization datasets show that HASHEE can reduce \u223c50% FLOPs of BART (Lewis et al., 2020) and CPT  while maintaining 97% ROUGE-1 score.\n\n References: \nSamuel, R., Bowman, G., Angeli, C., Potts, C. D., and Manning 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 632--642 10.18653/v1/d15-1075\nDaniel, M., Cer, M. T., Diab, E., Agirre, I., Lopez-Gazpio, L., and Specia 2017. Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. In Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. abs/1708.00055\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L. 2019. Universal transformers. In 7th International Conference on Learning Representations. ICLR 2019\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 4171--4186 10.18653/v1/n19-1423\nWilliam, B., Dolan, C., and Brockett 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005.\nElbayad, M., Gu, J., Grave, E., and Auli, M. 2020. Depth-adaptive transformer. In 8th International Conference on Learning Representations.\nGordon, M. A., Duh, K., and Andrews, N. 2020. Compressing BERT: studying the effects of weight pruning on transfer learning. In Proceedings of the 5th Workshop on Representation Learning for NLP, RepL4NLP@ACL 2020, Online. pp. 143--155 10.18653/v1/2020.repl4nlp-1.18\nMoritz Hermann, K., Kocisk\u00fd, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., and Blunsom, P. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. pp. 1693--1701\nHochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. In Neural Comput. pp. 1735--1780 10.1162/neco.1997.9.8.1735\nHovy, E. H., Marcus, M. P., Palmer, M., Ramshaw, L. A., and Weischedel, R. M. 2006. Ontonotes: The 90% solution. In Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings.\nHua, L., Wan, X., and Li, L. 2017. Overview of the nlpcc 2017 shared task: Single document summarization. In National CCF Conference on Natural Language Processing and Chinese Computing. pp. 942--947\nJiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., and Liu, Q. 2020. Tinybert: Distilling BERT for natural language understanding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020. pp. 4163--4174 10.18653/v1/2020.findings-emnlp.372\nKhot, T., Sabharwal, A., and Clark, P. 2018. Scitail: A textual entailment dataset from science question answering. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). pp. 5189--5197\nKim, B., Kim, H., and Kim, G. 2019. Abstractive summarization of reddit posts with multi-level memory networks. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 2519--2531 10.18653/v1/n19-1260\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In 8th International Conference on Learning Representations.\nLaverghetta, A., Mirzakhalov, J., and Licato, J. 2020. Towards a task-agnostic model of difficulty estimation for supervised learning tasks. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, AACL/IJCNLP 2021. pp. 16--23\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020.\n2020. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703\nLi, L., Lin, Y., Chen, D., Ren, S., Li, P., Zhou, J., and Sun, X. 2021. Cascadebert: Accelerating inference of pre-trained language models via calibrated complete models cascade. In Findings of EMNLP.\nLi, X., Shao, Y., Sun, T., Yan, H., Qiu, X., and Huang, X. 2021. Accelerating BERT inference for sequence labeling via earlyexit. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 189--199 10.18653/v1/2021.acl-long.16\nLiao, K., Zhang, Y., Ren, X., and Su, Q. 2021. A global past-future early exit method for accelerating inference of pretrained language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. pp. 2013--2023 10.18653/v1/2021.naacl-main.162\nLiu, W., Zhou, P., Wang, Z., Zhao, Z., Deng, H., and Ju, Q. 2020. Fastbert: a selfdistilling BERT with adaptive inference time. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. pp. 6035--6044\nLiu, X., Sun, T., He, J., Wu, L., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. In Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. abs/2110.07038\nLiu, X., Lai, H., Wong, D. F., and Chao, L. S. 2020. Norm-based curriculum learning for neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 427--436 10.18653/v1/2020.acl-main.41\nLiu, Y., Meng, F., Zhou, J., Chen, Y., and Xu, J. 2021. Faster depth-adaptive transformers. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. pp. 13424--13432\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. 2011. Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference. pp. 142--150\nMichel, P., Levy, O., and Neubig, G. 2019. Are sixteen heads really better than one?. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 14014--14024\nQiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., and Huang, X. 2020. Pre-trained models for natural language processing: A survey. In SCIENCE CHINA Technological Sciences. 10.1007/s11431-020-1647-3\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In J. Mach. Learn. Res. pp. 67\nReimers, N. and Gurevych, I. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 3980--3990 10.18653/v1/D19-1410\nRoller, S., Sukhbaatar, S., Szlam, A., and Weston, J. 2021. Hash layers for large sparse models. In Hash layers for large sparse models.\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. abs/1910.01108\nAdriaan, M. J., Schakel, B. J., and Wilson 2015. Measuring word significance using distributed representations of words. In Measuring word significance using distributed representations of words. abs/1508.02297\nSchwartz, R., Stanovsky, G., Swayamdipta, S., Dodge, J., and Smith, N. A. 2020. The right tool for the job: Matching model and instance complexities. In Proceedings of the 58th.\n2020. Annual Meeting of the Association for Computational Linguistics. In Annual Meeting of the Association for Computational Linguistics. pp. 6640--6651\nShao, Y., Geng, Z., Liu, Y., Dai, J., Yang, F., Zhe, L., Bao, H., and Qiu, X. 2021. CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. In CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. abs/2109.05729\nShen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer, K. 2020. Q-BERT: hessian based ultra low precision quantization of BERT. In The Thirty-Fourth AAAI Conference on Artificial Intelligence. pp. 8815--8821\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1631--1642\nSun, S., Cheng, Y., Gan, Z., and Liu, J. 2019. Patient knowledge distillation for BERT model compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 4322--4331 10.18653/v1/D19-1441\nSun, T., Zhou, Y., Liu, X., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. In Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. abs/2105.13792\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. pp. 5998--6008\nWang, R., Cheng, M., Chen, X., Tang, X., and Hsieh, C. 2021. Rethinking architecture selection in differentiable NAS. In 9th International Conference on Learning Representations.\nXin, J., Tang, R., Lee, J., Yu, Y., and Lin, J. 2020. Deebert: Dynamic early exiting for accelerating BERT inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 2246--2251\nXin, J., Tang, R., Yu, Y., and Lin, J. 2021. Berxit: Early exiting for BERT with better finetuning and extension to regression. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main. pp. 91--104\nXu, C., Zhou, W., Ge, T., Wei, F., and Zhou, M. 2020. Bert-of-theseus: Compressing BERT by progressive module replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. pp. 7859--7869 10.18653/v1/2020.emnlp-main.633\nXu, L., Hu, H., Zhang, X., Li, L., Cao, C., Li, Y., Xu, Y., Sun, K., Yu, D., Yu, C., Tian, Y., Dong, Q., Liu, W., Shi, B., Cui, Y., Li, J., Zeng, J., Wang, R., Xie, W., Li, Y., Patterson, Y., Tian, Z., Zhang, Y., Zhou, H., Liu, S., Zhao, Z., Zhao, Q., Yue, C., Zhang, X., Yang, Z., Richardson, K., and Lan, Z. 2020. CLUE: A chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics. pp. 4762--4772 10.18653/v1/2020.coling-main.419\nYang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., Quoc, V., and Le 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 5754--5764\nZhang, C., Yu, M., Wang, W., and Yan, F. 2019. Mark: Exploiting cloud services for costeffective, slo-aware machine learning inference serving. In 2019 USENIX Annual Technical Conference, USENIX ATC 2019. pp. 1049--1062\nZhou, W., Xu, C., Ge, T., Mcauley, J. J., Xu, K., and Wei, F. 2020. BERT loses patience: Fast and robust inference with early exit. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020.\nZhu, W. 2021. Leebert: Learned early exit for BERT with cross-level optimization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 2968--2980 10.18653/v1/2021.acl-long.231\nZiegel, E. R. 2003. The elements of statistical learning. In Technometrics. pp. 267--268 10.1198/tech.2003.s770"
    },
    {
        "filename": "paper_8.txt",
        "start": 3565,
        "end": 3582,
        "label": "Unsupported Claim",
        "text": "previous methods",
        "full_text": "Introduction\n\nEarly exiting is a widely used technique to accelerate inference of deep neural networks. With the rising of pre-trained language models (PLMs) (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;Qiu et al., 2020), early exiting is drawing increasing attention in the NLP community. At its core, early exiting allows simple instances to exit early while allowing hard instances to exit late. Thus, how to measure instance difficulty is a crucial problem.\n\nMost existing early exiting methods attach multiple internal classifiers to the PLM and adopt some heuristic metrics, such as entropy  (Xin et al., 2020;Liu et al., 2020a) or maximum softmax score (Schwartz et al., 2020) of internal outputs, to measure instance difficulty. However, these methods can not easily generalize to new tasks. On the one hand, these metrics are not accessible on some tasks such as regression. On the other hand, In order for these methods to perform well, one usually needs to fine-tune the threshold, which varies widely across different tasks and datasets.\n\nAnother way to measure instance difficulty is to directly learn it. Recent studies (Elbayad et al., 2020;Xin et al., 2021) that use the idea of \"learn-toexit\" have achieved promising results. They jointly train a neural model to predict for each instance the exiting layer. At their core, the learn-to-exit module is to estimate the difficulty for each instance. Compared with previous heuristically designed metrics for difficulty, learn-to-exit is task-agnostic and does not require threshold-tuning, therefore is a more promising way.\n\nDespite their success, it is still unknown whether or how well the instance difficulty can be learned. As a response, in this work, we construct datasets for two kinds of instance difficulty: (a) Humandefined difficulty, and (b) Model-defined difficulty. The dataset for human-defined difficulty has two labels, 0 for instances that can be annotated by human and 1 for instances that cannot. For modeldefined difficulty, we train a multi-exit BERT (Devlin et al., 2019), which is attached with an internal classifier at each layer, on a sentence-level classification task, SNLI (Bowman et al., 2015), and a token-level classification task, OntoNotes NER (Hovy et al., 2006). The trained multi-exit BERTs are then used to annotate for each development instance whether it can be correctly predicted by each internal classifier. Thus, our constructed sentence-level and token-level model-defined difficulty datasets are multi-label classification datasets. Experimental results demonstrate that, modern neu-ral networks perform poorly on predicting instance difficulty. This observation is consistent with previous work (Laverghetta et al., 2020) on estimating instance difficulty for curriculum learning.\n\nGiven that instance difficulty is hard to be predicted, then what works in the learn-to-exit modules? We hypothesis that the consistency between training and inference may play an important role. That is, for a training instance x i that is predicted to exit at layer l, an inference instance x j that is similar with x i should be predicted to exit at layer l, too. Since neural networks are usually smooth functions (Ziegel, 2003), this consistency can be easily satisfied by neural learn-to-exit modules. If this hypothesis holds, we can replace the learn-to-exit module with a simple hash function. In particular, we use hash functions to assign each token to a fixed exiting layer. This hash-based early exiting method is named HASHEE.\n\nCompared with previous methods that use heuristic metrics for difficulty or jointly learn to exit, HASHEE offers several advantages: (a) HASHEE requires no internal classifiers nor extra parameters, which are necessary in previous work. (b) HASHEE can perform token-level early exiting without supervision, therefore can be widely used on various tasks including language understanding and generation. (c) The speed-up ratio can be easily tuned by modifying the hash function. (d) HASHEE can significantly accelerate model inference on a per-batch basis instead of per-instance basis as in previous work Liu et al., 2020a;.\n\nWe conduct experiments on classification, regression, and generation tasks. Experimental results on ELUE (Liu et al., 2021a) demonstrate that HASHEE, despite its simplicity, can achieve higher performance with fewer FLOPs and inference time than previous state-of-the-art methods on various tasks. Besides, our experiments on several text summarization datasets show that HASHEE can reduce \u223c50% FLOPs of BART (Lewis et al., 2020) and CPT  while maintaining 97% ROUGE-1 score.\n\n References: \nSamuel, R., Bowman, G., Angeli, C., Potts, C. D., and Manning 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 632--642 10.18653/v1/d15-1075\nDaniel, M., Cer, M. T., Diab, E., Agirre, I., Lopez-Gazpio, L., and Specia 2017. Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. In Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. abs/1708.00055\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L. 2019. Universal transformers. In 7th International Conference on Learning Representations. ICLR 2019\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 4171--4186 10.18653/v1/n19-1423\nWilliam, B., Dolan, C., and Brockett 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005.\nElbayad, M., Gu, J., Grave, E., and Auli, M. 2020. Depth-adaptive transformer. In 8th International Conference on Learning Representations.\nGordon, M. A., Duh, K., and Andrews, N. 2020. Compressing BERT: studying the effects of weight pruning on transfer learning. In Proceedings of the 5th Workshop on Representation Learning for NLP, RepL4NLP@ACL 2020, Online. pp. 143--155 10.18653/v1/2020.repl4nlp-1.18\nMoritz Hermann, K., Kocisk\u00fd, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., and Blunsom, P. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. pp. 1693--1701\nHochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. In Neural Comput. pp. 1735--1780 10.1162/neco.1997.9.8.1735\nHovy, E. H., Marcus, M. P., Palmer, M., Ramshaw, L. A., and Weischedel, R. M. 2006. Ontonotes: The 90% solution. In Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings.\nHua, L., Wan, X., and Li, L. 2017. Overview of the nlpcc 2017 shared task: Single document summarization. In National CCF Conference on Natural Language Processing and Chinese Computing. pp. 942--947\nJiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., and Liu, Q. 2020. Tinybert: Distilling BERT for natural language understanding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020. pp. 4163--4174 10.18653/v1/2020.findings-emnlp.372\nKhot, T., Sabharwal, A., and Clark, P. 2018. Scitail: A textual entailment dataset from science question answering. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). pp. 5189--5197\nKim, B., Kim, H., and Kim, G. 2019. Abstractive summarization of reddit posts with multi-level memory networks. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 2519--2531 10.18653/v1/n19-1260\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In 8th International Conference on Learning Representations.\nLaverghetta, A., Mirzakhalov, J., and Licato, J. 2020. Towards a task-agnostic model of difficulty estimation for supervised learning tasks. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, AACL/IJCNLP 2021. pp. 16--23\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020.\n2020. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703\nLi, L., Lin, Y., Chen, D., Ren, S., Li, P., Zhou, J., and Sun, X. 2021. Cascadebert: Accelerating inference of pre-trained language models via calibrated complete models cascade. In Findings of EMNLP.\nLi, X., Shao, Y., Sun, T., Yan, H., Qiu, X., and Huang, X. 2021. Accelerating BERT inference for sequence labeling via earlyexit. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 189--199 10.18653/v1/2021.acl-long.16\nLiao, K., Zhang, Y., Ren, X., and Su, Q. 2021. A global past-future early exit method for accelerating inference of pretrained language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. pp. 2013--2023 10.18653/v1/2021.naacl-main.162\nLiu, W., Zhou, P., Wang, Z., Zhao, Z., Deng, H., and Ju, Q. 2020. Fastbert: a selfdistilling BERT with adaptive inference time. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. pp. 6035--6044\nLiu, X., Sun, T., He, J., Wu, L., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. In Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. abs/2110.07038\nLiu, X., Lai, H., Wong, D. F., and Chao, L. S. 2020. Norm-based curriculum learning for neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 427--436 10.18653/v1/2020.acl-main.41\nLiu, Y., Meng, F., Zhou, J., Chen, Y., and Xu, J. 2021. Faster depth-adaptive transformers. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. pp. 13424--13432\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. 2011. Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference. pp. 142--150\nMichel, P., Levy, O., and Neubig, G. 2019. Are sixteen heads really better than one?. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 14014--14024\nQiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., and Huang, X. 2020. Pre-trained models for natural language processing: A survey. In SCIENCE CHINA Technological Sciences. 10.1007/s11431-020-1647-3\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In J. Mach. Learn. Res. pp. 67\nReimers, N. and Gurevych, I. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 3980--3990 10.18653/v1/D19-1410\nRoller, S., Sukhbaatar, S., Szlam, A., and Weston, J. 2021. Hash layers for large sparse models. In Hash layers for large sparse models.\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. abs/1910.01108\nAdriaan, M. J., Schakel, B. J., and Wilson 2015. Measuring word significance using distributed representations of words. In Measuring word significance using distributed representations of words. abs/1508.02297\nSchwartz, R., Stanovsky, G., Swayamdipta, S., Dodge, J., and Smith, N. A. 2020. The right tool for the job: Matching model and instance complexities. In Proceedings of the 58th.\n2020. Annual Meeting of the Association for Computational Linguistics. In Annual Meeting of the Association for Computational Linguistics. pp. 6640--6651\nShao, Y., Geng, Z., Liu, Y., Dai, J., Yang, F., Zhe, L., Bao, H., and Qiu, X. 2021. CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. In CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. abs/2109.05729\nShen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer, K. 2020. Q-BERT: hessian based ultra low precision quantization of BERT. In The Thirty-Fourth AAAI Conference on Artificial Intelligence. pp. 8815--8821\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1631--1642\nSun, S., Cheng, Y., Gan, Z., and Liu, J. 2019. Patient knowledge distillation for BERT model compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 4322--4331 10.18653/v1/D19-1441\nSun, T., Zhou, Y., Liu, X., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. In Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. abs/2105.13792\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. pp. 5998--6008\nWang, R., Cheng, M., Chen, X., Tang, X., and Hsieh, C. 2021. Rethinking architecture selection in differentiable NAS. In 9th International Conference on Learning Representations.\nXin, J., Tang, R., Lee, J., Yu, Y., and Lin, J. 2020. Deebert: Dynamic early exiting for accelerating BERT inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 2246--2251\nXin, J., Tang, R., Yu, Y., and Lin, J. 2021. Berxit: Early exiting for BERT with better finetuning and extension to regression. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main. pp. 91--104\nXu, C., Zhou, W., Ge, T., Wei, F., and Zhou, M. 2020. Bert-of-theseus: Compressing BERT by progressive module replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. pp. 7859--7869 10.18653/v1/2020.emnlp-main.633\nXu, L., Hu, H., Zhang, X., Li, L., Cao, C., Li, Y., Xu, Y., Sun, K., Yu, D., Yu, C., Tian, Y., Dong, Q., Liu, W., Shi, B., Cui, Y., Li, J., Zeng, J., Wang, R., Xie, W., Li, Y., Patterson, Y., Tian, Z., Zhang, Y., Zhou, H., Liu, S., Zhao, Z., Zhao, Q., Yue, C., Zhang, X., Yang, Z., Richardson, K., and Lan, Z. 2020. CLUE: A chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics. pp. 4762--4772 10.18653/v1/2020.coling-main.419\nYang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., Quoc, V., and Le 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 5754--5764\nZhang, C., Yu, M., Wang, W., and Yan, F. 2019. Mark: Exploiting cloud services for costeffective, slo-aware machine learning inference serving. In 2019 USENIX Annual Technical Conference, USENIX ATC 2019. pp. 1049--1062\nZhou, W., Xu, C., Ge, T., Mcauley, J. J., Xu, K., and Wei, F. 2020. BERT loses patience: Fast and robust inference with early exit. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020.\nZhu, W. 2021. Leebert: Learned early exit for BERT with cross-level optimization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 2968--2980 10.18653/v1/2021.acl-long.231\nZiegel, E. R. 2003. The elements of statistical learning. In Technometrics. pp. 267--268 10.1198/tech.2003.s770"
    },
    {
        "filename": "paper_8.txt",
        "start": 3773,
        "end": 3786,
        "label": "Unsupported Claim",
        "text": "previous work",
        "full_text": "Introduction\n\nEarly exiting is a widely used technique to accelerate inference of deep neural networks. With the rising of pre-trained language models (PLMs) (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;Qiu et al., 2020), early exiting is drawing increasing attention in the NLP community. At its core, early exiting allows simple instances to exit early while allowing hard instances to exit late. Thus, how to measure instance difficulty is a crucial problem.\n\nMost existing early exiting methods attach multiple internal classifiers to the PLM and adopt some heuristic metrics, such as entropy  (Xin et al., 2020;Liu et al., 2020a) or maximum softmax score (Schwartz et al., 2020) of internal outputs, to measure instance difficulty. However, these methods can not easily generalize to new tasks. On the one hand, these metrics are not accessible on some tasks such as regression. On the other hand, In order for these methods to perform well, one usually needs to fine-tune the threshold, which varies widely across different tasks and datasets.\n\nAnother way to measure instance difficulty is to directly learn it. Recent studies (Elbayad et al., 2020;Xin et al., 2021) that use the idea of \"learn-toexit\" have achieved promising results. They jointly train a neural model to predict for each instance the exiting layer. At their core, the learn-to-exit module is to estimate the difficulty for each instance. Compared with previous heuristically designed metrics for difficulty, learn-to-exit is task-agnostic and does not require threshold-tuning, therefore is a more promising way.\n\nDespite their success, it is still unknown whether or how well the instance difficulty can be learned. As a response, in this work, we construct datasets for two kinds of instance difficulty: (a) Humandefined difficulty, and (b) Model-defined difficulty. The dataset for human-defined difficulty has two labels, 0 for instances that can be annotated by human and 1 for instances that cannot. For modeldefined difficulty, we train a multi-exit BERT (Devlin et al., 2019), which is attached with an internal classifier at each layer, on a sentence-level classification task, SNLI (Bowman et al., 2015), and a token-level classification task, OntoNotes NER (Hovy et al., 2006). The trained multi-exit BERTs are then used to annotate for each development instance whether it can be correctly predicted by each internal classifier. Thus, our constructed sentence-level and token-level model-defined difficulty datasets are multi-label classification datasets. Experimental results demonstrate that, modern neu-ral networks perform poorly on predicting instance difficulty. This observation is consistent with previous work (Laverghetta et al., 2020) on estimating instance difficulty for curriculum learning.\n\nGiven that instance difficulty is hard to be predicted, then what works in the learn-to-exit modules? We hypothesis that the consistency between training and inference may play an important role. That is, for a training instance x i that is predicted to exit at layer l, an inference instance x j that is similar with x i should be predicted to exit at layer l, too. Since neural networks are usually smooth functions (Ziegel, 2003), this consistency can be easily satisfied by neural learn-to-exit modules. If this hypothesis holds, we can replace the learn-to-exit module with a simple hash function. In particular, we use hash functions to assign each token to a fixed exiting layer. This hash-based early exiting method is named HASHEE.\n\nCompared with previous methods that use heuristic metrics for difficulty or jointly learn to exit, HASHEE offers several advantages: (a) HASHEE requires no internal classifiers nor extra parameters, which are necessary in previous work. (b) HASHEE can perform token-level early exiting without supervision, therefore can be widely used on various tasks including language understanding and generation. (c) The speed-up ratio can be easily tuned by modifying the hash function. (d) HASHEE can significantly accelerate model inference on a per-batch basis instead of per-instance basis as in previous work Liu et al., 2020a;.\n\nWe conduct experiments on classification, regression, and generation tasks. Experimental results on ELUE (Liu et al., 2021a) demonstrate that HASHEE, despite its simplicity, can achieve higher performance with fewer FLOPs and inference time than previous state-of-the-art methods on various tasks. Besides, our experiments on several text summarization datasets show that HASHEE can reduce \u223c50% FLOPs of BART (Lewis et al., 2020) and CPT  while maintaining 97% ROUGE-1 score.\n\n References: \nSamuel, R., Bowman, G., Angeli, C., Potts, C. D., and Manning 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pp. 632--642 10.18653/v1/d15-1075\nDaniel, M., Cer, M. T., Diab, E., Agirre, I., Lopez-Gazpio, L., and Specia 2017. Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. In Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. abs/1708.00055\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L. 2019. Universal transformers. In 7th International Conference on Learning Representations. ICLR 2019\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 4171--4186 10.18653/v1/n19-1423\nWilliam, B., Dolan, C., and Brockett 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005.\nElbayad, M., Gu, J., Grave, E., and Auli, M. 2020. Depth-adaptive transformer. In 8th International Conference on Learning Representations.\nGordon, M. A., Duh, K., and Andrews, N. 2020. Compressing BERT: studying the effects of weight pruning on transfer learning. In Proceedings of the 5th Workshop on Representation Learning for NLP, RepL4NLP@ACL 2020, Online. pp. 143--155 10.18653/v1/2020.repl4nlp-1.18\nMoritz Hermann, K., Kocisk\u00fd, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., and Blunsom, P. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. pp. 1693--1701\nHochreiter, S. and Schmidhuber, J. 1997. Long short-term memory. In Neural Comput. pp. 1735--1780 10.1162/neco.1997.9.8.1735\nHovy, E. H., Marcus, M. P., Palmer, M., Ramshaw, L. A., and Weischedel, R. M. 2006. Ontonotes: The 90% solution. In Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings.\nHua, L., Wan, X., and Li, L. 2017. Overview of the nlpcc 2017 shared task: Single document summarization. In National CCF Conference on Natural Language Processing and Chinese Computing. pp. 942--947\nJiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., and Liu, Q. 2020. Tinybert: Distilling BERT for natural language understanding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020. pp. 4163--4174 10.18653/v1/2020.findings-emnlp.372\nKhot, T., Sabharwal, A., and Clark, P. 2018. Scitail: A textual entailment dataset from science question answering. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). pp. 5189--5197\nKim, B., Kim, H., and Kim, G. 2019. Abstractive summarization of reddit posts with multi-level memory networks. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. pp. 2519--2531 10.18653/v1/n19-1260\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In 8th International Conference on Learning Representations.\nLaverghetta, A., Mirzakhalov, J., and Licato, J. 2020. Towards a task-agnostic model of difficulty estimation for supervised learning tasks. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, AACL/IJCNLP 2021. pp. 16--23\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 2020.\n2020. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7871--7880 10.18653/v1/2020.acl-main.703\nLi, L., Lin, Y., Chen, D., Ren, S., Li, P., Zhou, J., and Sun, X. 2021. Cascadebert: Accelerating inference of pre-trained language models via calibrated complete models cascade. In Findings of EMNLP.\nLi, X., Shao, Y., Sun, T., Yan, H., Qiu, X., and Huang, X. 2021. Accelerating BERT inference for sequence labeling via earlyexit. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 189--199 10.18653/v1/2021.acl-long.16\nLiao, K., Zhang, Y., Ren, X., and Su, Q. 2021. A global past-future early exit method for accelerating inference of pretrained language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. pp. 2013--2023 10.18653/v1/2021.naacl-main.162\nLiu, W., Zhou, P., Wang, Z., Zhao, Z., Deng, H., and Ju, Q. 2020. Fastbert: a selfdistilling BERT with adaptive inference time. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020. pp. 6035--6044\nLiu, X., Sun, T., He, J., Wu, L., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. In Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline. abs/2110.07038\nLiu, X., Lai, H., Wong, D. F., and Chao, L. S. 2020. Norm-based curriculum learning for neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 427--436 10.18653/v1/2020.acl-main.41\nLiu, Y., Meng, F., Zhou, J., Chen, Y., and Xu, J. 2021. Faster depth-adaptive transformers. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event. pp. 13424--13432\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 1907. Roberta: A robustly optimized BERT pretraining approach. In Roberta: A robustly optimized BERT pretraining approach.\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. 2011. Learning word vectors for sentiment analysis. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference. pp. 142--150\nMichel, P., Levy, O., and Neubig, G. 2019. Are sixteen heads really better than one?. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 14014--14024\nQiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., and Huang, X. 2020. Pre-trained models for natural language processing: A survey. In SCIENCE CHINA Technological Sciences. 10.1007/s11431-020-1647-3\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In J. Mach. Learn. Res. pp. 67\nReimers, N. and Gurevych, I. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 3980--3990 10.18653/v1/D19-1410\nRoller, S., Sukhbaatar, S., Szlam, A., and Weston, J. 2021. Hash layers for large sparse models. In Hash layers for large sparse models.\nSanh, V., Debut, L., Chaumond, J., and Wolf, T. 2019. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. In Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. abs/1910.01108\nAdriaan, M. J., Schakel, B. J., and Wilson 2015. Measuring word significance using distributed representations of words. In Measuring word significance using distributed representations of words. abs/1508.02297\nSchwartz, R., Stanovsky, G., Swayamdipta, S., Dodge, J., and Smith, N. A. 2020. The right tool for the job: Matching model and instance complexities. In Proceedings of the 58th.\n2020. Annual Meeting of the Association for Computational Linguistics. In Annual Meeting of the Association for Computational Linguistics. pp. 6640--6651\nShao, Y., Geng, Z., Liu, Y., Dai, J., Yang, F., Zhe, L., Bao, H., and Qiu, X. 2021. CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. In CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation. abs/2109.05729\nShen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer, K. 2020. Q-BERT: hessian based ultra low precision quantization of BERT. In The Thirty-Fourth AAAI Conference on Artificial Intelligence. pp. 8815--8821\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 1631--1642\nSun, S., Cheng, Y., Gan, Z., and Liu, J. 2019. Patient knowledge distillation for BERT model compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. pp. 4322--4331 10.18653/v1/D19-1441\nSun, T., Zhou, Y., Liu, X., Zhang, X., Jiang, H., and Cao, Z. Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. In Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR. abs/2105.13792\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. pp. 5998--6008\nWang, R., Cheng, M., Chen, X., Tang, X., and Hsieh, C. 2021. Rethinking architecture selection in differentiable NAS. In 9th International Conference on Learning Representations.\nXin, J., Tang, R., Lee, J., Yu, Y., and Lin, J. 2020. Deebert: Dynamic early exiting for accelerating BERT inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 2246--2251\nXin, J., Tang, R., Yu, Y., and Lin, J. 2021. Berxit: Early exiting for BERT with better finetuning and extension to regression. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main. pp. 91--104\nXu, C., Zhou, W., Ge, T., Wei, F., and Zhou, M. 2020. Bert-of-theseus: Compressing BERT by progressive module replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. pp. 7859--7869 10.18653/v1/2020.emnlp-main.633\nXu, L., Hu, H., Zhang, X., Li, L., Cao, C., Li, Y., Xu, Y., Sun, K., Yu, D., Yu, C., Tian, Y., Dong, Q., Liu, W., Shi, B., Cui, Y., Li, J., Zeng, J., Wang, R., Xie, W., Li, Y., Patterson, Y., Tian, Z., Zhang, Y., Zhou, H., Liu, S., Zhao, Z., Zhao, Q., Yue, C., Zhang, X., Yang, Z., Richardson, K., and Lan, Z. 2020. CLUE: A chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics. pp. 4762--4772 10.18653/v1/2020.coling-main.419\nYang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., Quoc, V., and Le 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. pp. 5754--5764\nZhang, C., Yu, M., Wang, W., and Yan, F. 2019. Mark: Exploiting cloud services for costeffective, slo-aware machine learning inference serving. In 2019 USENIX Annual Technical Conference, USENIX ATC 2019. pp. 1049--1062\nZhou, W., Xu, C., Ge, T., Mcauley, J. J., Xu, K., and Wei, F. 2020. BERT loses patience: Fast and robust inference with early exit. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020.\nZhu, W. 2021. Leebert: Learned early exit for BERT with cross-level optimization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021. pp. 2968--2980 10.18653/v1/2021.acl-long.231\nZiegel, E. R. 2003. The elements of statistical learning. In Technometrics. pp. 267--268 10.1198/tech.2003.s770"
    }
]