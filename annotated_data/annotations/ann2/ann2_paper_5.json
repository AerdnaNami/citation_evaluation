[
    {
        "filename": "paper_5.txt",
        "start": 2365,
        "end": 2376,
        "label": "Unsupported claim",
        "text": "ZuCo corpus",
        "full_text": "Introduction\n\nThe usefulness of learned self-attention functions often correlates with how well it aligns with human attention (Das et al., 2016;Klerke et al., 2016;Barrett et al., 2018;Klerke and Plank, 2019). In this paper, we evaluate how well attention flow (Abnar and Zuidema, 2020) in large language models, namely BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2020), aligns with human eye fixations during task-specific reading, compared to other shallow sequence labeling models (Lecun and Bengio, 1995;Vaswani et al., 2017) and a classic, heuristic model of human reading (Reichle et al., 2003). We compare the learned attention functions and the heuristic model across two task-specific English reading tasks, namely sentiment analysis (SST movie reviews) and relation extraction (Wikipedia), as well as natural reading, using a publicly available data set with eye-tracking recordings of native speakers of English (Hollenstein et al., 2018).\n\nContributions We compare human and model attention patterns on both sentiment reading and relation extraction tasks. In our analysis, we compare human attention to pre-trained Transformers (BERT, RoBERTa and T5), from-scratch training of two shallow sequence labeling architectures (Lecun and Bengio, 1995; Vaswani et al., 2017), as well as to a frequency baseline and a heuristic, cognitively inspired model of human reading called the E-Z Reader (Reichle et al., 2003). We find that the heuristic model correlates well with human reading, as has been reported in Sood et al. (2020b). However when we apply attention flow (Abnar and Zuidema, 2020), the pre-trained Transformer models also reach comparable levels of correlation strength. Further fine-tuning experiments on BERT did not result in increased correlation to human fixations. To understand what drives the differences between models, we perform an in-depth analysis of the effect of word predictability and POS tags on correlation strength. It reveals that Transformer models do not accurately capture tail phenomena for hard-to-predict words (in contrast to the E-Z Reader) and that Transformer attention flow shows comparably weak correlation on (proper) nouns while the E-Z Reader predicts importance of these more accurately, especially on the sentiment reading task. In addition we investigate a subset of the ZuCo corpus for which aligned task-specific and natural reading data is available and find that Transformers correlate stronger to natural reading patterns. But how faithful are these different attention patterns at producing correct task-classification on a state-of-the-art NLP model? We test this via an input reduction experiment on task-tuned BERT models which highlights the trade-off between a model\u2019s faithfulness and sparsity when comparing importance scores to human attention, i.e., less sparse (higher entropy) attention vectors seem to be less faithful with respect to model predictions. Our code is available at github.com/anon.\n\n References: \nAbdou, M., Kulmizev, A., Hill, F., Low, D. M., and S\u00f8gaard, A. 2019. Higher-order comparisons of sentence encoder representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 5838--5845 10.18653/v1/D19-1593\nAbnar, S. and Zuidema, W. 2020. Quantifying attention flow in transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4190--4197 10.18653/v1/2020.acl-main.385\nArras, L., Horn, F., Montavon, G., M\u00fcller, K., and Samek, W. 2016. Explaining predictions of non-linear classifiers in NLP. In Proceedings of the 1st Workshop on Representation Learning for NLP. pp. 1--7 10.18653/v1/W16-1601\nArras, L., Horn, F., Montavon, G., M\u00fcller, K., and Samek, W. 2017. What is relevant in a text document?\": An interpretable machine learning approach. In PLOS ONE. pp. 1--23 10.1371/journal.pone.0181142\nBach, S., Binder, A., Montavon, G., Klauschen, F., M\u00fcller, K., and Samek, W. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. In PLoS ONE. pp. e0130140 10.1371/journal.pone.0130140\nBarrett, M., Bingel, J., Hollenstein, N., Rei, M., and S\u00f8gaard, A. 2018. Sequence classification with human attention. In Proceedings of the 22nd Conference on Computational Natural Language Learning. pp. 302--312 10.18653/v1/K18-1030\nBarrett, M. and S\u00f8gaard, A. 2015. Reading behavior predicts syntactic categories. In Proceedings of the Nineteenth Conference on Computational Natural Language Learning. pp. 345--349 10.18653/v1/K15-1038\nBarrett, M. and S\u00f8gaard, A. 2015. Using reading behavior to predict grammatical functions. In Proceedings of the Sixth Workshop on Cognitive Aspects of Computational Language Learning. pp. 1--5 10.18653/v1/W15-2401\nBorji, A. and Itti, L. 2014. Defending yarbus: eye movements reveal observers' task. In Journal of vision. pp. 29 10.1167/14.3.29\nChelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., and Robinson, T. 2013. One billion word benchmark for measuring progress in statistical language modeling. In One billion word benchmark for measuring progress in statistical language modeling.\nChoi, W., Rutvik, H., Desai, J.M., and Henderson 2014. The neural substrates of natural reading: a comparison of normal and nonword text using eyetracking and fmri. In Frontiers in human neuroscience. pp. 1024\nChrupa\u0142a, G. and Alishahi, A. 2019. Correlating neural and symbolic representations of language. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 2952--2962 10.18653/v1/P19-1283\nChurch, K. and Liberman, M. 2021. The future of computational linguistics: On beyond alchemy. In Frontiers in Artificial Intelligence. pp. 10 10.3389/frai.2021.625341\nClark, K., Khandelwal, U., Levy, O., and Manning, C. D. 2019. What does BERT look at? an analysis of BERT's attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. pp. 276--286 10.18653/v1/W19-4828\nCoutrot, A., Hsiao, J. H., and Chan, A. B. 2017. Scanpath modeling and classification with hidden markov models. In Scanpath modeling and classification with hidden markov models.\nCulotta, A., Mccallum, A., and Betz, J. 2006. Integrating probabilistic extraction models and data mining to discover relations and patterns in text. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference. pp. 296--303\nDas, A., Agrawal, H., Zitnick, L., Parikh, D., and Batra, D. 2016. Human attention in visual question answering: Do humans and deep networks look at the same regions?. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pp. 932--937 10.18653/v1/D16-1092\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 4171--4186 10.18653/v1/N19-1423\nFeng, G. 2006. Eye movements as time-series random variables: A stochastic model of eye movement control in reading. In Cognitive Systems Research. pp. 70--95 10.1016/j.cogsys.2005.07.004\nShi Feng, E., Wallace, A., Grissom, I. I., Iyyer, M., Rodriguez, P., and Boyd-Graber, J. 2018. Pathologies of neural models make interpretations difficult. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 3719--3728 10.18653/v1/D18-1407\nGreene, M., Liu, T., and Wolfe, J. 2012. Reconsidering yarbus: A failure to predict observers' task from eye movement patterns. In Vision research. pp. 1--8 10.1016/j.visres.2012.03.019\nG\u00fcnther, F., Rinaldi, L., and Marelli, M. 2019. Vector-space models of semantic representation from a cognitive perspective: A discussion of common misconceptions. In Perspectives on Psychological Science. pp. 1006--1033 10.1177/1745691619861372\nHahn, M. and Keller, F. 2016. Modeling human reading with neural attention. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. pp. 85--95 10.18653/v1/D16-1009\nHaji, A., Abolhassani, -., and Clark, J. J. 2014. An inverse yarbus process: Predicting observers' task from eye movement patterns. In Vision Research. pp. 127--142 10.1016/j.visres.2014.08.014\nHara, T., Mochihashi, D., Kano, Y., and Aizawa, A. 2012. Predicting word fixations in text with a CRF model for capturing general reading strategies among readers. In Proceedings of the First Workshop on Eye-tracking and Natural Language Processing. pp. 55--70\nHenderson, J., Shinkareva, S., Wang, J., Luke, S., and Olejarczyk, J. 2013. Predicting cognitive state from eye movements. In PloS one. pp. e64937 10.1371/journal.pone.0064937\nHillen, R., G\u00fcnther, T., Kohlen, C., Eckers, C., Van Ermingen-Marbach, M., Sass, K., Scharke, W., Vollmar, J., Radach, R., and Heim, S. 2013. Identifying brain systems for gaze orienting during reading: fmri investigation of the landolt paradigm. In Frontiers in human neuroscience. pp. 384\nHollenstein, N. and Beinborn, L. 2021. Relative importance in sentence processing. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. pp. 141--150 10.18653/v1/2021.acl-short.19\nHollenstein, N., De La Torre, A., Langer, N., and Zhang, C. 2019. CogniVal: A framework for cognitive word embedding evaluation. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL). pp. 538--549 10.18653/v1/K19-1050\nHollenstein, N., Pirovano, F., Zhang, C., J\u00e4ger, L., and Beinborn, L. 2021. Multilingual language models predict human reading behavior. In Multilingual language models predict human reading behavior. arXiv:2104.05433\nHollenstein, N., Rotsztejn, J., Troendle, M., Pedroni, A., Zhang, C., and Langer, N. 2018. Zuco, a simultaneous eeg and eye-tracking resource for natural sentence reading. In Scientific data. pp. 1--13\nHonnibal, M. and Montani, I. Sofie Van Landeghem, and Adriane Boyd. 2020. spaCy: Industrialstrength Natural Language Processing in Python. In Sofie Van Landeghem, and Adriane Boyd. 2020. spaCy: Industrialstrength Natural Language Processing in Python. 10.5281/zenodo.1212303\nJain, S. and Wallace, B. C. 2019. Attention is not Explanation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 3543--3556 10.18653/v1/N19-1357\nKilgarriff, A. 1995. BNC database and word frequency lists. In BNC database and word frequency lists. Accessed: 07/2020\nKim, Y. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 1746--1751 10.3115/v1/D14-1181\nKlerke, S., Goldberg, Y., and S\u00f8gaard, A. 2016. Improving sentence compression by learning to predict gaze. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 1528--1533 10.18653/v1/N16-1179\nKlerke, S. and Plank, B. 2019. At a glance: The impact of gaze aggregation views on syntactic tagging. In Proceedings of the Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN). pp. 51--61 10.18653/v1/D19-6408\nKliegl, R., Grabner, E., Rolfs, M., and Engbert, R. 2004. Length, frequency, and predictability effects of words on eye movements in reading. In European Journal of Cognitive Psychology. pp. 262--284 10.1080/09541440340000213\nKneser, R. and Ney, H. 1995. Improved backing-off for m-gram language modeling. In 1995 international conference on acoustics, speech, and signal processing. pp. 181--184\nKoch, C. and Ullman, S. 1985. Shifts in Selective Visual Attention: Towards the Underlying Neural Circuitry. In Human Neurobiology. pp. 219--227\nKovaleva, O., Romanov, A., Rogers, A., and Rumshisky, A. 2019. Revealing the dark secrets of bert. In Revealing the dark secrets of bert. arXiv:1908.08593\nK\u00fcmmerer, M., Thomas, S. A., Wallis, M., and Bethge 2016. DeepGaze II: Reading fixations from deep features trained on object recognition. In DeepGaze II: Reading fixations from deep features trained on object recognition. ArXiv: 1610.01563\nK\u00fcmmerer, M., Thomas, S. A., Wallis, L. A., Gatys, M., and Bethge 2017. Understanding low-and high-level contributions to fixation prediction. In 2017 IEEE International Conference on Computer Vision (ICCV). pp. 4799--4808 10.1109/ICCV.2017.513\nLecun, Y. and Bengio, Y. 1995. Convolutional Networks for Images, Speech and Time Series. In Convolutional Networks for Images, Speech and Time Series. pp. 255--258\nLin, Z., Feng, M., Nogueira, C., Santos, M., Yu, B., Xiang, B., Zhou, Y., and Bengio 2017. A structured self-attentive sentence embedding. In A structured self-attentive sentence embedding. abs/1703.03130\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 2019. Roberta: A robustly optimized bert pretraining approach. In Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692\nTomasz, D., Loboda, P., Brusilovsky, J., and Brunstein 2011. Inferring word relevance from eyemovements of readers. In Proceedings of the 16th international conference on Intelligent user interfaces. pp. 175--184\nMalmaud, J., Levy, R., and Berzak, Y. 2020. Bridging information-seeking human gaze and machine reading comprehension. In Proceedings of the 24th Conference on Computational Natural Language Learning. pp. 142--152 10.18653/v1/2020.conll-1.11\nMandera, P., Keuleers, E., and Brysbaert, M. 2017. Explaining human performance in psycholinguistic tasks with models of semantic similarity based on prediction and counting : a review and empirical validation. In Journal of Memory and Language. pp. 57--78 10.1016/j.jml.2016.04.001\nMatthies, F. and S\u00f8gaard, A. 2013. With blinkers on: Robust prediction of eye movements across readers. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pp. 803--807\nSara, V., Milledge, H. I., and Blythe 2019. The changing role of phonology in reading development. In The changing role of phonology in reading development. 10.3390/vision3020023\nMiller, T. 2019. Explanation in artificial intelligence: Insights from the social sciences. In Artificial Intelligence. pp. 1--38 10.1016/j.artint.2018.07.007\nMishra, A., Dey, K., and Bhattacharyya, P. 2017. Learning cognitive features from gaze data for sentiment and sarcasm classification using convolutional neural network. In Proceedings of the 55th. 10.18653/v1/P17-1035\nAnnual Meeting of the Association for Computational Linguistics. In Annual Meeting of the Association for Computational Linguistics. pp. 377--387\nMishra, A., Kanojia, D., Nagar, S., Dey, K., and Bhattacharyya, P. 2016. Leveraging cognitive features for sentiment analysis. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning. pp. 156--166 10.18653/v1/K16-1016\nMontavon, G., Binder, A., Lapuschkin, S., Samek, W., and M\u00fcller, K. 2019. Layer-Wise Relevance Propagation: An Overview. In Layer-Wise Relevance Propagation: An Overview. pp. 193--209 10.1007/978-3-030-28954-6_10\nNiebur, E. and Koch, C. 1996. Control of selective visual attention: Modeling the \"where\" pathway. In Advances in Neural Information Processing Systems. pp. 802--808\nNilsson, M. and Nivre, J. 2009. Learning where to look: Modeling eye movements in reading. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning. pp. 93--101\nPennington, J., Socher, R., and Manning, C. D. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP). pp. 1532--1543\nPrasad, G., Nie, Y., Bansal, M., Jia, R., Kiela, D., and Williams, A. 2021. To what extent do human explanations of model behavior align with actual model behavior?. In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. pp. 1--14\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In Journal of Machine Learning Research. pp. 1--67\nRayner, K. 1998. Eye movements in reading and information processing: 20 years of research. In Psychological bulletin. pp. 372\nRayner, K. and Duffy, S. A. 1986. Lexical complexity and fixation times in reading: effects of word frequency, verb complexity, and lexical ambiguity. In Memory amp; cognition. pp. 191--201 10.3758/bf03197692\nRayner, K. and Reichle, E. D. 2010. Models of the reading process. In WIREs Cognitive Science. pp. 787--799 10.1002/wcs.68\nErik D Reichle, A., Pollatsek, Donald, L., Fisher, K., and Rayner 1998. Toward a model of eye movement control in reading. In Psychological review. pp. 125\nReichle, E. D., Rayner, K., and Pollatsek, A. 2003. The e-z reader model of eye-movement control in reading: comparisons to other models. The Behavioral and brain sciences. In The e-z reader model of eye-movement control in reading: comparisons to other models. The Behavioral and brain sciences. pp. 445--476 10.1017/S0140525X03000104\nRogers, T. and Wolmetz, M. 2016. Conceptual knowledge representation: A cross-section of current research. In Cognitive Neuropsychology. pp. 1--9 10.1080/02643294.2016.1188066\nSamek, W., Montavon, G., Lapuschkin, S., Anders, C. J., and M\u00fcller, K. 2021. Explaining deep neural networks and beyond: A review of methods and applications. Proceedings of the IEEE. In Explaining deep neural networks and beyond: A review of methods and applications. Proceedings of the IEEE. pp. 247--278 10.1109/JPROC.2021.3060483\nSchmidt, P. and Bie\u00dfmann, F. 2019. Quantifying interpretability and trust in machine learning systems. In CoRR. abs/1901.08558\nSerrano, S. and Smith, N. A. 2019. Is attention interpretable?. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 2931--2951 10.18653/v1/P19-1282\nSinha, K., Jia, R., Hupkes, D., Pineau, J., Williams, A., and Kiela, D. 2021. Masked language modeling and the distributional hypothesis: Order word matters pre-training for little. In Masked language modeling and the distributional hypothesis: Order word matters pre-training for little.\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Christopher, D., Manning, Andrew, Y., Ng, C., and Potts 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing. pp. 1631--1642\nS\u00f8gaard, A. 2016. Evaluating word embeddings with fMRI and eye-tracking. In Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP. pp. 116--121 10.18653/v1/W16-2521\nSood, E., Tannert, S., Frassinelli, D., Bulling, A., and Vu, N. T. 2020. Interpreting attention models with human visual attention in machine reading comprehension. In Proceedings of the 24th Conference on Computational Natural Language Learning. pp. 12--25 10.18653/v1/2020.conll-1.2\nSood, E., Tannert, S., Mueller, P., and Bulling, A. 2020. Improving natural language processing tasks with human gaze-guided neural attention. In Advances in Neural Information Processing Systems. pp. 6327--6341\nStolcke, A. 2002. Srilm -an extensible language modeling toolkit. In Proceedings of the 7th International Conference on Spoken Language Processing. pp. 901--904\nSugano, Y. and Bulling, A. 2016. Seeing with humans: Gaze-assisted neural image captioning. In Seeing with humans: Gaze-assisted neural image captioning. abs/1608.05203\nArun Balajee Vasudevan, D., Dai, L., and Van Gool 2018. Object referring in videos with language and human gaze. In 2018 IEEE Conference on Computer Vision and Pattern Recognition. pp. 4129--4138 10.1109/CVPR.2018.00434\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems. pp. 5998--6008\nWenzel, M. A., Bogojeski, M., and Blankertz, B. 2017. Real-time inference of word relevance from electroencephalogram and eye gaze. In Journal of neural engineering. pp. 56007\nWiegreffe, S. and Pinter, Y. 2019. Attention is not not explanation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 11--20 10.18653/v1/D19-1002\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., Patrick Von Platen, C., Ma, Y., Jernite, J., Plu, C., Xu, T. L., Scao, S., Gugger, M., Drame, Q., Lhoest, A., and Rush 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. pp. 38--45 10.18653/v1/2020.emnlp-demos.6\nYarbus, A. L. 1967. Eye Movements and Vision. Plenum. In Eye Movements and Vision. Plenum.\nZhang, Y. and Zhang, C. 2019. Using human attention to extract keyphrase from microblog post. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 5867--5872 10.18653/v1/P19-1588\nZhao, Y. and Bethard, S. 2020. How does BERT's attention change when you fine-tune? an analysis methodology and a case study in negation scope. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 4729--4747 10.18653/v1/2020.acl-main.429"
    }
]